[
  {
    "objectID": "python/README.html",
    "href": "python/README.html",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The raw UKHAD observational data needs to be resampled to the same grid of the RCP8.5 data. This can be done with the python/resampling/resampling_hads.py script, which takes an input grid and uses to resample the data using linear interpolation (for simplicity have added a default grid in data/rcp85_land-cpm_uk_2.2km_grid.nc).\nThe script runs under the conda environment created on the main README.md and has several options that can be understood by running the following from the resampling directory:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n  -h, --help            show this help message and exit\n  --input INPUT         Path where the .nc files to resample is located\n  --output OUTPUT       Path to save the resampled data data\n  --grid_data GRID_DATA\n                        Path where the .nc file with the grid to resample is located\nThe script expects the data to be files of .nc extension, have dimensions named projection_x_coordinate and projection_y_coordinate and to follow the format of the CEDA Archive. Furthermore, the layer/variable to be resampled must be on the beginning of the name of the file before any _ (e.g for tasmax is tasmax_hadukgrid_uk_1km_day_19930501-19930531.nc).\n\n\nFor example, to run the resampling on tasmax daily data found in the fileshare (https://dymestorage1.file.core.windows.net/vmfileshare).\n$ cd python/resampling\n$ python resampling_hads.py --input /Volumes/vmfileshare/ClimateData/Raw/HadsUKgrid/tasmax/day --output &lt;local-directory-path&gt;\nas there is not a --grid_data flag, the default file described above is used.\n\n\n\n\nIn [python/load_data/data_loader.py] we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb.\n\n\n\nThe code in the debiasing directory contains scripts that interface with implementations of the debiasing methods implemented by different libraries.\nNote: By March 2023 we have only implemented the python-cmethods library.\n\n\nThis repository contains two python scripts one for preprocessing/grouping data and one to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\n$ cd debiasing\n$ git submodule update --init --recursive\n\nThe preprocess_data.py script allows the user to specify directories from which the modelled (CPM/UKCP) data and observation (HADs) data should be loaded, as well as time periods to use for calibration and validation. The script parses the necessary files and combines them into two files for calibration (modelled and observed), and two files for validation (modelled and observed) - with the option to specify multiple validation periods. These can then be used as inputs to run_cmethods.py.\nThe run_cmethods.py script allow us to adjust climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data) and modelled data (historical CPM/UKCP data) for calibration, as well as observation and modelled data for validation (generated by preprocess_data.py). It calibrates the debiasing method using the calibration period data and debiases the modelled data for the validation period. The resulting output is saved as a .nc to a specified directory. The script will also produce a time-series and a map plot of the debiased data.\n\nUsage:\nThe scripts can be run from the command line using the following arguments:\n$ python3 preprocess_data.py --mod &lt;path to modelled datasets&gt; --obs &lt;path to observation datasets&gt; --shp &lt;shapefile&gt; --out &lt;output file path&gt; -v &lt;variable&gt; -u &lt;unit&gt; -r &lt;CPM model run number&gt; --calib_dates &lt;date range for calibration&gt; --valid_dates &lt;date range for validation&gt;\n\n$ python3 run_cmethods.py --input_data_folder &lt;input files directory&gt; --out &lt;output directory&gt; -m &lt;method&gt; -v &lt;variable&gt; -g &lt;group&gt; -k &lt;kind&gt; -n &lt;number of quantiles&gt; -p &lt;number of processes&gt;\nFor more details on the scripts and options you can run:\n$ python3 preprocess_data.py --help\nand\npython3 run_cmethods.py --help\nMain Functionality:\nThe preprocess_data.py script performs the following steps:\n\nParses the input arguments.\nLoads, merges and clips (if shapefile is provided) all calibration datasets and merges them into two distinct datasets: the m modelled and observed datasets.\nAligns the calendars of the two datasets, ensuring that they have the same time dimension.\nSaves the calibration datasets to the output directory.\nLoops over the validation time periods specified in the calib_dates variable and performs the following steps:\n\nLoads the modelled data for the current time period.\nLoads the observed data for the current time period.\nAligns and saves the datasets to the output directory.\n\n\nThe run_cmethods.py script performs the following steps: - Reads the input calibration and validation datasets from the input directory. - Applies the specified debiasing method, combining the calibration and valiation data. - Saves the resulting output to the specified directory. - Creates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\nWorking example.\nExample of how to run the two scripts using data stored in the Azure fileshare, running the scripts locally (uses input data that have been cropped to contain only the city of Glasgow. The two scripts will debias only the tasmax variable, run 05 of the CPM, for calibration years 1980-2009 and validation years 2010-2019. It uses the quantile_delta_mapping debiasing method:\n$ python3 preprocess_data.py --mod /Volumes/vmfileshare/ClimateData/Cropped/three.cities/CPM/Glasgow/ --obs /Volumes/vmfileshare/ClimateData/Cropped/three.cities/Hads.original360/Glasgow/ -v tasmax --out ./preprocessed_data/ --calib_dates 19800101-20091230 --valid_dates 20100101-20191230 --run_number 05\n\n$ python3 run_cmethods.py --input_data_folder ./preprocessed_data/  --out ./debiased_data/  --method quantile_delta_mapping --v tasmax -p 4\n\n\n\n\nTesting for python components uses pytest, with configuration specified in clim-recal/python/.pytest.ini. To run tests, ensure the environment.yml environment is installed and activated, then run pytest from within the clim-recal/python checkout directory. Note: tests are skipped unless run on a specific linux server wth data mounted to a specific path.\n(clim-recal)$ cd clim-recal\n(clim-recal)$ conda activate clim-recal\n(clim-recal)$ cd python\n(clim-recal)$ pytest\n...sss........sss.....                                                         [100%]\n============================== short test summary info ===============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_mod_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n16 passed, 6 skipped, 4 deselected in 0.26s"
  },
  {
    "objectID": "python/README.html#resampling-hads-grid-from-1-km-to-2.2-km",
    "href": "python/README.html#resampling-hads-grid-from-1-km-to-2.2-km",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The raw UKHAD observational data needs to be resampled to the same grid of the RCP8.5 data. This can be done with the python/resampling/resampling_hads.py script, which takes an input grid and uses to resample the data using linear interpolation (for simplicity have added a default grid in data/rcp85_land-cpm_uk_2.2km_grid.nc).\nThe script runs under the conda environment created on the main README.md and has several options that can be understood by running the following from the resampling directory:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n  -h, --help            show this help message and exit\n  --input INPUT         Path where the .nc files to resample is located\n  --output OUTPUT       Path to save the resampled data data\n  --grid_data GRID_DATA\n                        Path where the .nc file with the grid to resample is located\nThe script expects the data to be files of .nc extension, have dimensions named projection_x_coordinate and projection_y_coordinate and to follow the format of the CEDA Archive. Furthermore, the layer/variable to be resampled must be on the beginning of the name of the file before any _ (e.g for tasmax is tasmax_hadukgrid_uk_1km_day_19930501-19930531.nc).\n\n\nFor example, to run the resampling on tasmax daily data found in the fileshare (https://dymestorage1.file.core.windows.net/vmfileshare).\n$ cd python/resampling\n$ python resampling_hads.py --input /Volumes/vmfileshare/ClimateData/Raw/HadsUKgrid/tasmax/day --output &lt;local-directory-path&gt;\nas there is not a --grid_data flag, the default file described above is used."
  },
  {
    "objectID": "python/README.html#loading-ukcp-and-hads-data",
    "href": "python/README.html#loading-ukcp-and-hads-data",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "In [python/load_data/data_loader.py] we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb."
  },
  {
    "objectID": "python/README.html#running-debiasing-methods",
    "href": "python/README.html#running-debiasing-methods",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The code in the debiasing directory contains scripts that interface with implementations of the debiasing methods implemented by different libraries.\nNote: By March 2023 we have only implemented the python-cmethods library.\n\n\nThis repository contains two python scripts one for preprocessing/grouping data and one to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\n$ cd debiasing\n$ git submodule update --init --recursive\n\nThe preprocess_data.py script allows the user to specify directories from which the modelled (CPM/UKCP) data and observation (HADs) data should be loaded, as well as time periods to use for calibration and validation. The script parses the necessary files and combines them into two files for calibration (modelled and observed), and two files for validation (modelled and observed) - with the option to specify multiple validation periods. These can then be used as inputs to run_cmethods.py.\nThe run_cmethods.py script allow us to adjust climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data) and modelled data (historical CPM/UKCP data) for calibration, as well as observation and modelled data for validation (generated by preprocess_data.py). It calibrates the debiasing method using the calibration period data and debiases the modelled data for the validation period. The resulting output is saved as a .nc to a specified directory. The script will also produce a time-series and a map plot of the debiased data.\n\nUsage:\nThe scripts can be run from the command line using the following arguments:\n$ python3 preprocess_data.py --mod &lt;path to modelled datasets&gt; --obs &lt;path to observation datasets&gt; --shp &lt;shapefile&gt; --out &lt;output file path&gt; -v &lt;variable&gt; -u &lt;unit&gt; -r &lt;CPM model run number&gt; --calib_dates &lt;date range for calibration&gt; --valid_dates &lt;date range for validation&gt;\n\n$ python3 run_cmethods.py --input_data_folder &lt;input files directory&gt; --out &lt;output directory&gt; -m &lt;method&gt; -v &lt;variable&gt; -g &lt;group&gt; -k &lt;kind&gt; -n &lt;number of quantiles&gt; -p &lt;number of processes&gt;\nFor more details on the scripts and options you can run:\n$ python3 preprocess_data.py --help\nand\npython3 run_cmethods.py --help\nMain Functionality:\nThe preprocess_data.py script performs the following steps:\n\nParses the input arguments.\nLoads, merges and clips (if shapefile is provided) all calibration datasets and merges them into two distinct datasets: the m modelled and observed datasets.\nAligns the calendars of the two datasets, ensuring that they have the same time dimension.\nSaves the calibration datasets to the output directory.\nLoops over the validation time periods specified in the calib_dates variable and performs the following steps:\n\nLoads the modelled data for the current time period.\nLoads the observed data for the current time period.\nAligns and saves the datasets to the output directory.\n\n\nThe run_cmethods.py script performs the following steps: - Reads the input calibration and validation datasets from the input directory. - Applies the specified debiasing method, combining the calibration and valiation data. - Saves the resulting output to the specified directory. - Creates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\nWorking example.\nExample of how to run the two scripts using data stored in the Azure fileshare, running the scripts locally (uses input data that have been cropped to contain only the city of Glasgow. The two scripts will debias only the tasmax variable, run 05 of the CPM, for calibration years 1980-2009 and validation years 2010-2019. It uses the quantile_delta_mapping debiasing method:\n$ python3 preprocess_data.py --mod /Volumes/vmfileshare/ClimateData/Cropped/three.cities/CPM/Glasgow/ --obs /Volumes/vmfileshare/ClimateData/Cropped/three.cities/Hads.original360/Glasgow/ -v tasmax --out ./preprocessed_data/ --calib_dates 19800101-20091230 --valid_dates 20100101-20191230 --run_number 05\n\n$ python3 run_cmethods.py --input_data_folder ./preprocessed_data/  --out ./debiased_data/  --method quantile_delta_mapping --v tasmax -p 4"
  },
  {
    "objectID": "python/README.html#testing",
    "href": "python/README.html#testing",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "Testing for python components uses pytest, with configuration specified in clim-recal/python/.pytest.ini. To run tests, ensure the environment.yml environment is installed and activated, then run pytest from within the clim-recal/python checkout directory. Note: tests are skipped unless run on a specific linux server wth data mounted to a specific path.\n(clim-recal)$ cd clim-recal\n(clim-recal)$ conda activate clim-recal\n(clim-recal)$ cd python\n(clim-recal)$ pytest\n...sss........sss.....                                                         [100%]\n============================== short test summary info ===============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_mod_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n16 passed, 6 skipped, 4 deselected in 0.26s"
  },
  {
    "objectID": "docs/pipeline.html",
    "href": "docs/pipeline.html",
    "title": "Workflow",
    "section": "",
    "text": "Workflow diagram\n\n\n\n\n\ngraph TB\n\nsubgraph Legend\n    direction RL\n    data_external[(external data)]\n    data_fileshare[path to fileshare]\n    script_r([R script])\n    script_py([Python script])\n    script_bash([Bash script])\n    var[parameter]:::var\nend\n\n%%% INPUT DATA\nsubgraph CEDA\n    data_hads[(HADS)]\n    data_cpm[(UKCP2.2)]\n    data_hads --&gt; script_load\n    data_cpm --&gt; script_load\n    data_hads --&gt; script_load\nend\n\nsubgraph Core pipeline\n    subgraph Data Ingress\n        %%% Loading data to disk\n        script_load([ceda_ftp_download.py])\n        data_hads_raw[RAW/HadsUKgrid/../*.nc]\n        data_cpm_raw[RAW/UKCP2.2/../*.nc]\n        script_load --&gt; data_hads_raw\n        script_load --&gt; data_cpm_raw\n    end\n    subgraph Preprocessing\n        %% resampling & reprojecting\n        script_resampling([resampling_hads.py])\n        script_reproject([reproject_all.sh])\n\n        data_hads_res[Processed/HadsUKgrid/../*.nc]\n        data_cpm_rep[Reprojected/UKCP2.2/../*.tiff]\n\n        script_resampling --&gt; data_hads_res\n        script_reproject --&gt; data_cpm_rep\n\n        %% cropping\n        script_crop_city([Cropping_Rasters_to_three_cities.R])\n\n        data_cropped_cpm[Cropped/cpm/..]\n        data_cropped_hads[Cropped/hads/..]\n        script_crop_city --&gt; data_cropped_cpm\n        script_crop_city --&gt; data_cropped_hads\n\n\n    end\n\n    subgraph Data Splitting\n        data_outdir[Cropped/preprocessed/..]\n\n        script_preproc([preprocess_data.py])\n\n        data_out_train[../simh..]\n        data_out_calibrate[../simp..]\n        data_out_groundtruth_h[../obsh..]\n        data_out_groundtruth_p[../obsp..]\n\n        script_preproc --&gt; data_outdir\n\n        data_outdir --&gt; data_out_train\n        data_outdir --&gt; data_out_calibrate\n        data_outdir --&gt; data_out_groundtruth_h\n        data_outdir --&gt; data_out_groundtruth_p\n    end\n\n    subgraph bc[Bias Correction]\n        script_bc_py([run_cmethods.py])\n        script_bc_r([run_cmethods.R])\n        function_bc_r[[fitQmapQUANT.R]]\n\n\n        data_out_py[Debiased/...]\n        data_out_r[Debiased/R/QuantileMapping/resultsL*]\n\n        data_out_train --&gt; script_bc_py\n        data_out_calibrate --&gt; script_bc_py\n        data_out_groundtruth_h --&gt; script_bc_py\n        data_out_train --&gt; script_bc_r\n        data_out_calibrate --&gt; script_bc_r\n        data_out_groundtruth_h --&gt; script_bc_r\n        script_bc_r --&gt; function_bc_r\n\n        script_bc_py--&gt;data_out_py\n        function_bc_r--&gt;data_out_r\n    end\n\n    subgraph Assessment\n        script_asses[tbc]\n        data_out_groundtruth_p --&gt; script_asses\n    end\n    data_out_py --&gt; script_asses\n    data_out_r --&gt; script_asses\nend\n\n\nsubgraph nner_py[Execute Python pipeline for MO dataset]\n    data_shape_uk[(shape London)]\n    data_shape_gl[(shape Glasgow)]\n    data_shape_ma[(shape Manchester)]\n\n\n    script_BC_wrapper[three_cities_debiasing.sh]\n    param1[\"metric (eg tasmax)\"]:::var\n    param2[\"runs (eg 05)\"]:::var\n    param3[\"BC method (eg quantile_mapping)\"]:::var\n    param4[city]:::var\n\n    script_BC_wrapper --&gt; param1\n    param1 --&gt; param2\n    param2 --&gt; param3\n    param3 --&gt; param4\n    param4 -- for loop --&gt; script_preproc\n\n    %% Looping connections\n    param4 -.-&gt; param3\n    param3 -.-&gt; param2\n    param2 -.-&gt; param1\nend\n\nsubgraph nner_jupyter[Jupyter Notebook for Guidance]\n    direction BT\n    data_shape_gl2[(shape Glasgow)]\n    data_cpm2[(UKCP2.2_Monthly)]\n\n    param5[\"tasmax\"]:::var\n    param6[\"quantile_mapping\"]:::var\n    param7[Glasgow]:::var\n\n    script_BC_wrapper --&gt; param1\n    param5 --&gt; script_preproc\n    param6 --&gt; script_preproc\n    param7 --&gt; script_preproc\n\n    data_cpm2 --&gt; script_load\n    data_shape_gl2 --&gt; script_crop_city\nend\n\n%% between block connections\n%% input preproc 1\ndata_hads_raw --&gt; script_resampling\ndata_cpm_raw --&gt; script_reproject\n%% input cropping\ndata_cpm_rep --&gt; script_crop_city\n\ndata_hads_res --&gt; script_crop_city\ndata_shape_uk --&gt; script_crop_city\ndata_shape_ma --&gt; script_crop_city\ndata_shape_gl --&gt; script_crop_city\n\n%% input preproc2\ndata_cropped_cpm --&gt; script_preproc\ndata_cropped_hads --&gt; script_preproc\n\nparam4 -- for loop --&gt; script_bc_py\n\n\n%% class styles\nclassDef python fill:#4CAF50;\nclassDef r fill:#FF5722;\nclassDef bash fill:#f9f\nclassDef var fill:none,stroke:#0f0;\nclassDef dashed stroke-dasharray: 5 5;\n\nclass script_crop_city,script_crop_uk,function_bc_r,script_r,script_df_uk,function_bc,function_crop_bc,fn_crop_cpm,fn_crop_hads,fn_bc,script_bc_r r;\nclass script_load,script_resampling,script_preproc,script_bc_py,script_py python;\nclass script_reproject,script_BC_wrapper,script_bash bash;\nclass inner_py dashed;\nclass inner_r dashed;\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/reference/utils.html",
    "href": "docs/reference/utils.html",
    "title": "1 utils",
    "section": "",
    "text": "``\nUtility functions.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndate_range_to_str\nTake start_date and end_date str or date instances and return a range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and to pass to make_user.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nrm_user\nRemove user and user home folder.\n\n\n\n\n\ndate_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=DATE_FORMAT_STR, out_format_str=DATE_FORMAT_STR)\nTake start_date and end_date str or date instances and return a range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nutils.DateType\nFirst date in range.\nrequired\n\n\nend_date\nutils.DateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nDATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nDATE_FORMAT_STR\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n\n\n\n\ndate_to_str(date_obj, in_format_str=DATE_FORMAT_STR, out_format_str=DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\nutils.DateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nDATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nDATE_FORMAT_STR\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\niter_to_tuple_strs(iter_var)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\ntyping.Iterable[typing.Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n\n\n\n\nmake_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nuser and home folder name\nrequired\n\n\npassword\nstr\nlogin password\nrequired\n\n\ncode_path\npathlib.Path\npath to copy code from to user path\nRSTUDIO_DOCKER_USER_PATH\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = Path('/home/jovyan')\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nmake_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and to pass to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\npathlib.Path\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\ntyping.Callable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; code_path: Path = Path('/home/jovyan')\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = Path('tests/test_user_accounts.xlsx')\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file() for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\npath_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\npathlib.Path\nPath of folder to iterate through\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA Generator of Paths within folder path.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nrm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\npathlib.Path\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'"
  },
  {
    "objectID": "docs/reference/utils.html#functions",
    "href": "docs/reference/utils.html#functions",
    "title": "1 utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndate_range_to_str\nTake start_date and end_date str or date instances and return a range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and to pass to make_user.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nrm_user\nRemove user and user home folder.\n\n\n\n\n\ndate_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=DATE_FORMAT_STR, out_format_str=DATE_FORMAT_STR)\nTake start_date and end_date str or date instances and return a range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nutils.DateType\nFirst date in range.\nrequired\n\n\nend_date\nutils.DateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nDATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nDATE_FORMAT_STR\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n\n\n\n\ndate_to_str(date_obj, in_format_str=DATE_FORMAT_STR, out_format_str=DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\nutils.DateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nDATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nDATE_FORMAT_STR\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\niter_to_tuple_strs(iter_var)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\ntyping.Iterable[typing.Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n\n\n\n\nmake_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nuser and home folder name\nrequired\n\n\npassword\nstr\nlogin password\nrequired\n\n\ncode_path\npathlib.Path\npath to copy code from to user path\nRSTUDIO_DOCKER_USER_PATH\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = Path('/home/jovyan')\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nmake_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and to pass to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\npathlib.Path\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\ntyping.Callable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; code_path: Path = Path('/home/jovyan')\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = Path('tests/test_user_accounts.xlsx')\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file() for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\npath_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\npathlib.Path\nPath of folder to iterate through\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA Generator of Paths within folder path.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nrm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\npathlib.Path\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'"
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "rm(list=ls())\n\nknitr::opts_knit$set(root.dir=\"/Volumes/vmfileshare/ClimateData/\")\n\nlibrary(terra)\nlibrary(sp)\nlibrary(exactextractr)\n\ndd &lt;- \"/Volumes/vmfileshare/ClimateData/\"\n\n\nBias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS\n\n\n\n\n\n\nResampling script [here](https://github.com/alan-turing-institute/clim-recal/blob/main/R/Resampling.HADs.inR.R)\nThe 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")\n\n\n\n\nCrop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE"
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Bias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS"
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Resampling script [here](https://github.com/alan-turing-institute/clim-recal/blob/main/R/Resampling.HADs.inR.R)\nThe 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")"
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Crop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE"
  },
  {
    "objectID": "R/README.html",
    "href": "R/README.html",
    "title": "1 Methods implemented in R",
    "section": "",
    "text": "1 Methods implemented in R\n\n/Resampling - code for Resampling data to different extents (grid sizes)\n/bias-correction-methods - bias correction methods implemented in R\n/comparing-r-and-python - Comparing various pipeline aspects between R and python\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Welcome to clim-recal, a specialized resource designed to tackle systematic errors or biases in Regional Climate Models (RCMs). As researchers, policy-makers, and various stakeholders explore publicly available RCMs, they need to consider the challenge of biases that can affect the accurate representation of climate change signals.\nclim-recal provides both a broad review of available bias correction methods as well as software, practical tutorials and guidance that helps users apply these methods methods to various datasets.\nclim-recal is an extensive software library and guide to application of Bias Correction (BC) methods:\n\nContains accessible information about the why and how of bias correction for climate data\nIs a software library for for the application of BC methods (see our full pipeline for bias-correction of the ground-breaking local-scale (2.2km) Convection Permitting Model (CPM). clim-recal brings together different software packages in python and R that implement a variety of bias correction methods, making it easy to apply them to data and compare their outputs.\nWas developed in partnership with the MetOffice to ensure the propriety, quality, and usability of our work\nProvides a framework for open additions of new software libraries/bias correction methods (in planning)\n\n\n\n\nOverview: Bias Correction Pipeline\nDocumentation\nThe Datasets\nWhy Bias Correction?\nContributing\nFuture Plans\nLicense\n\n\n\n\nclim-recal is a debiasing pipeline, with the following steps:\n\nSet-up & data download We provide custom scripts to facilitate download of data\nPreprocessing This includes reprojecting, resampling & splitting the data prior to bias correction\nApply bias correction Our pipeline embeds two distinct methods of bias correction\nAssess the debiased data We have developed a way to assess the quality of the debiasing step across multiple alternative methods\n\nFor a quick start on bias correction, refer to our comprehensive analysis pipeline guide.\n\n\n\nWe are in the process of developing comprehensive documentation for our code base to supplement the guidance provided in this and other README.md files. In the interim, there is documentation available in the following forms:\n\nComments within R scripts\nCommand line --help documentation for some of our python scripts\npython function and class docstrings\nLocal render of documentation via quarto\n\n\n\nFor R scripts, please refer to contextual information and usage guidelines, and feel free to reach out with any specific queries.\n\n\n\nFor many of our python command line scripts, you can use the --help flag to access a summary of the available options and usage information:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n-h, --help            show this help message and exit\n--input INPUT         Path where the .nc files to resample is located\n--output OUTPUT       Path to save the resampled data data\n--grid_data GRID_DATA Path where the .nc file with the grid to resample is located\nThis will display all available options for the script, including their purposes.\n\n\n\nWe also hope to provide comprehensive documentation via quarto. This is a work in progress, but if you would like to render documentation locally you can do so via quarto and conda:\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.\n\n\n\n\n\n\nThe UK Climate Projections 2018 (UKCP18) dataset offers insights into the potential climate changes in the UK. UKCP18 is an advancement of the UKCP09 projections and delivers the latest evaluations of the UK’s possible climate alterations in land and marine regions throughout the 21st century. This crucial information aids in future Climate Change Risk Assessments and supports the UK’s adaptation to climate change challenges and opportunities as per the National Adaptation Programme.\n\n\n\nHadUK-Grid is a comprehensive collection of climate data for the UK, compiled from various land surface observations across the country. This data is organized into a uniform grid to ensure consistent coverage throughout the UK at up to 1km x 1km resolution. The dataset, spanning from 1836 to the present, includes a variety of climate variables such as air temperature, precipitation, sunshine, and wind speed, available on daily, monthly, seasonal, and annual timescales.\n\n\n\nThe geographical dataset can be used for visualising climate data. It mainly includes administrative boundaries published by the Office for National Statistics (ONS). The dataset is sharable under the Open Government Licence v.3.0 and is available for download via this link. We include a copy in the data/Geofiles folder for convenience. In addition, the clips for three cities’ boundaries from the same dataset are copied to three.cities subfolder.\n\n\n\n\nRegional climate models contain systematic errors, or biases in their output [1]. Biases arise in RCMs for a number of reasons, such as the assumptions in the general circulation models (GCMs), and in the downscaling process from GCM to RCM [1,2].\nResearchers, policy-makers and other stakeholders wishing to use publicly available RCMs need to consider a range of “bias correction” methods (sometimes referred to as “bias adjustment” or “recalibration”). Bias correction methods offer a means of adjusting the outputs of RCM in a manner that might better reflect future climate change signals whilst preserving the natural and internal variability of climate [2].\nPart of the clim-recal project is to review several bias correction methods. This work is ongoing and you can find our initial taxonomy here. When we’ve completed our literature review, it will be submitted for publication in an open peer-reviewed journal.\nOur work is however, just like climate data, intended to be dynamic, and we are in the process of setting up a pipeline for researchers creating new methods of bias correction to be able to submit their methods for inclusion on in the clim-recal repository.\n\nSenatore et al., 2022, https://doi.org/10.1016/j.ejrh.2022.101120\nAyar et al., 2021, https://doi.org/10.1038/s41598-021-82715-1\n\n\n\n\nWe hope to bring together the extensive work already undertaken by the climate science community and showcase a range of libraries and techniques. If you have suggestions on the repository, or would like to include a new method (see below) or library, please raise an issue or get in touch!\n\n\nTo use R in anaconda you may need to specify the conda-forge channel:\n$ conda config --env --add channels conda-forge\nSome libraries may be only available through pip, for example, these may require the generation / update of a requirements.txt:\n$ pip freeze &gt; requirements.txt\nand installing with:\n$ pip install -r requirements.txt\n\n\n\n\n\nMore BC Methods: Further bias correction of UKCP18 products. This is planned for a future release and is not available yet.\nPipeline for adding new methods: This is planned for a future release and is not available yet."
  },
  {
    "objectID": "README.html#table-of-contents",
    "href": "README.html#table-of-contents",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Overview: Bias Correction Pipeline\nDocumentation\nThe Datasets\nWhy Bias Correction?\nContributing\nFuture Plans\nLicense"
  },
  {
    "objectID": "README.html#overview-bias-correction-pipeline",
    "href": "README.html#overview-bias-correction-pipeline",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "clim-recal is a debiasing pipeline, with the following steps:\n\nSet-up & data download We provide custom scripts to facilitate download of data\nPreprocessing This includes reprojecting, resampling & splitting the data prior to bias correction\nApply bias correction Our pipeline embeds two distinct methods of bias correction\nAssess the debiased data We have developed a way to assess the quality of the debiasing step across multiple alternative methods\n\nFor a quick start on bias correction, refer to our comprehensive analysis pipeline guide."
  },
  {
    "objectID": "README.html#documentation",
    "href": "README.html#documentation",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "We are in the process of developing comprehensive documentation for our code base to supplement the guidance provided in this and other README.md files. In the interim, there is documentation available in the following forms:\n\nComments within R scripts\nCommand line --help documentation for some of our python scripts\npython function and class docstrings\nLocal render of documentation via quarto\n\n\n\nFor R scripts, please refer to contextual information and usage guidelines, and feel free to reach out with any specific queries.\n\n\n\nFor many of our python command line scripts, you can use the --help flag to access a summary of the available options and usage information:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n-h, --help            show this help message and exit\n--input INPUT         Path where the .nc files to resample is located\n--output OUTPUT       Path to save the resampled data data\n--grid_data GRID_DATA Path where the .nc file with the grid to resample is located\nThis will display all available options for the script, including their purposes.\n\n\n\nWe also hope to provide comprehensive documentation via quarto. This is a work in progress, but if you would like to render documentation locally you can do so via quarto and conda:\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts."
  },
  {
    "objectID": "README.html#the-datasets",
    "href": "README.html#the-datasets",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "The UK Climate Projections 2018 (UKCP18) dataset offers insights into the potential climate changes in the UK. UKCP18 is an advancement of the UKCP09 projections and delivers the latest evaluations of the UK’s possible climate alterations in land and marine regions throughout the 21st century. This crucial information aids in future Climate Change Risk Assessments and supports the UK’s adaptation to climate change challenges and opportunities as per the National Adaptation Programme.\n\n\n\nHadUK-Grid is a comprehensive collection of climate data for the UK, compiled from various land surface observations across the country. This data is organized into a uniform grid to ensure consistent coverage throughout the UK at up to 1km x 1km resolution. The dataset, spanning from 1836 to the present, includes a variety of climate variables such as air temperature, precipitation, sunshine, and wind speed, available on daily, monthly, seasonal, and annual timescales.\n\n\n\nThe geographical dataset can be used for visualising climate data. It mainly includes administrative boundaries published by the Office for National Statistics (ONS). The dataset is sharable under the Open Government Licence v.3.0 and is available for download via this link. We include a copy in the data/Geofiles folder for convenience. In addition, the clips for three cities’ boundaries from the same dataset are copied to three.cities subfolder."
  },
  {
    "objectID": "README.html#why-bias-correction",
    "href": "README.html#why-bias-correction",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Regional climate models contain systematic errors, or biases in their output [1]. Biases arise in RCMs for a number of reasons, such as the assumptions in the general circulation models (GCMs), and in the downscaling process from GCM to RCM [1,2].\nResearchers, policy-makers and other stakeholders wishing to use publicly available RCMs need to consider a range of “bias correction” methods (sometimes referred to as “bias adjustment” or “recalibration”). Bias correction methods offer a means of adjusting the outputs of RCM in a manner that might better reflect future climate change signals whilst preserving the natural and internal variability of climate [2].\nPart of the clim-recal project is to review several bias correction methods. This work is ongoing and you can find our initial taxonomy here. When we’ve completed our literature review, it will be submitted for publication in an open peer-reviewed journal.\nOur work is however, just like climate data, intended to be dynamic, and we are in the process of setting up a pipeline for researchers creating new methods of bias correction to be able to submit their methods for inclusion on in the clim-recal repository.\n\nSenatore et al., 2022, https://doi.org/10.1016/j.ejrh.2022.101120\nAyar et al., 2021, https://doi.org/10.1038/s41598-021-82715-1"
  },
  {
    "objectID": "README.html#contributing",
    "href": "README.html#contributing",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "We hope to bring together the extensive work already undertaken by the climate science community and showcase a range of libraries and techniques. If you have suggestions on the repository, or would like to include a new method (see below) or library, please raise an issue or get in touch!\n\n\nTo use R in anaconda you may need to specify the conda-forge channel:\n$ conda config --env --add channels conda-forge\nSome libraries may be only available through pip, for example, these may require the generation / update of a requirements.txt:\n$ pip freeze &gt; requirements.txt\nand installing with:\n$ pip install -r requirements.txt"
  },
  {
    "objectID": "README.html#future-plans",
    "href": "README.html#future-plans",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "More BC Methods: Further bias correction of UKCP18 products. This is planned for a future release and is not available yet.\nPipeline for adding new methods: This is planned for a future release and is not available yet."
  },
  {
    "objectID": "R/misc/Identifying_Runs.html",
    "href": "R/misc/Identifying_Runs.html",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Ruth C E Bowyer 2023-06-13\nrm(list=ls())\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n\nScript to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus\n\n\n\nData is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\nY &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above\n\n\n\n\nThe result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06"
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#about",
    "href": "R/misc/Identifying_Runs.html#about",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Script to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus"
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#load-data",
    "href": "R/misc/Identifying_Runs.html#load-data",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Data is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;"
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#comparing-runs",
    "href": "R/misc/Identifying_Runs.html#comparing-runs",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Y &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above"
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#everything-combined",
    "href": "R/misc/Identifying_Runs.html#everything-combined",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "The result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06"
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded for use\n\n\n\ndata_download.ceda_ftp_download.download_ftp\nFunction to connect to the CEDA archive and download data.\n\n\nutils\nUtility functions."
  },
  {
    "objectID": "docs/reference/index.html#data-source-management",
    "href": "docs/reference/index.html#data-source-management",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded for use\n\n\n\ndata_download.ceda_ftp_download.download_ftp\nFunction to connect to the CEDA archive and download data.\n\n\nutils\nUtility functions."
  },
  {
    "objectID": "docs/reference/data_download.ceda_ftp_download.download_ftp.html",
    "href": "docs/reference/data_download.ceda_ftp_download.download_ftp.html",
    "title": "1 data_download.ceda_ftp_download.download_ftp",
    "section": "",
    "text": "ceda_ftp_download.download_ftp(input, output, username, password, order)\nFunction to connect to the CEDA archive and download data.\n\n\nYou need to have a user account and provide your username and FTP password.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired"
  },
  {
    "objectID": "docs/reference/data_download.ceda_ftp_download.download_ftp.html#note",
    "href": "docs/reference/data_download.ceda_ftp_download.download_ftp.html#note",
    "title": "1 data_download.ceda_ftp_download.download_ftp",
    "section": "",
    "text": "You need to have a user account and provide your username and FTP password."
  },
  {
    "objectID": "docs/reference/data_download.ceda_ftp_download.download_ftp.html#parameters",
    "href": "docs/reference/data_download.ceda_ftp_download.download_ftp.html#parameters",
    "title": "1 data_download.ceda_ftp_download.download_ftp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired"
  },
  {
    "objectID": "docs/contributing.html",
    "href": "docs/contributing.html",
    "title": "Contributing to our project",
    "section": "",
    "text": "We welcome contributions to our repository and follow the Turing Way Contributing Guidelines. As the project develops we will expand this section with details more specific to our code base and potential use/application."
  },
  {
    "objectID": "docs/contributing.html#running-quarto-locally",
    "href": "docs/contributing.html#running-quarto-locally",
    "title": "Contributing to our project",
    "section": "1.1 Running Quarto Locally",
    "text": "1.1 Running Quarto Locally\nIf you would like to render documentation locally you can do so via a conda or docker\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.\n\n1.1.1 Locally via conda\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\n\n\n1.1.2 Locally via docker\nIf you have docker installed, you can run a version of the jupyter conifiguration and quarto. The simplest and quickest solution, assuming you have docker running, is:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\nThis should generate local sessions of:\n\njupyter for the python/ model: http://localhost:8888\nquarto documentation: http://localhost:8080"
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-conda",
    "href": "docs/contributing.html#running-tests-in-conda",
    "title": "Contributing to our project",
    "section": "4.1 Running tests in conda",
    "text": "4.1 Running tests in conda\nOnce the conda environment is installed, it should be straightforward to run the basic tests. The following example starts from a fresh clone of the repository:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ cd python         # Currently the tests must be run within the `python` folder\n$ pytest\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: code/clim-recal/python          # Path printed is tweaked here for convenience\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓sss✓✓✓✓✓✓✓✓sss✓                                  82% ████████▎\n utils.py ✓✓✓✓                                                              100% ██████████\nSaved badge to clim-recal/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 -----------\nName                                 Stmts   Miss  Cover\n--------------------------------------------------------\nconftest.py                             32      4    88%\ndata_download/ceda_ftp_download.py      59     59     0%\ndebiasing/preprocess_data.py           134    134     0%\ndebiasing/run_cmethods.py              108    108     0%\nload_data/data_loader.py                83     83     0%\nresampling/check_calendar.py            46     46     0%\nresampling/resampling_hads.py           59     59     0%\ntests/test_debiasing.py                188     27    86%\n--------------------------------------------------------\nTOTAL                                  732    520    29%\n\n5 files skipped due to complete coverage.\n\n=========================== short test summary info ============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n\nResults (0.14s):\n      16 passed\n       6 skipped\n       4 deselected\nThe SKIPPED messages of 6 doctests show they are automatically skipped if the linux server mount is not found, specifically data to test in /mnt/vmfileshare/ClimateData."
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-docker",
    "href": "docs/contributing.html#running-tests-in-docker",
    "title": "Contributing to our project",
    "section": "4.2 Running tests in Docker",
    "text": "4.2 Running tests in Docker\nWith a docker install, tests can be run in two ways. The simplest is via docker compose:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\n$ docker compose exec jupyter bash -c \"conda run -n clim-recal --cwd python pytest\"\nThis mirrors the way tests are run via GitHub Actions for continuous integration on https://github.com/alan-turing-institute/clim-recal.\nTo run tests that require mounting ClimateData (which are not enabled by default), you will need to have a local mount of the relevant drive. This is easiest to achieve by building the compose/Dockerfile separately (not using compose) with that drive mounted.\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker build -f compose/Dockerfile --tag 'clim-recal-test' .\n$ docker run -it -p 8888:8888 -v /Volumes/vmfileshare:/mnt/vmfileshare clim-recal-test .\nThis will print information to the terminal including the link to the new jupyter session in this form:\n[I 2023-11-16 13:46:31.350 ServerApp]     http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url\nBy copying your equivalent of http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url you should be able to get a jupyer instance with all necessary packages installed running in your browser.\nFrom there, you can select a Terminal options under Other to get access to the terminal within your local docker build. You can then change to the python folder and run the tests with the server option to include ClimateData tests as well (note the a-hash-sequence will depend on your build):\n(clim-recal) jovyan@a-hash-sequence:~$ cd python\n(clim-recal) jovyan@a-hash-sequence:~/python$ pytest -m server\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: /home/jovyan/python\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓✓                         100% ██████████\nSaved badge to /home/jovyan/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 ---------\nName                                             Stmts   Miss  Cover\n--------------------------------------------------------------------\nconftest.py                                         32      4    88%\ndata_download/ceda_ftp_download.py                  59     59     0%\ndebiasing/preprocess_data.py                       134     21    84%\ndebiasing/python-cmethods/cmethods/CMethods.py     213    144    32%\ndebiasing/run_cmethods.py                          108      8    93%\nload_data/data_loader.py                            83     83     0%\nresampling/check_calendar.py                        46     46     0%\nresampling/resampling_hads.py                       59     59     0%\ntests/test_debiasing.py                            188      5    97%\nutils.py                                            23      5    78%\n--------------------------------------------------------------------\nTOTAL                                              945    434    54%\n\n5 files skipped due to complete coverage.\n\n\nResults (955.60s (0:15:55)):\n       4 passed\n      22 deselected"
  }
]