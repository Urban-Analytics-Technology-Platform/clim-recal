[
  {
    "objectID": "docs/reference/clim_recal.debiasing.debias_wrapper.html",
    "href": "docs/reference/clim_recal.debiasing.debias_wrapper.html",
    "title": "1 clim_recal.debiasing.debias_wrapper",
    "section": "",
    "text": "clim_recal.debiasing.debias_wrapper\nWrapper for running preprocess_data.py and run_cmethods.py\n\n\n\n\n\nName\nDescription\n\n\n\n\nBaseRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfigType\nParameters needed for a model run.\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.BaseRunConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR, variable=VariableOptions.default(), run=RunOptions.default(), city=CityOptions.default(), method=MethodOptions.default())\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncommand_path\nReturn command path relative to running tests.\n\n\nrun_prefix_tuple\nSplit self.run_prefix by ’ ’ to a tuple.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalib_dates_to_str\nReturn date range as str from calib_date_start to calib_date_end.\n\n\ncmethods_out_path\nReturn path to save cmethods results.\n\n\nmod_path\nReturn city estimates path.\n\n\nobs_path\nReturn city observations path.\n\n\npreprocess_out_path\nReturn path to save results.\n\n\nto_cli_preprocess_str\nGenerate a command line interface str as a test example.\n\n\nto_cli_preprocess_tuple\nGenerate a tuple of str for a command line command.\n\n\nto_cli_preprocess_tuple_strs\nGenerate a command line interface str tuple a test example.\n\n\nto_cli_run_cmethods_str\nGenerate a command line interface str as a test example.\n\n\nto_cli_run_cmethods_tuple\nGenerate a tuple of str for a command line command.\n\n\nto_cli_run_cmethods_tuple_strs\nGenerate a command line interface str tuple a test example.\n\n\nvalid_dates_to_str\nReturn date range as str from valid_date_start to valid_date_end.\n\n\nyield_mod_folder\nIterable of all Paths in self.mod_folder.\n\n\nyield_obs_folder\nIterable of all Paths in self.obs_folder.\n\n\nyield_preprocess_out_folder\nIterable of all Paths in self.preprocess_out_folder.\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.calib_dates_to_str(start_date, end_date, in_format_str=None, out_format_str=None, split_str=None)\nReturn date range as str from calib_date_start to calib_date_end.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.calib_dates_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.calib_dates_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.calib_dates_to_str(date(2010, 1, 1), '20100330', split_str=\"_\")\n'20100101_20100330'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.cmethods_out_path(city=None, run=None)\nReturn path to save cmethods results.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.cmethods_out_path()\nPosixPath('/.../ClimateData/Debiased/three.cities.cropped/Manchester/05')\n&gt;&gt;&gt; config.cmethods_out_path(city='Glasgow', run='07')\nPosixPath('/.../ClimateData/Debiased/three.cities.cropped/Glasgow/07')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.mod_path(city=None)\nReturn city estimates path.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.mod_path()\nPosixPath('/.../ClimateData/Cropped/three.cities/CPM/Manchester')\n&gt;&gt;&gt; config.mod_path('Glasgow')\nPosixPath('/.../ClimateData/Cropped/three.cities/CPM/Glasgow')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.obs_path(city=None)\nReturn city observations path.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.obs_path()\nPosixPath('/.../ClimateData/Cropped/three.cities/Hads.updated360/Manchester')\n&gt;&gt;&gt; config.obs_path('Glasgow')\nPosixPath('/.../ClimateData/Cropped/three.cities/Hads.updated360/Glasgow')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.preprocess_out_path(city=None, run=None, variable=None)\nReturn path to save results.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.preprocess_out_path()\nPosixPath('.../ClimateData/Cropped/three.cities/Preprocessed/Manchester/05/tasmax')\n&gt;&gt;&gt; config.preprocess_out_path(city='Glasgow', run='07')\nPosixPath('.../ClimateData/Cropped/three.cities/Preprocessed/Glasgow/07/tasmax')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_str(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a command line interface str as a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.to_cli_preprocess_str() == CLI_PREPROCESS_DEFAULT_COMMAND_STR_CORRECT\nTrue\n&gt;&gt;&gt; CLI_PREPROCESS_DEFAULT_COMMAND_STR_CORRECT\n'python preprocess_data.py --mod /.../CPM/Manchester...'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_tuple(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a tuple of str for a command line command.\nThis will leave Path objects uncoverted. See self.to_cli_preprocess_tuple_strs for passing to a terminal.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_preprocess_tuple()\n&gt;&gt;&gt; assert command_str_tuple == CLI_PREPROCESS_DEFAULT_COMMAND_TUPLE_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_tuple_strs(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a command line interface str tuple a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_preprocess_tuple_strs()\n&gt;&gt;&gt; assert command_str_tuple == CLI_PREPROCESS_DEFAULT_COMMAND_TUPLE_STR_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_str(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a command line interface str as a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.to_cli_run_cmethods_str() == CLI_CMETHODS_DEFAULT_COMMAND_STR_CORRECT\nTrue\n&gt;&gt;&gt; CLI_CMETHODS_DEFAULT_COMMAND_STR_CORRECT\n'python run_cmethods.py...--method quantile_delta_mapping...'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_tuple(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a tuple of str for a command line command.\nThis will leave Path objects uncoverted. See self.to_cli_run_cmethods_tuple_strs for passing to a terminal.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_run_cmethods_tuple()\n&gt;&gt;&gt; assert command_str_tuple == CLI_CMETHODS_DEFAULT_COMMAND_TUPLE_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_tuple_strs(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a command line interface str tuple a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_run_cmethods_tuple_strs()\n&gt;&gt;&gt; assert command_str_tuple == CLI_CMEHTODS_DEFAULT_COMMAND_TUPLE_STR_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.valid_dates_to_str(start_date, end_date, in_format_str=None, out_format_str=None, split_str=None)\nReturn date range as str from valid_date_start to valid_date_end.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.valid_dates_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.valid_dates_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.valid_dates_to_str(date(2010, 1, 1), '20100330', split_str=\"_\")\n'20100101_20100330'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_mod_folder(city=None)\nIterable of all Paths in self.mod_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; len(tuple(config.yield_mod_folder())) == MOD_FOLDER_FILES_COUNT_CORRECT\nTrue\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_obs_folder(city=None)\nIterable of all Paths in self.obs_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; len(tuple(config.yield_obs_folder())) == OBS_FOLDER_FILES_COUNT_CORRECT\nTrue\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_preprocess_out_folder(city=None, run=None, variable=None)\nIterable of all Paths in self.preprocess_out_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; (len(tuple(config.yield_preprocess_out_folder())) ==\n...  PREPROCESS_OUT_FOLDER_FILES_COUNT_CORRECT)\nTrue\n\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfigType()\nParameters needed for a model run.",
    "crumbs": [
      "python",
      "Reference",
      "Debiasing",
      "Wrapper"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.debiasing.debias_wrapper.html#classes",
    "href": "docs/reference/clim_recal.debiasing.debias_wrapper.html#classes",
    "title": "1 clim_recal.debiasing.debias_wrapper",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBaseRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfig\nManage creating command line scripts to run debiasing cli.\n\n\nRunConfigType\nParameters needed for a model run.\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.BaseRunConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR)\nManage creating command line scripts to run debiasing cli.\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR, variable=VariableOptions.default(), run=RunOptions.default(), city=CityOptions.default(), method=MethodOptions.default())\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\nName\nDescription\n\n\n\n\ncommand_path\nReturn command path relative to running tests.\n\n\nrun_prefix_tuple\nSplit self.run_prefix by ’ ’ to a tuple.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalib_dates_to_str\nReturn date range as str from calib_date_start to calib_date_end.\n\n\ncmethods_out_path\nReturn path to save cmethods results.\n\n\nmod_path\nReturn city estimates path.\n\n\nobs_path\nReturn city observations path.\n\n\npreprocess_out_path\nReturn path to save results.\n\n\nto_cli_preprocess_str\nGenerate a command line interface str as a test example.\n\n\nto_cli_preprocess_tuple\nGenerate a tuple of str for a command line command.\n\n\nto_cli_preprocess_tuple_strs\nGenerate a command line interface str tuple a test example.\n\n\nto_cli_run_cmethods_str\nGenerate a command line interface str as a test example.\n\n\nto_cli_run_cmethods_tuple\nGenerate a tuple of str for a command line command.\n\n\nto_cli_run_cmethods_tuple_strs\nGenerate a command line interface str tuple a test example.\n\n\nvalid_dates_to_str\nReturn date range as str from valid_date_start to valid_date_end.\n\n\nyield_mod_folder\nIterable of all Paths in self.mod_folder.\n\n\nyield_obs_folder\nIterable of all Paths in self.obs_folder.\n\n\nyield_preprocess_out_folder\nIterable of all Paths in self.preprocess_out_folder.\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.calib_dates_to_str(start_date, end_date, in_format_str=None, out_format_str=None, split_str=None)\nReturn date range as str from calib_date_start to calib_date_end.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.calib_dates_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.calib_dates_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.calib_dates_to_str(date(2010, 1, 1), '20100330', split_str=\"_\")\n'20100101_20100330'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.cmethods_out_path(city=None, run=None)\nReturn path to save cmethods results.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.cmethods_out_path()\nPosixPath('/.../ClimateData/Debiased/three.cities.cropped/Manchester/05')\n&gt;&gt;&gt; config.cmethods_out_path(city='Glasgow', run='07')\nPosixPath('/.../ClimateData/Debiased/three.cities.cropped/Glasgow/07')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.mod_path(city=None)\nReturn city estimates path.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.mod_path()\nPosixPath('/.../ClimateData/Cropped/three.cities/CPM/Manchester')\n&gt;&gt;&gt; config.mod_path('Glasgow')\nPosixPath('/.../ClimateData/Cropped/three.cities/CPM/Glasgow')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.obs_path(city=None)\nReturn city observations path.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.obs_path()\nPosixPath('/.../ClimateData/Cropped/three.cities/Hads.updated360/Manchester')\n&gt;&gt;&gt; config.obs_path('Glasgow')\nPosixPath('/.../ClimateData/Cropped/three.cities/Hads.updated360/Glasgow')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.preprocess_out_path(city=None, run=None, variable=None)\nReturn path to save results.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.preprocess_out_path()\nPosixPath('.../ClimateData/Cropped/three.cities/Preprocessed/Manchester/05/tasmax')\n&gt;&gt;&gt; config.preprocess_out_path(city='Glasgow', run='07')\nPosixPath('.../ClimateData/Cropped/three.cities/Preprocessed/Glasgow/07/tasmax')\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_str(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a command line interface str as a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.to_cli_preprocess_str() == CLI_PREPROCESS_DEFAULT_COMMAND_STR_CORRECT\nTrue\n&gt;&gt;&gt; CLI_PREPROCESS_DEFAULT_COMMAND_STR_CORRECT\n'python preprocess_data.py --mod /.../CPM/Manchester...'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_tuple(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a tuple of str for a command line command.\nThis will leave Path objects uncoverted. See self.to_cli_preprocess_tuple_strs for passing to a terminal.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_preprocess_tuple()\n&gt;&gt;&gt; assert command_str_tuple == CLI_PREPROCESS_DEFAULT_COMMAND_TUPLE_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_preprocess_tuple_strs(variable=None, run=None, city=None, calib_start=None, calib_end=None, valid_start=None, valid_end=None)\nGenerate a command line interface str tuple a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_preprocess_tuple_strs()\n&gt;&gt;&gt; assert command_str_tuple == CLI_PREPROCESS_DEFAULT_COMMAND_TUPLE_STR_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_str(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a command line interface str as a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.to_cli_run_cmethods_str() == CLI_CMETHODS_DEFAULT_COMMAND_STR_CORRECT\nTrue\n&gt;&gt;&gt; CLI_CMETHODS_DEFAULT_COMMAND_STR_CORRECT\n'python run_cmethods.py...--method quantile_delta_mapping...'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_tuple(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a tuple of str for a command line command.\nThis will leave Path objects uncoverted. See self.to_cli_run_cmethods_tuple_strs for passing to a terminal.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_run_cmethods_tuple()\n&gt;&gt;&gt; assert command_str_tuple == CLI_CMETHODS_DEFAULT_COMMAND_TUPLE_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.to_cli_run_cmethods_tuple_strs(city=None, run=None, variable=None, method=None, input_data_path=None, cmethods_out_path=None, processors=None)\nGenerate a command line interface str tuple a test example.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; command_str_tuple: tuple[str, ...] = config.to_cli_run_cmethods_tuple_strs()\n&gt;&gt;&gt; assert command_str_tuple == CLI_CMEHTODS_DEFAULT_COMMAND_TUPLE_STR_CORRECT\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.valid_dates_to_str(start_date, end_date, in_format_str=None, out_format_str=None, split_str=None)\nReturn date range as str from valid_date_start to valid_date_end.\n\n\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; config.valid_dates_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.valid_dates_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; config.valid_dates_to_str(date(2010, 1, 1), '20100330', split_str=\"_\")\n'20100101_20100330'\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_mod_folder(city=None)\nIterable of all Paths in self.mod_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; len(tuple(config.yield_mod_folder())) == MOD_FOLDER_FILES_COUNT_CORRECT\nTrue\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_obs_folder(city=None)\nIterable of all Paths in self.obs_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; len(tuple(config.yield_obs_folder())) == OBS_FOLDER_FILES_COUNT_CORRECT\nTrue\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfig.yield_preprocess_out_folder(city=None, run=None, variable=None)\nIterable of all Paths in self.preprocess_out_folder.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip('requires \"vmfileshare/ClimateData\" mounted')\n&gt;&gt;&gt; config: RunConfig = RunConfig()\n&gt;&gt;&gt; (len(tuple(config.yield_preprocess_out_folder())) ==\n...  PREPROCESS_OUT_FOLDER_FILES_COUNT_CORRECT)\nTrue\n\n\n\n\n\n\nclim_recal.debiasing.debias_wrapper.RunConfigType()\nParameters needed for a model run.",
    "crumbs": [
      "python",
      "Reference",
      "Debiasing",
      "Wrapper"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.data_loader.html",
    "href": "docs/reference/clim_recal.data_loader.html",
    "title": "1 clim_recal.data_loader",
    "section": "",
    "text": "clim_recal.data_loader\n\n\n\n\n\nName\nDescription\n\n\n\n\nclip_dataset\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\nload_and_merge\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\nload_data\nThis function takes a date range and a variable and loads and merges\n\n\nreformat_file\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\nclim_recal.data_loader.clip_dataset(xa, variable, shapefile)\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxa\nxarray.DataArray\nxArray containing a given variable (e.g. rainfall)\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nshapefile\nstr\nPath to a shape file used to clip resulting dataset, must be in the same CRS of the input xArray.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA clipped xarray dataset\n\n\n\n\n\n\n\n\n\n\nclim_recal.data_loader.load_and_merge(date_range, files, variable, write_crs_format=BritishNationalGridCoordinates)\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_range\npython.clim_recal.utils.core.DateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nfiles\nlist[str]\nList of strings with path to files to be loaded.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nAn xarray containing all loaded and merged data\n\n\n\n\n\n\n\nclim_recal.data_loader.load_data(input_path, date_range, variable, filter_filenames_on_variable=False, run_number=None, filter_filenames_on_run_number=False, use_pr=False, shapefile_path=None, extension='nc')\nThis function takes a date range and a variable and loads and merges xarrays based on those parameters.\nIf shapefile is provided it crops the data to that region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_path\nstr\nPath to where .nc or .tif files are found\nrequired\n\n\ndate_range\npython.clim_recal.utils.core.DateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nfilter_filenames_on_variable\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “variable” as a substring. When False, filtering does not happen.\nFalse\n\n\nrun_number\nstr | None\nA string representing the CPM run number to use (out of 13 CPM runs available in the database). Only files whose file name contains the substring run_number will be used. If None, all files in input_path are parsed, regardless of run number in filename.\nNone\n\n\nfilter_filenames_on_run_number\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “2.2km_” followed by “run_number”. When False, filtering does not happen. This should only be used for CPM files. For HADs files this should always be set to False.\nFalse\n\n\nuse_pr\nbool\nIf True, replace variable with “pr” string when filtering the file names.\nFalse\n\n\nshapefile_path\nstr | None\nPath to a shape file used to clip resulting dataset.\nNone\n\n\nextension\nstr\nExtension of the files to be loaded, it can be .nc or .tif files.\n'nc'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA DataArray containing all loaded and merged and clipped data\n\n\n\n\n\n\n\nclim_recal.data_loader.reformat_file(file, variable, spatial_config=BritishNationalGridCoordinates)\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile\nstr\nPath as a str to tiff file.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded.\nrequired\n\n\nspatial_config\nstr\nSpatial coordinate configuration for export.\nBritishNationalGridCoordinates\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA formatted xarray",
    "crumbs": [
      "python",
      "Reference",
      "Data Loading"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.data_loader.html#functions",
    "href": "docs/reference/clim_recal.data_loader.html#functions",
    "title": "1 clim_recal.data_loader",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nclip_dataset\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\nload_and_merge\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\nload_data\nThis function takes a date range and a variable and loads and merges\n\n\nreformat_file\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\nclim_recal.data_loader.clip_dataset(xa, variable, shapefile)\nSpatially clip xa (a DataArray) variable via shapefile.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxa\nxarray.DataArray\nxArray containing a given variable (e.g. rainfall)\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nshapefile\nstr\nPath to a shape file used to clip resulting dataset, must be in the same CRS of the input xArray.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA clipped xarray dataset\n\n\n\n\n\n\n\n\n\n\nclim_recal.data_loader.load_and_merge(date_range, files, variable, write_crs_format=BritishNationalGridCoordinates)\nLoad files into xarrays, select a time range and a variable and merge into a sigle xarray.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_range\npython.clim_recal.utils.core.DateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nfiles\nlist[str]\nList of strings with path to files to be loaded.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nAn xarray containing all loaded and merged data\n\n\n\n\n\n\n\nclim_recal.data_loader.load_data(input_path, date_range, variable, filter_filenames_on_variable=False, run_number=None, filter_filenames_on_run_number=False, use_pr=False, shapefile_path=None, extension='nc')\nThis function takes a date range and a variable and loads and merges xarrays based on those parameters.\nIf shapefile is provided it crops the data to that region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_path\nstr\nPath to where .nc or .tif files are found\nrequired\n\n\ndate_range\npython.clim_recal.utils.core.DateRange\nA tuple of datetime objects representing the start and end date\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded\nrequired\n\n\nfilter_filenames_on_variable\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “variable” as a substring. When False, filtering does not happen.\nFalse\n\n\nrun_number\nstr | None\nA string representing the CPM run number to use (out of 13 CPM runs available in the database). Only files whose file name contains the substring run_number will be used. If None, all files in input_path are parsed, regardless of run number in filename.\nNone\n\n\nfilter_filenames_on_run_number\nbool\nWhen True, files in the input_path will be filtered based on whether their file name contains “2.2km_” followed by “run_number”. When False, filtering does not happen. This should only be used for CPM files. For HADs files this should always be set to False.\nFalse\n\n\nuse_pr\nbool\nIf True, replace variable with “pr” string when filtering the file names.\nFalse\n\n\nshapefile_path\nstr | None\nPath to a shape file used to clip resulting dataset.\nNone\n\n\nextension\nstr\nExtension of the files to be loaded, it can be .nc or .tif files.\n'nc'\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA DataArray containing all loaded and merged and clipped data\n\n\n\n\n\n\n\nclim_recal.data_loader.reformat_file(file, variable, spatial_config=BritishNationalGridCoordinates)\nLoad tif file and reformat xarray into expected format.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile\nstr\nPath as a str to tiff file.\nrequired\n\n\nvariable\nstr\nA string representing the variable to be loaded.\nrequired\n\n\nspatial_config\nstr\nSpatial coordinate configuration for export.\nBritishNationalGridCoordinates\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.DataArray\nA formatted xarray",
    "crumbs": [
      "python",
      "Reference",
      "Data Loading"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html",
    "href": "docs/reference/clim_recal.utils.core.html",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "clim_recal.utils.core\nUtility functions.\n\n\n\n\n\nName\nDescription\n\n\n\n\nMonthDay\nA class to ease generating annual time series.\n\n\n\n\n\nclim_recal.utils.core.MonthDay(self, month=1, day=1, format_str=ISO_DATE_FORMAT_STR)\nA class to ease generating annual time series.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmonth\nint\nWhat Month as an integer from 1 to 12.\n\n\nday\nint\nWhat day in the month, an integer from 1 to 31.\n\n\nformat_str\nstr\nFormat to use if generating a str.\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year(1980)\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980')\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980', as_str=True)\n'1980-01-01'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_year\nReturn a date or str of date given year.\n\n\nfrom_year_range_to_str\nReturn an annual range str.\n\n\n\n\n\nclim_recal.utils.core.MonthDay.from_year(year, as_str=False)\nReturn a date or str of date given year.\n\n\n\nclim_recal.utils.core.MonthDay.from_year_range_to_str(start_year, end_year, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nReturn an annual range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nend_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nsplit_str\nstr\nstr between start_date and end_date generated.\nDATE_FORMAT_SPLIT_STR\n\n\ninclude_end_date\nbool\nWhether to include the end_date in the final str. If False follow python convention to return the day prior.\nTrue\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year_range_to_str(1980, 1982, include_end_date=False)\n'19800101-19811231'\n&gt;&gt;&gt; jan_1.from_year_range_to_str('1980', 2020)\n'19800101-20200101'\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nannual_data_path\nReturn Path of annual data files.\n\n\nannual_data_paths_generator\nYield Path of annual data files.\n\n\ncheck_package_path\nReturn path for test running.\n\n\nclimate_data_mount_path\nReturn likely climate data mount path based on operating system.\n\n\ncsv_reader\nYield a dict per row from a CSV file at path.\n\n\ndate_range_generator\nReturn a tuple of DateType objects.\n\n\ndate_range_to_str\nTake start_date and end_date and return a date range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\nensure_date\nEnsure passed date_to_check is a date.\n\n\nis_climate_data_mounted\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nis_platform_darwin\nCheck if sys.platform is Darwin (macOS).\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmultiprocess_execute\nRun method_name as from iter via multiprocessing.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nproduct_dict\nReturn product combinatorics of kwargs.\n\n\nresults_path\nReturn Path: path/name_time.extension.\n\n\nrun_callable_attr\nExtract method_name from instance to call.\n\n\ntime_str\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\nclim_recal.utils.core.annual_data_path(start_year=1980, end_year=1981, month_day=DEFAULT_START_MONTH_DAY, include_end_date=False, parent_path=None, file_name_middle_str=CPM_FILE_NAME_MIDDLE_STR, file_name_extension=NETCDF_EXTENSION, data_type=MAX_TEMP_STR, make_paths=False)\nReturn Path of annual data files.\n\n\n&gt;&gt;&gt; str(annual_data_path(parent_path=Path('test/path')))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.annual_data_paths_generator(start_year=1980, end_year=1986, **kwargs)\nYield Path of annual data files.\n\n\n&gt;&gt;&gt; pprint(tuple(annual_data_paths_generator()))\n(PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19811201-19821130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19821201-19831130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19831201-19841130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19841201-19851130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19851201-19861130.nc'))\n&gt;&gt;&gt; parent_path_example: Iterator[Path] = annual_data_paths_generator(\n...     parent_path=Path('test/path'))\n&gt;&gt;&gt; str(next(parent_path_example))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.check_package_path(strict=True, try_chdir=False)\nReturn path for test running.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrict\nbool\nWhether to raise a ValueError if check fails.\nTrue\n\n\ntry_chdir\nbool\nWhether to attempt changing directory if initial check fails\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf strict and checks fail.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nWhether to check if call was successful.\n\n\n\n\n\n\n\n&gt;&gt;&gt; check_package_path()\nTrue\n&gt;&gt;&gt; chdir('..')\n&gt;&gt;&gt; check_package_path(strict=False)\nFalse\n&gt;&gt;&gt; check_package_path()\nTraceback (most recent call last):\n    ...\nValueError: 'clim-recal' pipeline must be run in...\n&gt;&gt;&gt; check_package_path(try_chdir=True)\nTrue\n\n\n\n\nclim_recal.utils.core.climate_data_mount_path(is_darwin=None, full_path=True)\nReturn likely climate data mount path based on operating system.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nis_darwin\nbool | None\nWhether to use CLIMATE_DATA_MOUNT_PATH_DARWIN or call is_platform_darwin if None. fixture is_platform_darwin.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe Path climate data would likely be mounted to.\n\n\n\n\n\n\n\n\nclim_recal.utils.core.csv_reader(path, **kwargs)\nYield a dict per row from a CSV file at path.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nos.PathLike\nCSV file Path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for csv.DictReader.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; csv_path: Path = TEST_AUTH_CSV_PATH\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\n&gt;&gt;&gt; tuple(csv_reader(csv_path))\n({'user_name': 'sally', 'password': 'fig*new£kid'},\n {'user_name': 'george', 'password': 'tee&iguana*sky'},\n {'user_name': 'susan', 'password': 'history!bill-walk'})\n\n\n\n\nclim_recal.utils.core.date_range_generator(start_date, end_date, inclusive=False, skip_dates=None, start_format_str=CLI_DATE_FORMAT_STR, end_format_str=CLI_DATE_FORMAT_STR, output_format_str=CLI_DATE_FORMAT_STR, skip_dates_format_str=CLI_DATE_FORMAT_STR, yield_type=date)\nReturn a tuple of DateType objects.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\npython.clim_recal.utils.core.DateType\nDateType at start of time series.\nrequired\n\n\nend_date\npython.clim_recal.utils.core.DateType\nDateType at end of time series.\nrequired\n\n\ninclusive\nbool\nWhether to include the end_date in the returned time series.\nFalse\n\n\nskip_dates\ntyping.Iterable[python.clim_recal.utils.core.DateType] | python.clim_recal.utils.core.DateType | None\nDates to skip between start_date and end_date.\nNone\n\n\nstart_format_str\nstr\nA strftime format to apply if start_date type is str.\nCLI_DATE_FORMAT_STR\n\n\nend_format_str\nstr\nA strftime format to apply if end_date type is str.\nCLI_DATE_FORMAT_STR\n\n\noutput_format_str\nstr\nA strftime format to apply if yield_type is str.\nCLI_DATE_FORMAT_STR\n\n\nskip_dates_format_str\nstr\nA strftime format to apply if any skip_dates are str.\nCLI_DATE_FORMAT_STR\n\n\nyield_type\ntype[datetime.date] | type[str]\nWhether which date type to return in tuple (date or str).\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Iterator[python.clim_recal.utils.core.DateType]\nA tuple of date or str objects (only one type throughout).\n\n\n\n\n\n\n&gt;&gt;&gt; four_years: tuple[date] = tuple(date_range_generator('19801130', '19841130'))\n&gt;&gt;&gt; len(four_years)\n1461\n&gt;&gt;&gt; four_years_inclusive: tuple[date] = tuple(\n...     date_range_generator('1980-11-30', '19841130',\n...                          inclusive=True,\n...                          start_format_str=ISO_DATE_FORMAT_STR))\n&gt;&gt;&gt; len(four_years_inclusive)\n1462\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = tuple(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates='19840229'))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1461\n&gt;&gt;&gt; skip_dates: tuple[date] = (date(1981, 12, 1), date(1982, 12, 1))\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = list(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates=skip_dates))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1460\n&gt;&gt;&gt; skip_dates in four_years_inclusive_skip\nFalse\n\n\n\n\nclim_recal.utils.core.date_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nTake start_date and end_date and return a date range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\npython.clim_recal.utils.core.DateType\nFirst date in range.\nrequired\n\n\nend_date\npython.clim_recal.utils.core.DateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str of date range from start_date to end_date in the out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330', include_end_date=False)\n'20100101-20100329'\n\n\n\n\nclim_recal.utils.core.date_to_str(date_obj, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\npython.clim_recal.utils.core.DateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str version of date_obj in out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\nclim_recal.utils.core.ensure_date(date_to_check, format_str=CLI_DATE_FORMAT_STR)\nEnsure passed date_to_check is a date.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_to_check\npython.clim_recal.utils.core.DateType\nDate or str to ensure is a date.\nrequired\n\n\nformat_str\nstr\nstrptime str to convert date_to_check if necessary.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndate instance from date_to_check.\n\n\n\n\n\n\n\n&gt;&gt;&gt; ensure_date('19801130')\ndatetime.date(1980, 11, 30)\n&gt;&gt;&gt; ensure_date(date(1980, 11, 30))\ndatetime.date(1980, 11, 30)\n\n\n\n\nclim_recal.utils.core.is_climate_data_mounted(mount_path=None)\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nConsider refactoring to climate_data_mount_path returns None when conditions here return False.\n\n\n\n\nclim_recal.utils.core.is_platform_darwin()\nCheck if sys.platform is Darwin (macOS).\n\n\n\nclim_recal.utils.core.iter_to_tuple_strs(iter_var, func=str)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\ntyping.Iterable[typing.Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\nfunc\ntyping.Callable[[typing.Any], str]\nCallable to convert each obj in iter_val to a str.\nstr\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[str, …]\nA tuple of all iter_var elements in str format.\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n&gt;&gt;&gt; iter_to_tuple_strs(\n...     ['cat', 1, Path('a/path')],\n...     lambda x: f'{x:02}' if type(x) == int else str(x))\n('cat', '01', 'a/path')\n\n\n\n\nclim_recal.utils.core.multiprocess_execute(iter, method_name=None, cpus=None)\nRun method_name as from iter via multiprocessing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter\ntyping.Sequence\nSequence of instances to iterate over calling method_name from.\nrequired\n\n\nmethod_name\nstr | None\nWhat to call from objects in inter.\nNone\n\n\ncpus\nint | None\nNumber of cpus to pass to Pool for multiprocessing.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; from clim_recal.resample import CPMResampler\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     stop_index=3,\n...     output_path=resample_test_cpm_output_path,\n... )\n&gt;&gt;&gt; multiprocess_execute(cpm_resampler, method_name=\"exists\")\n[True, True, True]\n\n\n\nFailed asserting cpus &lt;= total - 1\n\n\n\n\nclim_recal.utils.core.path_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nos.PathLike\nPath to folder to iterate through.\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Generator[typing.Optional[pathlib.Path], None, None]\nNone if FileNotFoundError error and strict is False.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nclim_recal.utils.core.product_dict(**kwargs)\nReturn product combinatorics of kwargs.\n\n\n&gt;&gt;&gt; pprint(tuple(\n...     product_dict(animal=['cat', 'fish'], activity=('swim', 'eat'))))\n({'activity': 'swim', 'animal': 'cat'},\n {'activity': 'eat', 'animal': 'cat'},\n {'activity': 'swim', 'animal': 'fish'},\n {'activity': 'eat', 'animal': 'fish'})\n\n\n\n\nclim_recal.utils.core.results_path(name, path=None, time=None, extension=None, mkdir=False)\nReturn Path: path/name_time.extension.\n\n\n&gt;&gt;&gt; str(results_path('hads', 'test_example/folder', extension='cat'))\n'test_example/folder/hads_..._...-....cat'\n\n\n\n\nclim_recal.utils.core.run_callable_attr(instance, method_name='execute', *args, **kwargs)\nExtract method_name from instance to call.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nobject\nobject to call method_name from.\nrequired\n\n\nmethod_name\nstr\nName of method on object to call.\n'execute'\n\n\n*args\n\nParameters passed to method_name from instance.\n()\n\n\n**kwargs\n\nParameters passed to method_name from instance.\n{}\n\n\n\n\n\n\nThis is primarily meant to address issues with pickle, particularly when using multiprocessing and lambda functions yield pickle errors.\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; run_callable_attr(jan_1, 'from_year', 1984)\ndatetime.date(1984, 1, 1)\n\n\n\n\nclim_recal.utils.core.time_str(time=None, format=RUN_TIME_STAMP_FORMAT, replace_char_tuple=None)\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\ndatetime.date | datetime.datetime | None\nTime to format. Will be set to current time if None is passed, and add current time if a date is passed to convert to a datetime.\nNone\n\n\nformat\nstr\nstrftime str format to return time as. If replace_chars is passed, these will be used to replace those in strftime.\nRUN_TIME_STAMP_FORMAT\n\n\nreplace_char_tuple\ntuple[str, str] | None\ntuple of (char_to_match, char_to_replace) order.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; time_str(datetime(2024, 10, 10, 10, 10, 10))\n'24-10-10_10-10'",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html#classes",
    "href": "docs/reference/clim_recal.utils.core.html#classes",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nMonthDay\nA class to ease generating annual time series.\n\n\n\n\n\nclim_recal.utils.core.MonthDay(self, month=1, day=1, format_str=ISO_DATE_FORMAT_STR)\nA class to ease generating annual time series.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmonth\nint\nWhat Month as an integer from 1 to 12.\n\n\nday\nint\nWhat day in the month, an integer from 1 to 31.\n\n\nformat_str\nstr\nFormat to use if generating a str.\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year(1980)\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980')\ndatetime.date(1980, 1, 1)\n&gt;&gt;&gt; jan_1.from_year('1980', as_str=True)\n'1980-01-01'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_year\nReturn a date or str of date given year.\n\n\nfrom_year_range_to_str\nReturn an annual range str.\n\n\n\n\n\nclim_recal.utils.core.MonthDay.from_year(year, as_str=False)\nReturn a date or str of date given year.\n\n\n\nclim_recal.utils.core.MonthDay.from_year_range_to_str(start_year, end_year, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nReturn an annual range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nend_year\nint | str\nStarting year to combine with self.month and self.day.\nrequired\n\n\nsplit_str\nstr\nstr between start_date and end_date generated.\nDATE_FORMAT_SPLIT_STR\n\n\ninclude_end_date\nbool\nWhether to include the end_date in the final str. If False follow python convention to return the day prior.\nTrue\n\n\n\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; jan_1.from_year_range_to_str(1980, 1982, include_end_date=False)\n'19800101-19811231'\n&gt;&gt;&gt; jan_1.from_year_range_to_str('1980', 2020)\n'19800101-20200101'",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.core.html#functions",
    "href": "docs/reference/clim_recal.utils.core.html#functions",
    "title": "1 clim_recal.utils.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nannual_data_path\nReturn Path of annual data files.\n\n\nannual_data_paths_generator\nYield Path of annual data files.\n\n\ncheck_package_path\nReturn path for test running.\n\n\nclimate_data_mount_path\nReturn likely climate data mount path based on operating system.\n\n\ncsv_reader\nYield a dict per row from a CSV file at path.\n\n\ndate_range_generator\nReturn a tuple of DateType objects.\n\n\ndate_range_to_str\nTake start_date and end_date and return a date range str.\n\n\ndate_to_str\nReturn a str in date_format_str of date_obj.\n\n\nensure_date\nEnsure passed date_to_check is a date.\n\n\nis_climate_data_mounted\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nis_platform_darwin\nCheck if sys.platform is Darwin (macOS).\n\n\niter_to_tuple_strs\nReturn a tuple with all components converted to strs.\n\n\nmultiprocess_execute\nRun method_name as from iter via multiprocessing.\n\n\npath_iterdir\nReturn an Generator after ensuring path exists.\n\n\nproduct_dict\nReturn product combinatorics of kwargs.\n\n\nresults_path\nReturn Path: path/name_time.extension.\n\n\nrun_callable_attr\nExtract method_name from instance to call.\n\n\ntime_str\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\nclim_recal.utils.core.annual_data_path(start_year=1980, end_year=1981, month_day=DEFAULT_START_MONTH_DAY, include_end_date=False, parent_path=None, file_name_middle_str=CPM_FILE_NAME_MIDDLE_STR, file_name_extension=NETCDF_EXTENSION, data_type=MAX_TEMP_STR, make_paths=False)\nReturn Path of annual data files.\n\n\n&gt;&gt;&gt; str(annual_data_path(parent_path=Path('test/path')))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.annual_data_paths_generator(start_year=1980, end_year=1986, **kwargs)\nYield Path of annual data files.\n\n\n&gt;&gt;&gt; pprint(tuple(annual_data_paths_generator()))\n(PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19811201-19821130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19821201-19831130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19831201-19841130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19841201-19851130.nc'),\n PosixPath('tasmax_rcp85_land-cpm_uk_2.2km_01_day_19851201-19861130.nc'))\n&gt;&gt;&gt; parent_path_example: Iterator[Path] = annual_data_paths_generator(\n...     parent_path=Path('test/path'))\n&gt;&gt;&gt; str(next(parent_path_example))\n'test/path/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19801201-19811130.nc'\n\n\n\n\nclim_recal.utils.core.check_package_path(strict=True, try_chdir=False)\nReturn path for test running.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrict\nbool\nWhether to raise a ValueError if check fails.\nTrue\n\n\ntry_chdir\nbool\nWhether to attempt changing directory if initial check fails\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nIf strict and checks fail.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nWhether to check if call was successful.\n\n\n\n\n\n\n\n&gt;&gt;&gt; check_package_path()\nTrue\n&gt;&gt;&gt; chdir('..')\n&gt;&gt;&gt; check_package_path(strict=False)\nFalse\n&gt;&gt;&gt; check_package_path()\nTraceback (most recent call last):\n    ...\nValueError: 'clim-recal' pipeline must be run in...\n&gt;&gt;&gt; check_package_path(try_chdir=True)\nTrue\n\n\n\n\nclim_recal.utils.core.climate_data_mount_path(is_darwin=None, full_path=True)\nReturn likely climate data mount path based on operating system.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nis_darwin\nbool | None\nWhether to use CLIMATE_DATA_MOUNT_PATH_DARWIN or call is_platform_darwin if None. fixture is_platform_darwin.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe Path climate data would likely be mounted to.\n\n\n\n\n\n\n\n\nclim_recal.utils.core.csv_reader(path, **kwargs)\nYield a dict per row from a CSV file at path.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nos.PathLike\nCSV file Path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for csv.DictReader.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; csv_path: Path = TEST_AUTH_CSV_PATH\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\n&gt;&gt;&gt; tuple(csv_reader(csv_path))\n({'user_name': 'sally', 'password': 'fig*new£kid'},\n {'user_name': 'george', 'password': 'tee&iguana*sky'},\n {'user_name': 'susan', 'password': 'history!bill-walk'})\n\n\n\n\nclim_recal.utils.core.date_range_generator(start_date, end_date, inclusive=False, skip_dates=None, start_format_str=CLI_DATE_FORMAT_STR, end_format_str=CLI_DATE_FORMAT_STR, output_format_str=CLI_DATE_FORMAT_STR, skip_dates_format_str=CLI_DATE_FORMAT_STR, yield_type=date)\nReturn a tuple of DateType objects.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\npython.clim_recal.utils.core.DateType\nDateType at start of time series.\nrequired\n\n\nend_date\npython.clim_recal.utils.core.DateType\nDateType at end of time series.\nrequired\n\n\ninclusive\nbool\nWhether to include the end_date in the returned time series.\nFalse\n\n\nskip_dates\ntyping.Iterable[python.clim_recal.utils.core.DateType] | python.clim_recal.utils.core.DateType | None\nDates to skip between start_date and end_date.\nNone\n\n\nstart_format_str\nstr\nA strftime format to apply if start_date type is str.\nCLI_DATE_FORMAT_STR\n\n\nend_format_str\nstr\nA strftime format to apply if end_date type is str.\nCLI_DATE_FORMAT_STR\n\n\noutput_format_str\nstr\nA strftime format to apply if yield_type is str.\nCLI_DATE_FORMAT_STR\n\n\nskip_dates_format_str\nstr\nA strftime format to apply if any skip_dates are str.\nCLI_DATE_FORMAT_STR\n\n\nyield_type\ntype[datetime.date] | type[str]\nWhether which date type to return in tuple (date or str).\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Iterator[python.clim_recal.utils.core.DateType]\nA tuple of date or str objects (only one type throughout).\n\n\n\n\n\n\n&gt;&gt;&gt; four_years: tuple[date] = tuple(date_range_generator('19801130', '19841130'))\n&gt;&gt;&gt; len(four_years)\n1461\n&gt;&gt;&gt; four_years_inclusive: tuple[date] = tuple(\n...     date_range_generator('1980-11-30', '19841130',\n...                          inclusive=True,\n...                          start_format_str=ISO_DATE_FORMAT_STR))\n&gt;&gt;&gt; len(four_years_inclusive)\n1462\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = tuple(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates='19840229'))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1461\n&gt;&gt;&gt; skip_dates: tuple[date] = (date(1981, 12, 1), date(1982, 12, 1))\n&gt;&gt;&gt; four_years_inclusive_skip: tuple[date] = list(\n...     date_range_generator('19801130', '19841130',\n...                          inclusive=True,\n...                          skip_dates=skip_dates))\n&gt;&gt;&gt; len(four_years_inclusive_skip)\n1460\n&gt;&gt;&gt; skip_dates in four_years_inclusive_skip\nFalse\n\n\n\n\nclim_recal.utils.core.date_range_to_str(start_date, end_date, split_str=DATE_FORMAT_SPLIT_STR, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR, include_end_date=True)\nTake start_date and end_date and return a date range str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\npython.clim_recal.utils.core.DateType\nFirst date in range.\nrequired\n\n\nend_date\npython.clim_recal.utils.core.DateType\nLast date in range\nrequired\n\n\nsplit_str\nstr\nchar to split returned date range str.\nDATE_FORMAT_SPLIT_STR\n\n\nin_format_str\nstr\nA strftime format str to convert start_date from.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert end_date from.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str of date range from start_date to end_date in the out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_range_to_str('20100101', '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330')\n'20100101-20100330'\n&gt;&gt;&gt; date_range_to_str(date(2010, 1, 1), '20100330', include_end_date=False)\n'20100101-20100329'\n\n\n\n\nclim_recal.utils.core.date_to_str(date_obj, in_format_str=CLI_DATE_FORMAT_STR, out_format_str=CLI_DATE_FORMAT_STR)\nReturn a str in date_format_str of date_obj.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_obj\npython.clim_recal.utils.core.DateType\nA datetime.date or str object to convert.\nrequired\n\n\nin_format_str\nstr\nA strftime format str to convert date_obj from if date_obj is a str.\nCLI_DATE_FORMAT_STR\n\n\nout_format_str\nstr\nA strftime format str to convert date_obj to.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA str version of date_obj in out_format_str format.\n\n\n\n\n\n\n&gt;&gt;&gt; date_to_str('20100101')\n'20100101'\n&gt;&gt;&gt; date_to_str(date(2010, 1, 1))\n'20100101'\n\n\n\n\nclim_recal.utils.core.ensure_date(date_to_check, format_str=CLI_DATE_FORMAT_STR)\nEnsure passed date_to_check is a date.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndate_to_check\npython.clim_recal.utils.core.DateType\nDate or str to ensure is a date.\nrequired\n\n\nformat_str\nstr\nstrptime str to convert date_to_check if necessary.\nCLI_DATE_FORMAT_STR\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndate instance from date_to_check.\n\n\n\n\n\n\n\n&gt;&gt;&gt; ensure_date('19801130')\ndatetime.date(1980, 11, 30)\n&gt;&gt;&gt; ensure_date(date(1980, 11, 30))\ndatetime.date(1980, 11, 30)\n\n\n\n\nclim_recal.utils.core.is_climate_data_mounted(mount_path=None)\nCheck if CLIMATE_DATA_MOUNT_PATH is mounted.\n\n\nConsider refactoring to climate_data_mount_path returns None when conditions here return False.\n\n\n\n\nclim_recal.utils.core.is_platform_darwin()\nCheck if sys.platform is Darwin (macOS).\n\n\n\nclim_recal.utils.core.iter_to_tuple_strs(iter_var, func=str)\nReturn a tuple with all components converted to strs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter_var\ntyping.Iterable[typing.Any]\nIterable of objects that can be converted into strs.\nrequired\n\n\nfunc\ntyping.Callable[[typing.Any], str]\nCallable to convert each obj in iter_val to a str.\nstr\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple[str, …]\nA tuple of all iter_var elements in str format.\n\n\n\n\n\n\n&gt;&gt;&gt; iter_to_tuple_strs(['cat', 1, Path('a/path')])\n('cat', '1', 'a/path')\n&gt;&gt;&gt; iter_to_tuple_strs(\n...     ['cat', 1, Path('a/path')],\n...     lambda x: f'{x:02}' if type(x) == int else str(x))\n('cat', '01', 'a/path')\n\n\n\n\nclim_recal.utils.core.multiprocess_execute(iter, method_name=None, cpus=None)\nRun method_name as from iter via multiprocessing.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\niter\ntyping.Sequence\nSequence of instances to iterate over calling method_name from.\nrequired\n\n\nmethod_name\nstr | None\nWhat to call from objects in inter.\nNone\n\n\ncpus\nint | None\nNumber of cpus to pass to Pool for multiprocessing.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; from clim_recal.resample import CPMResampler\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     stop_index=3,\n...     output_path=resample_test_cpm_output_path,\n... )\n&gt;&gt;&gt; multiprocess_execute(cpm_resampler, method_name=\"exists\")\n[True, True, True]\n\n\n\nFailed asserting cpus &lt;= total - 1\n\n\n\n\nclim_recal.utils.core.path_iterdir(path, strict=False)\nReturn an Generator after ensuring path exists.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nos.PathLike\nPath to folder to iterate through.\nrequired\n\n\nstrict\nbool\nWhether to raise FileNotFoundError if path not found.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFileNotFoundError\nRaised if strict = True and path does not exist.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Generator[typing.Optional[pathlib.Path], None, None]\nNone if FileNotFoundError error and strict is False.\n\n\n\n\n\n\n&gt;&gt;&gt; tmp_path = getfixture('tmp_path')\n&gt;&gt;&gt; from os import chdir\n&gt;&gt;&gt; chdir(tmp_path)\n&gt;&gt;&gt; example_path: Path = Path('a/test/path')\n&gt;&gt;&gt; example_path.exists()\nFalse\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent, strict=True))\nTraceback (most recent call last):\n    ...\nFileNotFoundError: [Errno 2] No such file or directory: 'a/test'\n&gt;&gt;&gt; example_path.parent.mkdir(parents=True)\n&gt;&gt;&gt; example_path.touch()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n(PosixPath('a/test/path'),)\n&gt;&gt;&gt; example_path.unlink()\n&gt;&gt;&gt; tuple(path_iterdir(example_path.parent))\n()\n\n\n\n\nclim_recal.utils.core.product_dict(**kwargs)\nReturn product combinatorics of kwargs.\n\n\n&gt;&gt;&gt; pprint(tuple(\n...     product_dict(animal=['cat', 'fish'], activity=('swim', 'eat'))))\n({'activity': 'swim', 'animal': 'cat'},\n {'activity': 'eat', 'animal': 'cat'},\n {'activity': 'swim', 'animal': 'fish'},\n {'activity': 'eat', 'animal': 'fish'})\n\n\n\n\nclim_recal.utils.core.results_path(name, path=None, time=None, extension=None, mkdir=False)\nReturn Path: path/name_time.extension.\n\n\n&gt;&gt;&gt; str(results_path('hads', 'test_example/folder', extension='cat'))\n'test_example/folder/hads_..._...-....cat'\n\n\n\n\nclim_recal.utils.core.run_callable_attr(instance, method_name='execute', *args, **kwargs)\nExtract method_name from instance to call.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\nobject\nobject to call method_name from.\nrequired\n\n\nmethod_name\nstr\nName of method on object to call.\n'execute'\n\n\n*args\n\nParameters passed to method_name from instance.\n()\n\n\n**kwargs\n\nParameters passed to method_name from instance.\n{}\n\n\n\n\n\n\nThis is primarily meant to address issues with pickle, particularly when using multiprocessing and lambda functions yield pickle errors.\n\n\n\n&gt;&gt;&gt; jan_1: MonthDay = MonthDay()\n&gt;&gt;&gt; run_callable_attr(jan_1, 'from_year', 1984)\ndatetime.date(1984, 1, 1)\n\n\n\n\nclim_recal.utils.core.time_str(time=None, format=RUN_TIME_STAMP_FORMAT, replace_char_tuple=None)\nReturn a str of passed or generated time suitable for a file name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime\ndatetime.date | datetime.datetime | None\nTime to format. Will be set to current time if None is passed, and add current time if a date is passed to convert to a datetime.\nNone\n\n\nformat\nstr\nstrftime str format to return time as. If replace_chars is passed, these will be used to replace those in strftime.\nRUN_TIME_STAMP_FORMAT\n\n\nreplace_char_tuple\ntuple[str, str] | None\ntuple of (char_to_match, char_to_replace) order.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; time_str(datetime(2024, 10, 10, 10, 10, 10))\n'24-10-10_10-10'",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.config.html",
    "href": "docs/reference/clim_recal.config.html",
    "title": "1 clim_recal.config",
    "section": "",
    "text": "clim_recal.config\n\n\n\n\n\nName\nDescription\n\n\n\n\nClimRecalConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalRunsConfigType\nLists of parameters to generate RunConfigType instances.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR, variables=(VariableOptions.default()), runs=(RunOptions.default()), cities=(CityOptions.default()), methods=(MethodOptions.default()), multiprocess=False, cpus=DEFAULT_CPUS, output_path=DEFAULT_OUTPUT_PATH, resample_folder=DEFAULT_RESAMPLE_FOLDER, hads_folder=HADS_OUTPUT_LOCAL_PATH, cpm_folder=CPM_OUTPUT_LOCAL_PATH, cpm_kwargs=dict(), hads_kwargs=dict(), start_index=0, stop_index=None, add_local_dated_results_path=True, local_dated_results_path_prefix='run')\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvariables\ntyping.Sequence[python.clim_recal.utils.data.VariableOptions]\nVariables to include in the model, eg. tasmax, tasmin.\n\n\nruns\ntyping.Sequence[python.clim_recal.utils.data.RunOptions]\nWhich model runs to include, eg. “01”, “08”, “11”.\n\n\ncities\ntyping.Sequence[python.clim_recal.debiasing.debias_wrapper.CityOptions] | None\nWhich cities to crop data to. Future plans facilitate skipping to run for entire UK.\n\n\nmethods\ntyping.Sequence[python.clim_recal.debiasing.debias_wrapper.MethodOptions]\nWhich debiasing methods to apply.\n\n\nmultiprocess\nbool\nWhether to use multiprocess where available\n\n\ncpus\nint | None\nNumber of cpus to use if multiprocessing\n\n\noutput_path\nos.PathLike\nPath to save all intermediate and final results to.\n\n\nresample_folder\nos.PathLike\nPath to append to output_path for resampling result files.\n\n\nhads_folder\nos.PathLike\nPath to append to output_path / resample_folder for resampling HADs files.\n\n\ncpm_folder\nos.PathLike\nPath to append to output_path / resample_folder for resampling CPM files.\n\n\ncpm_kwargs\ndict\nA dict of parameters to pass to a CPMResamplerManager.\n\n\nhads_kwargs\ndict\nA dict of parameters to pass to HADsResamplerManager.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; run_config: ClimRecalConfig = ClimRecalConfig(\n...     cities=('Manchester', 'Glasgow'),\n...     output_path=test_runs_output_path,\n...     cpus=1)\n&gt;&gt;&gt; run_config\n&lt;ClimRecalConfig(variables_count=1, runs_count=1, cities_count=2,\n                 methods_count=1, cpm_folders_count=1,\n                 hads_folders_count=1, start_index=0,\n                 stop_index=None, cpus=1)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nrun_models\nRun all specified models.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig.run_models()\nRun all specified models.\n\n\n&gt;&gt;&gt; runs: dict[tuple, dict] = clim_runner.run_models()\n&gt;&gt;&gt; pprint(tuple(runs.keys()))\n(('Glasgow', 'tasmax', '05', 'quantile_delta_mapping'),\n ('Manchester', 'tasmax', '05', 'quantile_delta_mapping'))\n\n\n\n\n\n\nclim_recal.config.ClimRecalRunsConfigType()\nLists of parameters to generate RunConfigType instances.",
    "crumbs": [
      "python",
      "Reference",
      "Configure"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.config.html#classes",
    "href": "docs/reference/clim_recal.config.html#classes",
    "title": "1 clim_recal.config",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nClimRecalConfig\nManage creating command line scripts to run debiasing cli.\n\n\nClimRecalRunsConfigType\nLists of parameters to generate RunConfigType instances.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig(self, command_dir=COMMAND_DIR_DEFAULT, run_prefix=RUN_PREFIX_DEFAULT, preprocess_data_file=PREPROCESS_FILE_NAME, run_cmethods_file=CMETHODS_FILE_NAME, data_path=DATA_PATH_DEFAULT, mod_folder=MOD_FOLDER_DEFAULT, obs_folder=OBS_FOLDER_DEFAULT, preprocess_out_folder=PREPROCESS_OUT_FOLDER_DEFAULT, cmethods_out_folder=CMETHODS_OUT_FOLDER_DEFAULT, calib_date_start=CALIB_DATE_START_DEFAULT, calib_date_end=CALIB_DATE_END_DEFAULT, valid_date_start=VALID_DATE_START_DEFAULT, valid_date_end=VALID_DATE_END_DEFAULT, processors=PROCESSESORS_DEFAULT, date_format_str=CLI_DATE_FORMAT_STR, date_split_str=DATE_FORMAT_SPLIT_STR, variables=(VariableOptions.default()), runs=(RunOptions.default()), cities=(CityOptions.default()), methods=(MethodOptions.default()), multiprocess=False, cpus=DEFAULT_CPUS, output_path=DEFAULT_OUTPUT_PATH, resample_folder=DEFAULT_RESAMPLE_FOLDER, hads_folder=HADS_OUTPUT_LOCAL_PATH, cpm_folder=CPM_OUTPUT_LOCAL_PATH, cpm_kwargs=dict(), hads_kwargs=dict(), start_index=0, stop_index=None, add_local_dated_results_path=True, local_dated_results_path_prefix='run')\nManage creating command line scripts to run debiasing cli.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvariables\ntyping.Sequence[python.clim_recal.utils.data.VariableOptions]\nVariables to include in the model, eg. tasmax, tasmin.\n\n\nruns\ntyping.Sequence[python.clim_recal.utils.data.RunOptions]\nWhich model runs to include, eg. “01”, “08”, “11”.\n\n\ncities\ntyping.Sequence[python.clim_recal.debiasing.debias_wrapper.CityOptions] | None\nWhich cities to crop data to. Future plans facilitate skipping to run for entire UK.\n\n\nmethods\ntyping.Sequence[python.clim_recal.debiasing.debias_wrapper.MethodOptions]\nWhich debiasing methods to apply.\n\n\nmultiprocess\nbool\nWhether to use multiprocess where available\n\n\ncpus\nint | None\nNumber of cpus to use if multiprocessing\n\n\noutput_path\nos.PathLike\nPath to save all intermediate and final results to.\n\n\nresample_folder\nos.PathLike\nPath to append to output_path for resampling result files.\n\n\nhads_folder\nos.PathLike\nPath to append to output_path / resample_folder for resampling HADs files.\n\n\ncpm_folder\nos.PathLike\nPath to append to output_path / resample_folder for resampling CPM files.\n\n\ncpm_kwargs\ndict\nA dict of parameters to pass to a CPMResamplerManager.\n\n\nhads_kwargs\ndict\nA dict of parameters to pass to HADsResamplerManager.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; run_config: ClimRecalConfig = ClimRecalConfig(\n...     cities=('Manchester', 'Glasgow'),\n...     output_path=test_runs_output_path,\n...     cpus=1)\n&gt;&gt;&gt; run_config\n&lt;ClimRecalConfig(variables_count=1, runs_count=1, cities_count=2,\n                 methods_count=1, cpm_folders_count=1,\n                 hads_folders_count=1, start_index=0,\n                 stop_index=None, cpus=1)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nrun_models\nRun all specified models.\n\n\n\n\n\nclim_recal.config.ClimRecalConfig.run_models()\nRun all specified models.\n\n\n&gt;&gt;&gt; runs: dict[tuple, dict] = clim_runner.run_models()\n&gt;&gt;&gt; pprint(tuple(runs.keys()))\n(('Glasgow', 'tasmax', '05', 'quantile_delta_mapping'),\n ('Manchester', 'tasmax', '05', 'quantile_delta_mapping'))\n\n\n\n\n\n\nclim_recal.config.ClimRecalRunsConfigType()\nLists of parameters to generate RunConfigType instances.",
    "crumbs": [
      "python",
      "Reference",
      "Configure"
    ]
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded for use\n\n\n\nclim_recal.pipeline\nWrappers to automate the entire pipeline.\n\n\nclim_recal.ceda_ftp_download\n\n\n\nclim_recal.data_loader\n\n\n\nclim_recal.config\n\n\n\nclim_recal.resample\nResample UKHADS data and UKCP18 data.\n\n\nclim_recal.debiasing.debias_wrapper\nWrapper for running preprocess_data.py and run_cmethods.py\n\n\nclim_recal.utils.core\nUtility functions.\n\n\nclim_recal.utils.server\nUtility functions.\n\n\nclim_recal.utils.xarray\n\n\n\nclim_recal.utils.data",
    "crumbs": [
      "python",
      "Reference"
    ]
  },
  {
    "objectID": "docs/reference/index.html#data-source-management",
    "href": "docs/reference/index.html#data-source-management",
    "title": "1 Function reference",
    "section": "",
    "text": "How data is downloaded for use\n\n\n\nclim_recal.pipeline\nWrappers to automate the entire pipeline.\n\n\nclim_recal.ceda_ftp_download\n\n\n\nclim_recal.data_loader\n\n\n\nclim_recal.config\n\n\n\nclim_recal.resample\nResample UKHADS data and UKCP18 data.\n\n\nclim_recal.debiasing.debias_wrapper\nWrapper for running preprocess_data.py and run_cmethods.py\n\n\nclim_recal.utils.core\nUtility functions.\n\n\nclim_recal.utils.server\nUtility functions.\n\n\nclim_recal.utils.xarray\n\n\n\nclim_recal.utils.data",
    "crumbs": [
      "python",
      "Reference"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html",
    "href": "docs/reference/clim_recal.utils.xarray.html",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "clim_recal.utils.xarray\n\n\n\n\n\nName\nDescription\n\n\n\n\nBoundsTupleType\nGeoPandas bounds: (minx, miny, maxx, maxy).\n\n\nChangeDayType\nA set of tuples of month and day numbers for enforce_date_changes.\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.\n\n\nMONTH_DAY_XARRAY_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 leap years.\n\n\nMONTH_DAY_XARRAY_NO_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 non leap years.\n\n\nTHREE_CITY_CENTRE_COORDS\nCity centre (lon, lat) tuple coords of Glasgow, Manchester and London.\n\n\nXArrayEngineType\nEngine types supported by xarray as str.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nIntermediateCPMFilesManager\nManage intermediate files and paths for CPM calendar projection.\n\n\n\n\n\nclim_recal.utils.xarray.IntermediateCPMFilesManager(self, variable_name, output_path, time_series_start, time_series_end, subfolder_path=CPM_LOCAL_INTERMEDIATE_PATH, file_name_prefix='', subfolder_time_stamp=False)\nManage intermediate files and paths for CPM calendar projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable_name\nstr\nName of variable (e.g. tasmax). Included in file names.\nrequired\n\n\noutput_path\npathlib.Path | None\nFolder to save results to.\nrequired\n\n\nsubfolder_path\npathlib.Path\nPath to place intermediate files relative to output_path.\nCPM_LOCAL_INTERMEDIATE_PATH\n\n\ntime_series_start\ndatetime.datetime | datetime.date | cftime._cftime.Datetime360Day\nStart of time series covered to include in file name.\nrequired\n\n\ntime_series_end\ndatetime.datetime | datetime.date | cftime._cftime.Datetime360Day\nEnd of time series covered to include in file name.\nrequired\n\n\nfile_name_prefix\nstr\nstr to add at the start of the output file names (after integers).\n''\n\n\nsubfolder_time_stamp\nbool\nWhether to include a time stamp in the subfolder file name.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; test_path = getfixture('tmp_path')\n&gt;&gt;&gt; intermedate_files_manager: IntermediateCPMFilesManager = IntermediateCPMFilesManager(\n...     file_name_prefix=\"test-1980\",\n...     variable_name=\"tasmax\",\n...     output_path=test_path / 'intermediate_test_folder',\n...     subfolder_path='test_subfolder',\n...     time_series_start=date(1980, 12, 1),\n...     time_series_end=date(1981, 11, 30),\n... )\n&gt;&gt;&gt; str(intermedate_files_manager.output_path)\n'.../intermediate_test_folder'\n&gt;&gt;&gt; str(intermedate_files_manager.intermediate_files_folder)\n'.../test_subfolder'\n&gt;&gt;&gt; str(intermedate_files_manager.final_nc_path)\n'.../test_subfolder/3-test-1980-tasmax-19801201-19811130-cpm-365-or-366-27700-final.nc'\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\napply_geo_func\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\ncftime360_to_date\nConvert a Datetime360Day into a date.\n\n\ncftime_range_gen\nConvert a banded time index a banded standard (Gregorian).\n\n\nconvert_xr_calendar\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\ncorrect_int_time_datafile\nLoad a Dataset from path and generate time index.\n\n\ncpm_reproject_with_standard_calendar\nConvert raw cpm_xr_time_series to an 365/366 days and 27700 coords.\n\n\ncpm_xarray_to_standard_calendar\nConvert a CPM nc file of 360 day calendars to standard calendar.\n\n\ncrop_nc\nCrop xr_time_series with crop_path shapefile.\n\n\nensure_xr_dataset\nReturn xr_time_series as a xarray.Dataset instance.\n\n\nfile_name_to_start_end_dates\nReturn dates of file name with date_format-date_format structure.\n\n\ngdal_warp_wrapper\nExecute the gdalwrap function within python.\n\n\ngenerate_360_to_standard\nReturn array_to_expand 360 days expanded to 365 or 366 days.\n\n\ninterpolate_coords\nReproject xr_time_series to x_resolution/y_resolution.\n\n\ninterpolate_xr_ts_nans\nInterpolate nan values in a Dataset time series.\n\n\nxarray_example\nGenerate spatial and temporal xarray objects.\n\n\n\n\n\nclim_recal.utils.xarray.apply_geo_func(source_path, func, export_folder, new_path_name_func=None, to_netcdf=True, to_raster=False, export_path_as_output_path_kwarg=False, return_results=False, **kwargs)\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_path\nos.PathLike\nnetcdf file to apply func to.\nrequired\n\n\nfunc\ntyping.Callable[[xarray.Dataset], xarray.Dataset]\nCallable to modify netcdf.\nrequired\n\n\nexport_folder\nos.PathLike\nWhere to save results.\nrequired\n\n\npath_name_replace_tuple\n\nOptional replacement str to apply to source_path.name when exporting\nrequired\n\n\nto_netcdf\nbool\nWhether to call to_netcdf method on results Dataset.\nTrue\n\n\n\n\n\n\n\nclim_recal.utils.xarray.cftime360_to_date(cf_360)\nConvert a Datetime360Day into a date.\n\n\n&gt;&gt;&gt; cftime360_to_date(Datetime360Day(1980, 1, 1))\ndatetime.date(1980, 1, 1)\n\n\n\n\nclim_recal.utils.xarray.cftime_range_gen(time_data_array, **kwargs)\nConvert a banded time index a banded standard (Gregorian).\n\n\n\nclim_recal.utils.xarray.convert_xr_calendar(xr_time_series, align_on=DEFAULT_CALENDAR_ALIGN, calendar=CFCalendarSTANDARD, use_cftime=False, missing_value=np.nan, interpolate_na=False, ensure_output_type_is_dataset=False, interpolate_method=DEFAULT_INTERPOLATION_METHOD, keep_crs=True, keep_attrs=True, limit=1, engine=NETCDF4_XARRAY_ENGINE, extrapolate_fill_value=True, check_cftime_cols=None, cftime_range_gen_kwargs=None, **kwargs)\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\nShort time examples (like 2 skipped out of 8 days) raises: ValueError(\"date_range_like was unable to generate a range as the source frequency was not inferable.\")\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxr_time_series\nxarray.DataArray | xarray.Dataset | os.PathLike\nA DataArray or Dataset to convert to calendar time.\nrequired\n\n\nalign_on\npython.clim_recal.utils.xarray.ConvertCalendarAlignOptions\nWhether and how to align calendar types.\nDEFAULT_CALENDAR_ALIGN\n\n\ncalendar\nxarray.core.types.CFCalendar\nType of calendar to convert xr_time_series to.\nCFCalendarSTANDARD\n\n\nuse_cftime\nbool\nWhether to enforce cftime vs datetime64 time format.\nFalse\n\n\nmissing_value\ntyping.Any | None\nMissing value to populate missing date interpolations with.\nnp.nan\n\n\nkeep_crs\nbool\nReapply initial Coordinate Reference System (CRS) after time projection.\nTrue\n\n\ninterpolate_na\nbool\nWhether to apply temporal interpolation for missing values.\nFalse\n\n\ninterpolate_method\nxarray.core.types.InterpOptions\nWhich InterpOptions method to apply if interpolate_na is True.\nDEFAULT_INTERPOLATION_METHOD\n\n\nkeep_attrs\nbool\nWhether to keep all attributes on after interpolate_na\nTrue\n\n\nlimit\nint\nLimit of number of continuous missing day values allowed in interpolate_na.\n1\n\n\nengine\npython.clim_recal.utils.xarray.XArrayEngineType\nWhich XArrayEngineType to use in parsing files and operations.\nNETCDF4_XARRAY_ENGINE\n\n\nextrapolate_fill_value\nbool\nIf True, then pass fill_value=extrapolate. See: * https://docs.xarray.dev/en/stable/generated/xarray.Dataset.interpolate_na.html * https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d\nTrue\n\n\ncheck_cftime_cols\ntuple[str] | None\nColumns to check cftime format on\nNone\n\n\ncftime_range_gen_kwargs\ndict[str, typing.Any] | None\nAny kwargs to pass to cftime_range_gen\nNone\n\n\n**kwargs\n\nAny additional parameters to pass to interpolate_na.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nLikely from xarray calling date_range_like.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.Dataset | xarray.DataArray\nConverted xr_time_series to specified calendar with optional interpolation.\n\n\n\n\n\n\nCertain values may fail to interpolate in cases of 360 -&gt; 365/366 (Gregorian) calendar. Examples include projecting CPM data, which is able to fill in measurement values (e.g. tasmax) but the year and month_number variables have nan values",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html#attributes",
    "href": "docs/reference/clim_recal.utils.xarray.html#attributes",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nBoundsTupleType\nGeoPandas bounds: (minx, miny, maxx, maxy).\n\n\nChangeDayType\nA set of tuples of month and day numbers for enforce_date_changes.\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.\n\n\nMONTH_DAY_XARRAY_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 leap years.\n\n\nMONTH_DAY_XARRAY_NO_LEAP_YEAR_DROP\nA set of month and day tuples dropped for xarray.day_360 non leap years.\n\n\nTHREE_CITY_CENTRE_COORDS\nCity centre (lon, lat) tuple coords of Glasgow, Manchester and London.\n\n\nXArrayEngineType\nEngine types supported by xarray as str.",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html#classes",
    "href": "docs/reference/clim_recal.utils.xarray.html#classes",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nIntermediateCPMFilesManager\nManage intermediate files and paths for CPM calendar projection.\n\n\n\n\n\nclim_recal.utils.xarray.IntermediateCPMFilesManager(self, variable_name, output_path, time_series_start, time_series_end, subfolder_path=CPM_LOCAL_INTERMEDIATE_PATH, file_name_prefix='', subfolder_time_stamp=False)\nManage intermediate files and paths for CPM calendar projection.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable_name\nstr\nName of variable (e.g. tasmax). Included in file names.\nrequired\n\n\noutput_path\npathlib.Path | None\nFolder to save results to.\nrequired\n\n\nsubfolder_path\npathlib.Path\nPath to place intermediate files relative to output_path.\nCPM_LOCAL_INTERMEDIATE_PATH\n\n\ntime_series_start\ndatetime.datetime | datetime.date | cftime._cftime.Datetime360Day\nStart of time series covered to include in file name.\nrequired\n\n\ntime_series_end\ndatetime.datetime | datetime.date | cftime._cftime.Datetime360Day\nEnd of time series covered to include in file name.\nrequired\n\n\nfile_name_prefix\nstr\nstr to add at the start of the output file names (after integers).\n''\n\n\nsubfolder_time_stamp\nbool\nWhether to include a time stamp in the subfolder file name.\nFalse\n\n\n\n\n\n\n&gt;&gt;&gt; test_path = getfixture('tmp_path')\n&gt;&gt;&gt; intermedate_files_manager: IntermediateCPMFilesManager = IntermediateCPMFilesManager(\n...     file_name_prefix=\"test-1980\",\n...     variable_name=\"tasmax\",\n...     output_path=test_path / 'intermediate_test_folder',\n...     subfolder_path='test_subfolder',\n...     time_series_start=date(1980, 12, 1),\n...     time_series_end=date(1981, 11, 30),\n... )\n&gt;&gt;&gt; str(intermedate_files_manager.output_path)\n'.../intermediate_test_folder'\n&gt;&gt;&gt; str(intermedate_files_manager.intermediate_files_folder)\n'.../test_subfolder'\n&gt;&gt;&gt; str(intermedate_files_manager.final_nc_path)\n'.../test_subfolder/3-test-1980-tasmax-19801201-19811130-cpm-365-or-366-27700-final.nc'",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.xarray.html#functions",
    "href": "docs/reference/clim_recal.utils.xarray.html#functions",
    "title": "1 clim_recal.utils.xarray",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napply_geo_func\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\ncftime360_to_date\nConvert a Datetime360Day into a date.\n\n\ncftime_range_gen\nConvert a banded time index a banded standard (Gregorian).\n\n\nconvert_xr_calendar\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\ncorrect_int_time_datafile\nLoad a Dataset from path and generate time index.\n\n\ncpm_reproject_with_standard_calendar\nConvert raw cpm_xr_time_series to an 365/366 days and 27700 coords.\n\n\ncpm_xarray_to_standard_calendar\nConvert a CPM nc file of 360 day calendars to standard calendar.\n\n\ncrop_nc\nCrop xr_time_series with crop_path shapefile.\n\n\nensure_xr_dataset\nReturn xr_time_series as a xarray.Dataset instance.\n\n\nfile_name_to_start_end_dates\nReturn dates of file name with date_format-date_format structure.\n\n\ngdal_warp_wrapper\nExecute the gdalwrap function within python.\n\n\ngenerate_360_to_standard\nReturn array_to_expand 360 days expanded to 365 or 366 days.\n\n\ninterpolate_coords\nReproject xr_time_series to x_resolution/y_resolution.\n\n\ninterpolate_xr_ts_nans\nInterpolate nan values in a Dataset time series.\n\n\nxarray_example\nGenerate spatial and temporal xarray objects.\n\n\n\n\n\nclim_recal.utils.xarray.apply_geo_func(source_path, func, export_folder, new_path_name_func=None, to_netcdf=True, to_raster=False, export_path_as_output_path_kwarg=False, return_results=False, **kwargs)\nApply a Callable to netcdf_source file and export via to_netcdf.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_path\nos.PathLike\nnetcdf file to apply func to.\nrequired\n\n\nfunc\ntyping.Callable[[xarray.Dataset], xarray.Dataset]\nCallable to modify netcdf.\nrequired\n\n\nexport_folder\nos.PathLike\nWhere to save results.\nrequired\n\n\npath_name_replace_tuple\n\nOptional replacement str to apply to source_path.name when exporting\nrequired\n\n\nto_netcdf\nbool\nWhether to call to_netcdf method on results Dataset.\nTrue\n\n\n\n\n\n\n\nclim_recal.utils.xarray.cftime360_to_date(cf_360)\nConvert a Datetime360Day into a date.\n\n\n&gt;&gt;&gt; cftime360_to_date(Datetime360Day(1980, 1, 1))\ndatetime.date(1980, 1, 1)\n\n\n\n\nclim_recal.utils.xarray.cftime_range_gen(time_data_array, **kwargs)\nConvert a banded time index a banded standard (Gregorian).\n\n\n\nclim_recal.utils.xarray.convert_xr_calendar(xr_time_series, align_on=DEFAULT_CALENDAR_ALIGN, calendar=CFCalendarSTANDARD, use_cftime=False, missing_value=np.nan, interpolate_na=False, ensure_output_type_is_dataset=False, interpolate_method=DEFAULT_INTERPOLATION_METHOD, keep_crs=True, keep_attrs=True, limit=1, engine=NETCDF4_XARRAY_ENGINE, extrapolate_fill_value=True, check_cftime_cols=None, cftime_range_gen_kwargs=None, **kwargs)\nConvert cpm 360 day time series to a standard 365/366 day time series.\n\n\nShort time examples (like 2 skipped out of 8 days) raises: ValueError(\"date_range_like was unable to generate a range as the source frequency was not inferable.\")\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nxr_time_series\nxarray.DataArray | xarray.Dataset | os.PathLike\nA DataArray or Dataset to convert to calendar time.\nrequired\n\n\nalign_on\npython.clim_recal.utils.xarray.ConvertCalendarAlignOptions\nWhether and how to align calendar types.\nDEFAULT_CALENDAR_ALIGN\n\n\ncalendar\nxarray.core.types.CFCalendar\nType of calendar to convert xr_time_series to.\nCFCalendarSTANDARD\n\n\nuse_cftime\nbool\nWhether to enforce cftime vs datetime64 time format.\nFalse\n\n\nmissing_value\ntyping.Any | None\nMissing value to populate missing date interpolations with.\nnp.nan\n\n\nkeep_crs\nbool\nReapply initial Coordinate Reference System (CRS) after time projection.\nTrue\n\n\ninterpolate_na\nbool\nWhether to apply temporal interpolation for missing values.\nFalse\n\n\ninterpolate_method\nxarray.core.types.InterpOptions\nWhich InterpOptions method to apply if interpolate_na is True.\nDEFAULT_INTERPOLATION_METHOD\n\n\nkeep_attrs\nbool\nWhether to keep all attributes on after interpolate_na\nTrue\n\n\nlimit\nint\nLimit of number of continuous missing day values allowed in interpolate_na.\n1\n\n\nengine\npython.clim_recal.utils.xarray.XArrayEngineType\nWhich XArrayEngineType to use in parsing files and operations.\nNETCDF4_XARRAY_ENGINE\n\n\nextrapolate_fill_value\nbool\nIf True, then pass fill_value=extrapolate. See: * https://docs.xarray.dev/en/stable/generated/xarray.Dataset.interpolate_na.html * https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d\nTrue\n\n\ncheck_cftime_cols\ntuple[str] | None\nColumns to check cftime format on\nNone\n\n\ncftime_range_gen_kwargs\ndict[str, typing.Any] | None\nAny kwargs to pass to cftime_range_gen\nNone\n\n\n**kwargs\n\nAny additional parameters to pass to interpolate_na.\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValueError\nLikely from xarray calling date_range_like.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nxarray.Dataset | xarray.DataArray\nConverted xr_time_series to specified calendar with optional interpolation.\n\n\n\n\n\n\nCertain values may fail to interpolate in cases of 360 -&gt; 365/366 (Gregorian) calendar. Examples include projecting CPM data, which is able to fill in measurement values (e.g. tasmax) but the year and month_number variables have nan values",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "xarray"
    ]
  },
  {
    "objectID": "docs/datasets.html",
    "href": "docs/datasets.html",
    "title": "Exported Datasets",
    "section": "",
    "text": "Thanks for your interest in the clim-recal data and metrics. We plan to release a set of combined observation and simulation datasets for three cities in the UK and invite authors of bias correction methods to benchmark their methods for those cities.\nWe are still completing this work. As soon as the data are ready for publication, we will update this page with information on how to access them - check back soon. In the meantime, please check our blog article describing this work.\n\n\n\n Back to top"
  },
  {
    "objectID": "python/README.html",
    "href": "python/README.html",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The raw UKHAD observational data needs to be resampled to the same grid of the RCP8.5 data. This can be done with the python/resampling/resampling_hads.py script, which takes an input grid and uses to resample the data using linear interpolation (for simplicity have added a default grid in data/rcp85_land-cpm_uk_2.2km_grid.nc).\nThe script runs under the conda environment created on the main README.md and has several options that can be understood by running the following from the resampling directory:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n  -h, --help            show this help message and exit\n  --input INPUT         Path where the .nc files to resample is located\n  --output OUTPUT       Path to save the resampled data data\n  --grid_data GRID_DATA\n                        Path where the .nc file with the grid to resample is located\nThe script expects the data to be files of .nc extension, have dimensions named projection_x_coordinate and projection_y_coordinate and to follow the format of the CEDA Archive.\nFurthermore, the layer/variable to be resampled must be on the beginning of the name of the file before any _ (e.g for tasmax is tasmax_hadukgrid_uk_1km_day_19930501-19930531.nc).\n\n\nFor example, to run the resampling on tasmax daily data found in the fileshare (https://dymestorage1.file.core.windows.net/vmfileshare).\n$ cd python/resampling\n$ python resampling_hads.py --input /Volumes/vmfileshare/ClimateData/Raw/HadsUKgrid/tasmax/day --output &lt;local-directory-path&gt;\nas there is not a --grid_data flag, the default file described above is used.\n\n\n\n\nIn [python/clim_recal/data_loader.py] we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb.\n\n\n\nThe code in the debiasing directory contains scripts that interface with implementations of the debiasing methods implemented by different libraries.\nNote: By March 2023 we have only implemented the python-cmethods library.\n\n\nThis repository contains two python scripts one for preprocessing/grouping data and one to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\n$ cd debiasing\n$ git submodule update --init --recursive\n\nThe preprocess_data.py script allows the user to specify directories from which the modelled (CPM/UKCP) data and observation (HADs) data should be loaded, as well as time periods to use for calibration and validation. The script parses the necessary files and combines them into two files for calibration (modelled and observed), and two files for validation (modelled and observed) - with the option to specify multiple validation periods. These can then be used as inputs to run_cmethods.py.\nThe run_cmethods.py script allow us to adjust climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data) and modelled data (historical CPM/UKCP data) for calibration, as well as observation and modelled data for validation (generated by preprocess_data.py). It calibrates the debiasing method using the calibration period data and debiases the modelled data for the validation period. The resulting output is saved as a .nc to a specified directory. The script will also produce a time-series and a map plot of the debiased data.\n\nUsage:\nThe scripts can be run from the command line using the following arguments:\n$ python3 preprocess_data.py --mod &lt;path to modelled datasets&gt; --obs &lt;path to observation datasets&gt; --shp &lt;shapefile&gt; --out &lt;output file path&gt; -v &lt;variable&gt; -u &lt;unit&gt; -r &lt;CPM model run number&gt; --calib_dates &lt;date range for calibration&gt; --valid_dates &lt;date range for validation&gt;\n\n$ python3 run_cmethods.py --input_data_folder &lt;input files directory&gt; --out &lt;output directory&gt; -m &lt;method&gt; -v &lt;variable&gt; -g &lt;group&gt; -k &lt;kind&gt; -n &lt;number of quantiles&gt; -p &lt;number of processes&gt;\nFor more details on the scripts and options you can run:\n$ python3 preprocess_data.py --help\nand\npython3 run_cmethods.py --help\nMain Functionality:\nThe preprocess_data.py script performs the following steps:\n\nParses the input arguments.\nLoads, merges and clips (if shapefile is provided) all calibration datasets and merges them into two distinct datasets: the m modelled and observed datasets.\nAligns the calendars of the two datasets, ensuring that they have the same time dimension.\nSaves the calibration datasets to the output directory.\nLoops over the validation time periods specified in the calib_dates variable and performs the following steps:\n\nLoads the modelled data for the current time period.\nLoads the observed data for the current time period.\nAligns and saves the datasets to the output directory.\n\n\nThe run_cmethods.py script performs the following steps: - Reads the input calibration and validation datasets from the input directory. - Applies the specified debiasing method, combining the calibration and valiation data. - Saves the resulting output to the specified directory. - Creates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\nWorking example.\nExample of how to run the two scripts using data stored in the Azure fileshare, running the scripts locally (uses input data that have been cropped to contain only the city of Glasgow. The two scripts will debias only the tasmax variable, run 05 of the CPM, for calibration years 1980-2009 and validation years 2010-2019. It uses the quantile_delta_mapping debiasing method:\n$ python3 preprocess_data.py --mod /Volumes/vmfileshare/ClimateData/Cropped/three.cities/CPM/Glasgow/ --obs /Volumes/vmfileshare/ClimateData/Cropped/three.cities/Hads.original360/Glasgow/ -v tasmax --out ./preprocessed_data/ --calib_dates 19800101-20091230 --valid_dates 20100101-20191230 --run_number 05\n\n$ python3 run_cmethods.py --input_data_folder ./preprocessed_data/  --out ./debiased_data/  --method quantile_delta_mapping --v tasmax -p 4\n\n\n\n\nTesting for python components uses pytest, with configuration specified in pyproject.toml. To run tests, ensure the conda-lock.yml environment is installed and activated, then run pytest from within the clim-recal/python checkout directory. Note: some tests are skipped unless run on a specific linux server wth data mounted to a specific path.\n$ cd clim-recal\n$ conda-lock install --name clim-recal conda-lock.yml\n$ conda activate clim-recal\n(clim-recal)$ cd python\n(clim-recal)$ pytest\n...sss........sss.....                                                         [100%]\n============================== short test summary info ===============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_mod_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n16 passed, 6 skipped, 4 deselected in 0.26s",
    "crumbs": [
      "python"
    ]
  },
  {
    "objectID": "python/README.html#resampling-hads-grid-from-1-km-to-2.2-km",
    "href": "python/README.html#resampling-hads-grid-from-1-km-to-2.2-km",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The raw UKHAD observational data needs to be resampled to the same grid of the RCP8.5 data. This can be done with the python/resampling/resampling_hads.py script, which takes an input grid and uses to resample the data using linear interpolation (for simplicity have added a default grid in data/rcp85_land-cpm_uk_2.2km_grid.nc).\nThe script runs under the conda environment created on the main README.md and has several options that can be understood by running the following from the resampling directory:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n  -h, --help            show this help message and exit\n  --input INPUT         Path where the .nc files to resample is located\n  --output OUTPUT       Path to save the resampled data data\n  --grid_data GRID_DATA\n                        Path where the .nc file with the grid to resample is located\nThe script expects the data to be files of .nc extension, have dimensions named projection_x_coordinate and projection_y_coordinate and to follow the format of the CEDA Archive.\nFurthermore, the layer/variable to be resampled must be on the beginning of the name of the file before any _ (e.g for tasmax is tasmax_hadukgrid_uk_1km_day_19930501-19930531.nc).\n\n\nFor example, to run the resampling on tasmax daily data found in the fileshare (https://dymestorage1.file.core.windows.net/vmfileshare).\n$ cd python/resampling\n$ python resampling_hads.py --input /Volumes/vmfileshare/ClimateData/Raw/HadsUKgrid/tasmax/day --output &lt;local-directory-path&gt;\nas there is not a --grid_data flag, the default file described above is used.",
    "crumbs": [
      "python"
    ]
  },
  {
    "objectID": "python/README.html#loading-ukcp-and-hads-data",
    "href": "python/README.html#loading-ukcp-and-hads-data",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "In [python/clim_recal/data_loader.py] we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb.",
    "crumbs": [
      "python"
    ]
  },
  {
    "objectID": "python/README.html#running-debiasing-methods",
    "href": "python/README.html#running-debiasing-methods",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "The code in the debiasing directory contains scripts that interface with implementations of the debiasing methods implemented by different libraries.\nNote: By March 2023 we have only implemented the python-cmethods library.\n\n\nThis repository contains two python scripts one for preprocessing/grouping data and one to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\n$ cd debiasing\n$ git submodule update --init --recursive\n\nThe preprocess_data.py script allows the user to specify directories from which the modelled (CPM/UKCP) data and observation (HADs) data should be loaded, as well as time periods to use for calibration and validation. The script parses the necessary files and combines them into two files for calibration (modelled and observed), and two files for validation (modelled and observed) - with the option to specify multiple validation periods. These can then be used as inputs to run_cmethods.py.\nThe run_cmethods.py script allow us to adjust climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data) and modelled data (historical CPM/UKCP data) for calibration, as well as observation and modelled data for validation (generated by preprocess_data.py). It calibrates the debiasing method using the calibration period data and debiases the modelled data for the validation period. The resulting output is saved as a .nc to a specified directory. The script will also produce a time-series and a map plot of the debiased data.\n\nUsage:\nThe scripts can be run from the command line using the following arguments:\n$ python3 preprocess_data.py --mod &lt;path to modelled datasets&gt; --obs &lt;path to observation datasets&gt; --shp &lt;shapefile&gt; --out &lt;output file path&gt; -v &lt;variable&gt; -u &lt;unit&gt; -r &lt;CPM model run number&gt; --calib_dates &lt;date range for calibration&gt; --valid_dates &lt;date range for validation&gt;\n\n$ python3 run_cmethods.py --input_data_folder &lt;input files directory&gt; --out &lt;output directory&gt; -m &lt;method&gt; -v &lt;variable&gt; -g &lt;group&gt; -k &lt;kind&gt; -n &lt;number of quantiles&gt; -p &lt;number of processes&gt;\nFor more details on the scripts and options you can run:\n$ python3 preprocess_data.py --help\nand\npython3 run_cmethods.py --help\nMain Functionality:\nThe preprocess_data.py script performs the following steps:\n\nParses the input arguments.\nLoads, merges and clips (if shapefile is provided) all calibration datasets and merges them into two distinct datasets: the m modelled and observed datasets.\nAligns the calendars of the two datasets, ensuring that they have the same time dimension.\nSaves the calibration datasets to the output directory.\nLoops over the validation time periods specified in the calib_dates variable and performs the following steps:\n\nLoads the modelled data for the current time period.\nLoads the observed data for the current time period.\nAligns and saves the datasets to the output directory.\n\n\nThe run_cmethods.py script performs the following steps: - Reads the input calibration and validation datasets from the input directory. - Applies the specified debiasing method, combining the calibration and valiation data. - Saves the resulting output to the specified directory. - Creates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\nWorking example.\nExample of how to run the two scripts using data stored in the Azure fileshare, running the scripts locally (uses input data that have been cropped to contain only the city of Glasgow. The two scripts will debias only the tasmax variable, run 05 of the CPM, for calibration years 1980-2009 and validation years 2010-2019. It uses the quantile_delta_mapping debiasing method:\n$ python3 preprocess_data.py --mod /Volumes/vmfileshare/ClimateData/Cropped/three.cities/CPM/Glasgow/ --obs /Volumes/vmfileshare/ClimateData/Cropped/three.cities/Hads.original360/Glasgow/ -v tasmax --out ./preprocessed_data/ --calib_dates 19800101-20091230 --valid_dates 20100101-20191230 --run_number 05\n\n$ python3 run_cmethods.py --input_data_folder ./preprocessed_data/  --out ./debiased_data/  --method quantile_delta_mapping --v tasmax -p 4",
    "crumbs": [
      "python"
    ]
  },
  {
    "objectID": "python/README.html#testing",
    "href": "python/README.html#testing",
    "title": "1 Methods implemented in Python",
    "section": "",
    "text": "Testing for python components uses pytest, with configuration specified in pyproject.toml. To run tests, ensure the conda-lock.yml environment is installed and activated, then run pytest from within the clim-recal/python checkout directory. Note: some tests are skipped unless run on a specific linux server wth data mounted to a specific path.\n$ cd clim-recal\n$ conda-lock install --name clim-recal conda-lock.yml\n$ conda activate clim-recal\n(clim-recal)$ cd python\n(clim-recal)$ pytest\n...sss........sss.....                                                         [100%]\n============================== short test summary info ===============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_mod_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n16 passed, 6 skipped, 4 deselected in 0.26s",
    "crumbs": [
      "python"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "rm(list=ls())\n\nknitr::opts_knit$set(root.dir=\"/Volumes/vmfileshare/ClimateData/\")\n\nlibrary(terra)\nlibrary(sp)\nlibrary(exactextractr)\n\ndd &lt;- \"/Volumes/vmfileshare/ClimateData/\"\n\n\nBias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS\n\n\n\n\n\n\nResampling script here The 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")\n\n\n\n\nCrop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE",
    "crumbs": [
      "R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#about",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Bias correction techniques in general require observational data to compare with climate projections in order to appropriately correct the bias.\nThe HadUK grid is a 1km x 1km gridded dataset derived from meterological station observations.\nThe first UKCP product for review is the UCKP convection-permitting dataset, on a 2.2km grid. Therefore, we are resmapling the 1km grid using bilenear interpolation to 2.2km grid extent.\nWe have ran this seperately in both r and python. The aim of this doc is to:\n\nEnsure both methods produce the same result\nEnsure the grid has been resampled to the correct extent and CRS",
    "crumbs": [
      "R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#data",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Resampling script here The 2.2km grid was derived from a reprojected (to BNG) UKCP 2.2km .nc file\nIn resampling it resampled the Sea as xx so replacing those vals as NA\nr1 &lt;- paste0(dd,\"TestData.R-python/Resampled_HADs_tasmax.2000.01.tif\")\nr1 &lt;- rast(r1)#Contains 31 layers for each day of Jan\n\n#In the resampling, the method used seemed to have relable all Sea values as '1.000000e+20' so relabelling them here (but to be checked as to why they've been valued like this in the resampling)\nr1[r1 &gt; 200] = NA\n\n#check the crs\ncrs(r1, proj=T)\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\n#Plot to check\nplot(r1$tasmax_1)\n\n\n\n\nResampling script here\nTHIS UPDATED 17/02/23\npy.pros.tasmax &lt;- list.files(paste0(dd,\"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"))\nr2 &lt;- py.pros.tasmax[grepl(\"200001\", py.pros.tasmax)] #Same file as resampled above\nr2 &lt;- paste0(paste0(dd, \"Processed/HadsUKgrid/resampled_2.2km_newgrid/tasmax/day\"),\"/\",r2)\nr2 &lt;- rast(r2)\ncrs(r2, proj=T) #check crs\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n## Ok so interesting is missing a crs slot on read - I wonder why this is? This could cause future problem potentially?\n\nplot(r2$tasmax_1)\n\n\n\n\nf &lt;- paste0(dd, \"Raw/HadsUKgrid/tasmax/day/\")\nhads.tasmax &lt;- list.files(f)\n\nhads.tasmax2 &lt;- hads.tasmax[grepl(\"200001\", hads.tasmax )] #Same file as resampled above\nog &lt;- paste0(f, hads.tasmax2)\n\nog &lt;- rast(og)\ncrs(og, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +a=6377563.396 +rf=299.324961266495 +units=m +no_defs\"\n\nplot(og$tasmax_1)\n ### 1d. UKCP example\nFor comparing the grids\nf &lt;- paste0(dd,\"Processed/UKCP2.2_Reproj/tasmax_bng2/01/latest/tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130.tif\")\nukcp &lt;- rast(f)\nukcp.r &lt;- ukcp$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`\n\ncrs(ukcp.r, proj=T)\n\n## [1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n#plot(ukcp.r)\n\n\n\nJust comparing by cropping to Scotland (bbox created here)\nscotland &lt;- vect(\"~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/CLIM-RECAL/clim-recal/data/Scotland/Scotland.bbox.shp\")",
    "crumbs": [
      "R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "href": "R/comparing-r-and-python/HADs-reprojection/WIP-Comparing-HADs-grids.html#comparisons",
    "title": "1 Comparing-Reprojections-HADs",
    "section": "",
    "text": "Crop extents to be the same\n#Noticed the crop takes longer on r2_c - for investigation!\n\nb &lt;- Sys.time()\nr1_c &lt;- terra::crop(r1, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n\n## Time difference of 0.02198005 secs\nplot(r1_c$tasmax_1)\n\nb &lt;- Sys.time()\nr2_c &lt;- terra::crop(r2, scotland, snap=\"in\")\ne &lt;- Sys.time()\ne-b\n## Time difference of 33.57785 secs\nplot(r2_c$tasmax_1)\n\nog_c &lt;- terra::crop(og, scotland, snap=\"in\")\nplot(og_c$tasmax_1)\n Ok there are some differences that I can see from the plot between the two resampled files!\n## Cropping to a small area to compare with the same orginal HADS file\ni &lt;- rast()\next(i) &lt;- c(200000, 210000, 700000, 710000)\nr1_ci &lt;- crop(r1_c, i)\nplot(r1_ci$tasmax_1)\n\n#Get number of cells in cropped extent\ncells &lt;- cells(r1_ci)\n\n#get coords for all cells (for comparing above)\nr.reproj_c_xy &lt;- sapply(cells, function(i){xyFromCell(r1_ci, i)})\n\nr.reproj_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 200935.7 203135.7 205335.7 207535.7 200935.7 203135.7 205335.7\n## [2,] 707331.7 705131.7 705131.7 705131.7 705131.7 702931.7 702931.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]\n## [1,] 209735.7 200935.7 203135.7 209735.7\n## [2,] 702931.7 700731.7 700731.7 700731.7\next(r1_ci)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\nr2_ci &lt;- crop(r2_c, i)\nplot(r2_ci$tasmax_1)\n\next(r2_ci)\n## SpatExtent : 199842.521629267, 210842.521629267, 699702.676089679, 710702.676089679 (xmin, xmax, ymin, ymax)\nog_ci &lt;- crop(og_c, i)\next(og_c)\n## SpatExtent : 6000, 470000, 531000, 1220000 (xmin, xmax, ymin, ymax)\nplot(og_ci$tasmax_1)\n\nukcp_c &lt;- terra::crop(ukcp.r, i)\nplot(ukcp_c$`tasmax_rcp85_land-cpm_uk_2.2km_01_day_19991201-20001130_31`)\n\next(ukcp_c)\n## SpatExtent : 199835.67457102, 210835.67457102, 699631.658882901, 710631.658882901 (xmin, xmax, ymin, ymax)\n#Get number of cells in cropped extent\ncells &lt;- cells(ukcp_c)\n\n#get coords for all cells (for comparing above)\nukcp_c_xy &lt;- sapply(cells, function(i){xyFromCell(ukcp_c, i)})\n\nukcp_c_xy\n##          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7\n## [2,] 709531.7 709531.7 709531.7 709531.7 709531.7 707331.7 707331.7 707331.7\n##          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n## [1,] 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7 209735.7 200935.7\n## [2,] 707331.7 707331.7 705131.7 705131.7 705131.7 705131.7 705131.7 702931.7\n##         [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n## [1,] 203135.7 205335.7 207535.7 209735.7 200935.7 203135.7 205335.7 207535.7\n## [2,] 702931.7 702931.7 702931.7 702931.7 700731.7 700731.7 700731.7 700731.7\n##         [,25]\n## [1,] 209735.7\n## [2,] 700731.7\nall(ukcp_c_xy, r.reproj_c_xy)\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## Warning in all(ukcp_c_xy, r.reproj_c_xy): coercing argument of type 'double' to\n## logical\n\n## [1] TRUE",
    "crumbs": [
      "R",
      "Comparing R and Python",
      "WIP Comparing HADs grids"
    ]
  },
  {
    "objectID": "R/README.html",
    "href": "R/README.html",
    "title": "1 Methods implemented in R",
    "section": "",
    "text": "1 Methods implemented in R\n\n/Resampling - code for Resampling data to different extents (grid sizes)\n/bias-correction-methods - bias correction methods implemented in R\n/comparing-r-and-python - Comparing various pipeline aspects between R and python\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Welcome to clim-recal, a specialized resource designed to tackle systematic errors or biases in Regional Climate Models (RCMs). As researchers, policy-makers, and various stakeholders explore publicly available RCMs, they need to consider the challenge of biases that can affect the accurate representation of climate change signals.\nclim-recal provides both a broad review of available bias correction methods as well as software, practical tutorials and guidance that helps users apply these methods methods to various datasets.\nclim-recal is an extensive software library and guide to application of Bias Correction (BC) methods:\n\nContains accessible information about the why and how of bias correction for climate data\nIs a software library for for the application of BC methods (see our full pipeline for bias-correction of the ground-breaking local-scale (2.2km) Convection Permitting Model (CPM). clim-recal brings together different software packages in python and R that implement a variety of bias correction methods, making it easy to apply them to data and compare their outputs.\nWas developed in partnership with the MetOffice to ensure the propriety, quality, and usability of our work\nProvides a framework for open additions of new software libraries/bias correction methods (in planning)\n\n\n\nWARNING: The documentation below is out of date in some cases, and we are in the process of updating this. In the meantime, see our Exported Datasets page for updates on datasets released for three UK cities.\n\n\n\n\nOverview: Bias Correction Pipeline\nDocumentation\nThe Datasets\nWhy Bias Correction?\nContributing\nFuture Plans\nLicense\n\n\n\n\nclim-recal is a debiasing pipeline, with the following steps:\n\nSet-up & data download We provide custom scripts to facilitate download of data\nPreprocessing This includes reprojecting, resampling & splitting the data prior to bias correction\nApply bias correction Our pipeline embeds two distinct methods of bias correction\nAssess the debiased data We have developed a way to assess the quality of the debiasing step across multiple alternative methods\n\nFor a quick start on bias correction, refer to our comprehensive analysis pipeline guide.\n\n\n\nWe are in the process of developing comprehensive documentation for our code base to supplement the guidance provided in this and other README.md files. In the interim, there is documentation available in the following forms:\n\nComments within R scripts\nCommand line --help documentation for some of our python scripts\npython function and class docstrings\nLocal render of documentation via quarto\n\n\n\nFor R scripts, please refer to contextual information and usage guidelines, and feel free to reach out with any specific queries.\n\n\n\nFor many of our python command line scripts, you can use the --help flag to access a summary of the available options and usage information:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n-h, --help            show this help message and exit\n--input INPUT         Path where the .nc files to resample is located\n--output OUTPUT       Path to save the resampled data data\n--grid_data GRID_DATA Path where the .nc file with the grid to resample is located\nThis will display all available options for the script, including their purposes.\n\n\n\nWe also hope to provide comprehensive documentation via quarto. This is a work in progress, but if you would like to render documentation locally you can do so via quarto and conda:\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.\n\n\n\n\n\n\nThe UK Climate Projections 2018 (UKCP18) dataset offers insights into the potential climate changes in the UK. UKCP18 is an advancement of the UKCP09 projections and delivers the latest evaluations of the UK’s possible climate alterations in land and marine regions throughout the 21st century. This crucial information aids in future Climate Change Risk Assessments and supports the UK’s adaptation to climate change challenges and opportunities as per the National Adaptation Programme.\n\n\n\nHadUK-Grid is a comprehensive collection of climate data for the UK, compiled from various land surface observations across the country. This data is organized into a uniform grid to ensure consistent coverage throughout the UK at up to 1km x 1km resolution. The dataset, spanning from 1836 to the present, includes a variety of climate variables such as air temperature, precipitation, sunshine, and wind speed, available on daily, monthly, seasonal, and annual timescales.\n\n\n\nThe geographical dataset can be used for visualising climate data. It mainly includes administrative boundaries published by the Office for National Statistics (ONS). The dataset is sharable under the Open Government Licence v.3.0 and is available for download via this link. We include a copy in the data/Geofiles folder for convenience. In addition, the clips for three cities’ boundaries from the same dataset are copied to three.cities subfolder.\n\n\n\n\nRegional climate models contain systematic errors, or biases in their output [1]. Biases arise in RCMs for a number of reasons, such as the assumptions in the general circulation models (GCMs), and in the downscaling process from GCM to RCM [1,2].\nResearchers, policy-makers and other stakeholders wishing to use publicly available RCMs need to consider a range of “bias correction” methods (sometimes referred to as “bias adjustment” or “recalibration”). Bias correction methods offer a means of adjusting the outputs of RCM in a manner that might better reflect future climate change signals whilst preserving the natural and internal variability of climate [2].\nPart of the clim-recal project is to review several bias correction methods. This work is ongoing and you can find our initial taxonomy here. When we’ve completed our literature review, it will be submitted for publication in an open peer-reviewed journal.\nOur work is however, just like climate data, intended to be dynamic, and we are in the process of setting up a pipeline for researchers creating new methods of bias correction to be able to submit their methods for inclusion on in the clim-recal repository.\n\nSenatore et al., 2022, https://doi.org/10.1016/j.ejrh.2022.101120\nAyar et al., 2021, https://doi.org/10.1038/s41598-021-82715-1\n\n\n\n\nWe hope to bring together the extensive work already undertaken by the climate science community and showcase a range of libraries and techniques. If you have suggestions on the repository, or would like to include a new method (see below) or library, please raise an issue or get in touch!\n\n\nTo use R in anaconda you may need to specify the conda-forge channel:\n$ conda config --env --add channels conda-forge\nSome libraries may be only available through pip, for example, these may require the generation / update of a requirements.txt:\n$ pip freeze &gt; requirements.txt\nand installing with:\n$ pip install -r requirements.txt\n\n\n\n\n\nMore BC Methods: Further bias correction of UKCP18 products. This is planned for a future release and is not available yet.\nPipeline for adding new methods: This is planned for a future release and is not available yet.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#data-results-and-documentation-updates",
    "href": "README.html#data-results-and-documentation-updates",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "WARNING: The documentation below is out of date in some cases, and we are in the process of updating this. In the meantime, see our Exported Datasets page for updates on datasets released for three UK cities.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#table-of-contents",
    "href": "README.html#table-of-contents",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Overview: Bias Correction Pipeline\nDocumentation\nThe Datasets\nWhy Bias Correction?\nContributing\nFuture Plans\nLicense",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#overview-bias-correction-pipeline",
    "href": "README.html#overview-bias-correction-pipeline",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "clim-recal is a debiasing pipeline, with the following steps:\n\nSet-up & data download We provide custom scripts to facilitate download of data\nPreprocessing This includes reprojecting, resampling & splitting the data prior to bias correction\nApply bias correction Our pipeline embeds two distinct methods of bias correction\nAssess the debiased data We have developed a way to assess the quality of the debiasing step across multiple alternative methods\n\nFor a quick start on bias correction, refer to our comprehensive analysis pipeline guide.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#documentation",
    "href": "README.html#documentation",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "We are in the process of developing comprehensive documentation for our code base to supplement the guidance provided in this and other README.md files. In the interim, there is documentation available in the following forms:\n\nComments within R scripts\nCommand line --help documentation for some of our python scripts\npython function and class docstrings\nLocal render of documentation via quarto\n\n\n\nFor R scripts, please refer to contextual information and usage guidelines, and feel free to reach out with any specific queries.\n\n\n\nFor many of our python command line scripts, you can use the --help flag to access a summary of the available options and usage information:\n$ python resampling_hads.py --help\n\nusage: resampling_hads.py [-h] --input INPUT [--output OUTPUT] [--grid_data GRID_DATA]\n\noptions:\n-h, --help            show this help message and exit\n--input INPUT         Path where the .nc files to resample is located\n--output OUTPUT       Path to save the resampled data data\n--grid_data GRID_DATA Path where the .nc file with the grid to resample is located\nThis will display all available options for the script, including their purposes.\n\n\n\nWe also hope to provide comprehensive documentation via quarto. This is a work in progress, but if you would like to render documentation locally you can do so via quarto and conda:\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#the-datasets",
    "href": "README.html#the-datasets",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "The UK Climate Projections 2018 (UKCP18) dataset offers insights into the potential climate changes in the UK. UKCP18 is an advancement of the UKCP09 projections and delivers the latest evaluations of the UK’s possible climate alterations in land and marine regions throughout the 21st century. This crucial information aids in future Climate Change Risk Assessments and supports the UK’s adaptation to climate change challenges and opportunities as per the National Adaptation Programme.\n\n\n\nHadUK-Grid is a comprehensive collection of climate data for the UK, compiled from various land surface observations across the country. This data is organized into a uniform grid to ensure consistent coverage throughout the UK at up to 1km x 1km resolution. The dataset, spanning from 1836 to the present, includes a variety of climate variables such as air temperature, precipitation, sunshine, and wind speed, available on daily, monthly, seasonal, and annual timescales.\n\n\n\nThe geographical dataset can be used for visualising climate data. It mainly includes administrative boundaries published by the Office for National Statistics (ONS). The dataset is sharable under the Open Government Licence v.3.0 and is available for download via this link. We include a copy in the data/Geofiles folder for convenience. In addition, the clips for three cities’ boundaries from the same dataset are copied to three.cities subfolder.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#why-bias-correction",
    "href": "README.html#why-bias-correction",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "Regional climate models contain systematic errors, or biases in their output [1]. Biases arise in RCMs for a number of reasons, such as the assumptions in the general circulation models (GCMs), and in the downscaling process from GCM to RCM [1,2].\nResearchers, policy-makers and other stakeholders wishing to use publicly available RCMs need to consider a range of “bias correction” methods (sometimes referred to as “bias adjustment” or “recalibration”). Bias correction methods offer a means of adjusting the outputs of RCM in a manner that might better reflect future climate change signals whilst preserving the natural and internal variability of climate [2].\nPart of the clim-recal project is to review several bias correction methods. This work is ongoing and you can find our initial taxonomy here. When we’ve completed our literature review, it will be submitted for publication in an open peer-reviewed journal.\nOur work is however, just like climate data, intended to be dynamic, and we are in the process of setting up a pipeline for researchers creating new methods of bias correction to be able to submit their methods for inclusion on in the clim-recal repository.\n\nSenatore et al., 2022, https://doi.org/10.1016/j.ejrh.2022.101120\nAyar et al., 2021, https://doi.org/10.1038/s41598-021-82715-1",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#contributing",
    "href": "README.html#contributing",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "We hope to bring together the extensive work already undertaken by the climate science community and showcase a range of libraries and techniques. If you have suggestions on the repository, or would like to include a new method (see below) or library, please raise an issue or get in touch!\n\n\nTo use R in anaconda you may need to specify the conda-forge channel:\n$ conda config --env --add channels conda-forge\nSome libraries may be only available through pip, for example, these may require the generation / update of a requirements.txt:\n$ pip freeze &gt; requirements.txt\nand installing with:\n$ pip install -r requirements.txt",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "README.html#future-plans",
    "href": "README.html#future-plans",
    "title": "1 Welcome to the clim-recal repository!",
    "section": "",
    "text": "More BC Methods: Further bias correction of UKCP18 products. This is planned for a future release and is not available yet.\nPipeline for adding new methods: This is planned for a future release and is not available yet.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "setup-instructions.html",
    "href": "setup-instructions.html",
    "title": "clim-recal installation",
    "section": "",
    "text": "clim-recal is a set of tools to manage UK climate data and projections, and running a range of correction methods in python and R. For ease of installation and use, we focus primarily on the python portion, but in principal the R notebooks should be useable via RStudio, which suggests packages necessary to install. We also provide docker installation options which include both R and python dependencies.",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "setup-instructions.html#conda-mamba",
    "href": "setup-instructions.html#conda-mamba",
    "title": "clim-recal installation",
    "section": "2.1 Conda / Mamba",
    "text": "2.1 Conda / Mamba\nAt present either conda or mamba are needed to use clim-recal. Installation instructions for these are available:\n\nconda: https://conda.io/projects/conda/en/latest/user-guide/install/index.html\nmamba: https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "setup-instructions.html#conda-lock",
    "href": "setup-instructions.html#conda-lock",
    "title": "clim-recal installation",
    "section": "2.2 conda-lock",
    "text": "2.2 conda-lock\nconda-lock provides a means of managing dependencies for conda/mamba environments on precise, reproducible levels, and can incorporated local pyproject.toml configurations.\nThere are many ways to install conda-lock (for most up to date options see README.md):\n\npipxpipcondamamba\n\n\npipx install conda-lock\n\n\npip install conda-lock\n\n\nconda install --channel=conda-forge --name=base conda-lock\n\n\nmamba install --channel=conda-forge --name=base conda-lock\n\n\n\nOnce conda-lock is installed, clim-recal can be installed via\n\npipxpipcondamamba\n\n\ncd `clim-recal`\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nconda activate base\nconda-lock install conda-lock.yml\n\n\ncd `clim-recal`\nmamba activate base\nconda-lock install conda-lock.yml",
    "crumbs": [
      "Install"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html",
    "href": "R/misc/Identifying_Runs.html",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Ruth C E Bowyer 2023-06-13\nrm(list=ls())\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n\nScript to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus\n\n\n\nData is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\nY &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above\n\n\n\n\nThe result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06",
    "crumbs": [
      "R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#about",
    "href": "R/misc/Identifying_Runs.html#about",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Script to identify the mean, 2nd highest and 2nd lowers daily tasmax per UKCP18 CPM run.\nThese runs will be the focus of initial bias correction focus",
    "crumbs": [
      "R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#load-data",
    "href": "R/misc/Identifying_Runs.html#load-data",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Data is tasmax runs converted to dataframe using sript ‘ConvertingAllCPMdataTOdf.R’, with files later renamed.Then daily means for historical periods and future periods were calculated using ‘calc.mean.sd.daily.R’ and summaries saved as .csv\nIn retrospect the conversion to df might not have been necessary/the most resource efficient, see comment here:https://tmieno2.github.io/R-as-GIS-for-Economists/turning-a-raster-object-into-a-data-frame.html – this was tested and using terra::global to calculate the raster-wide mean was less efficient\nUpdate 13.05.23 - Adding in infill data, mean to be calculated over the whole time period\nAs of June 2023, the tasmax-as-dataframe and tasmax daily means and the df data is located in vmfileshare/Interim/tasmax_dfs/\nThere is an error in the naming convention - Y00_Y20 should be Y01 to reflect the infill data time period (although this does cover a breif period of 2000) - to be updated in future\nRuns &lt;- c(\"01\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\")\n\nfiles &lt;- list.files(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\")\n\nfiles &lt;- files[grepl(\".csv\", files)]\nfp &lt;- paste0(\"/Users/rbowyer/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/tempdata/\", files)\n# Creating objects for names and filepath for each of the timer periods, for easy loading\nnames &lt;- gsub(\"df.avs_|.csv|df.\", \"\", files)\ni &lt;- c(\"hist\", \"Y00_Y20\",\"Y21_Y40\", \"Y41_Y60\", \"Y61_Y80\")\n\nnamesL &lt;- lapply(i, function(i){\n  n &lt;- names[grepl(i, names)]\n  })\n\nnames(namesL) &lt;- paste0(\"names_\",i)\nlist2env(namesL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\ndfL &lt;- lapply(i, function(i){\n  fp &lt;- fp[grepl(i, fp)]\n  dfs &lt;- lapply(fp, read.csv)\n  n &lt;- namesL[[paste0(\"names_\",i)]]\n  names(dfs) &lt;- n\n  return(dfs)\n  })\n\nnames(dfL) &lt;- paste0(\"dfs_\", i)\nlist2env(dfL, .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;",
    "crumbs": [
      "R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#comparing-runs",
    "href": "R/misc/Identifying_Runs.html#comparing-runs",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "Y &lt;- rep(c(1981:2000), each=360)\n\ndfs_hist &lt;- lapply(names_hist, function(i){\n  df &lt;- dfs_hist[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the historical period\nhistorical_means &lt;- dfs_hist %&gt;% reduce(rbind)\n\n\n\nggplot(historical_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n\n    theme_bw() + xlab(\"Day (Historical 1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nhistorical_means$model &lt;- as.factor(historical_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(historical_means$model))\nhistorical_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nhistorical_means$Yf &lt;- as.factor(historical_means$Y)\n\nhistorical_means_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(historical_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(historical_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(historical_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nhistorical_max_y &lt;- historical_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(historical_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (Historical 1980 - 2000)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nhistorical_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\nThe daily max is quite different than the means - something to bear in mind but interesting to think about - eg Run 4 here has the 2nd lowest spread of max max temp, but is selected above based on means\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, historical_means)\nav1$coefficients[order(av1$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, historical_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelhist_Run10 modelhist_Run02 modelhist_Run05 modelhist_Run07 modelhist_Run09\n##         9.89052        11.06863        11.21424        11.22048        11.27647\n## modelhist_Run04 modelhist_Run03 modelhist_Run08 modelhist_Run01 modelhist_Run06\n##        11.29057        11.35848        11.45257        11.45414        11.86451\n## modelhist_Run11 modelhist_Run12\n##        11.99148        12.31870\nMax of means:\nav3 &lt;- aov(max ~ model - 1, historical_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelhist_Run10 modelhist_Run04 modelhist_Run02 modelhist_Run05 modelhist_Run03\n##        18.12329        18.81126        18.90054        19.01801        19.10454\n## modelhist_Run09 modelhist_Run01 modelhist_Run08 modelhist_Run07 modelhist_Run11\n##        19.23705        19.31541        19.44439        19.54981        19.57548\n## modelhist_Run06 modelhist_Run12\n##        19.88375        20.47650\nMax vals are different but based on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\n\n\n\nY &lt;- rep(c(2021:2040), each=360)\n\n\ndfs_Y21_Y40 &lt;- lapply(names_Y21_Y40, function(i){\n  df &lt;- dfs_Y21_Y40[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y21_Y40 period\nY21_Y40_means &lt;- dfs_Y21_Y40 %&gt;% reduce(rbind)\n\n\n\nggplot(Y21_Y40_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Daily (1980 - 2000)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY21_Y40_means$model &lt;- as.factor(Y21_Y40_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y21_Y40_means$model))\nY21_Y40_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY21_Y40_means$Yf &lt;- as.factor(Y21_Y40_means$Y)\n\nY21_Y40_means_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y21_Y40_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y21_Y40_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y21_Y40_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY21_Y40_max_y &lt;- Y21_Y40_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y21_Y40_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2021 - 2040)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY21_Y40_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y21_Y40_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y21_Y40_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run02\n##           10.93136           12.36223           12.64493           12.67791\n## modelY21_Y40_Run09 modelY21_Y40_Run03 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           12.72584           12.85999           12.92934           13.03640\n## modelY21_Y40_Run04 modelY21_Y40_Run12 modelY21_Y40_Run06 modelY21_Y40_Run11\n##           13.07768           13.20011           13.38047           13.60076\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y21_Y40_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY21_Y40_Run10 modelY21_Y40_Run02 modelY21_Y40_Run09 modelY21_Y40_Run03\n##           19.29044           20.69596           20.82538           21.05558\n## modelY21_Y40_Run05 modelY21_Y40_Run07 modelY21_Y40_Run08 modelY21_Y40_Run01\n##           21.09128           21.22942           21.33484           21.37443\n## modelY21_Y40_Run04 modelY21_Y40_Run06 modelY21_Y40_Run12 modelY21_Y40_Run11\n##           21.49363           21.98667           22.09476           22.65178\nBased on means then selection would be Run 02 (2nd lowest), Run 04 & Run 03, and Run 11 (2nd lowest)\nBased on this period, the seelction would be: Run 05, Run 03, Run 08, Run 06 (so definetly Run 3 but others to be discussed)\n\n\n\nY &lt;- rep(c(2061:2080), each=360)\n\n\ndfs_Y61_Y80 &lt;- lapply(names_Y61_Y80, function(i){\n  df &lt;- dfs_Y61_Y80[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  return(df)\n})\n\n#Create a single df in long form of Runs for the Y61_Y80 period\nY61_Y80_means &lt;- dfs_Y61_Y80 %&gt;% reduce(rbind)\n\n\n\nggplot(Y61_Y80_means) +\n    geom_line(aes(x=dn, y=mean, group=model, colour=model)) +\n  # Removing sd ribbon for ease of viewing\n  #geom_ribbon(aes(x =dn, ymin = mean - sd, ymax= mean + sd), alpha=0.4) +\n    theme_bw() + xlab(\"Day (2060 - 2080)\") +\n    ylab(\"Daily mean max temp (tasmax) oC\") +\n  #scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position = \"none\") +\n  facet_wrap(.~ model, ncol=3) + guides(fill = FALSE)\n\n\n\n\n#Create a pallete specific to the runs so when reordered maintain the same colours\nY61_Y80_means$model &lt;- as.factor(Y61_Y80_means$model)\nc &lt;- brewer.pal(12, \"Paired\")\nmy_colours &lt;- setNames(c, levels(Y61_Y80_means$model))\nY61_Y80_means %&gt;%\n  mutate(model = fct_reorder(model, mean, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean), y=mean, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n    scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means, aes(sample=mean, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\n#Aggregating to year for annual average\n\nY61_Y80_means$Yf &lt;- as.factor(Y61_Y80_means$Y)\n\nY61_Y80_means_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))\nggplot(Y61_Y80_means_y) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n# Plotting with SDs in geom_ribbon to see if anything wildely different\nggplot(Y61_Y80_means_y) +\n    geom_ribbon(aes(as.numeric(Yf), y=mean.annual,\n                               ymin = mean.annual - sd.annual,\n                              ymax= mean.annual + sd.annual,\n                    fill=model),  alpha=0.4) +\n    geom_line(aes(x = as.numeric(Yf), y=mean.annual,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual mean of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()) + facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_means_y %&gt;%\n  mutate(model = fct_reorder(model, mean.annual, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, mean.annual), y=mean.annual, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max daily max temp oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nggplot(Y61_Y80_means_y, aes(sample=mean.annual, colour=factor(model))) +\n  stat_qq() +\n  stat_qq_line()+\n  theme_bw()+\n  scale_color_manual(values = my_colours) +\n  facet_wrap(.~model, ncol=3)\n\n\n\n\nY61_Y80_max_y &lt;- Y61_Y80_means %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(max=max(mean, na.rm=T))\nggplot(Y61_Y80_max_y) +\n      geom_line(aes(x = as.numeric(Yf), y=max,\n              color=model)) +\n    theme_bw() + xlab(\"Year (2061 - 2080)\") +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") +\n  scale_fill_brewer(palette = \"Paired\", name = \"\") +\n   scale_colour_brewer(palette = \"Paired\", name = \"\") +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nY61_Y80_max_y %&gt;%\n  mutate(model = fct_reorder(model, max, .fun='median')) %&gt;%\n    ggplot(aes(x=reorder(model, max), y=max, fill=model)) +\n    geom_boxplot() + theme_bw() +\n    ylab(\"Annual max of mean daily max temp (tasmax) oC\") + xlab(\"model\") +\n   scale_fill_manual(values = my_colours) +\n    theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\nDaily means:\n#-1 removes the intercept to compare coefficients of all Runs\nav1 &lt;- aov(mean ~ model - 1, Y61_Y80_means)\nav1$coefficients[order(av1$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nAnnual means:\nav2 &lt;- aov(mean.annual ~ model - 1, Y61_Y80_means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run01 modelY61_Y80_Run08\n##           12.70342           13.87016           14.55815           14.65973\n## modelY61_Y80_Run04 modelY61_Y80_Run09 modelY61_Y80_Run03 modelY61_Y80_Run12\n##           14.69527           14.76917           14.79545           14.87939\n## modelY61_Y80_Run07 modelY61_Y80_Run02 modelY61_Y80_Run11 modelY61_Y80_Run06\n##           14.94320           15.01577           15.11392           15.11814\nMax of means\nav3 &lt;- aov(max ~ model - 1, Y61_Y80_max_y)\nav3$coefficients[order(av3$coefficients)]\n## modelY61_Y80_Run10 modelY61_Y80_Run05 modelY61_Y80_Run03 modelY61_Y80_Run04\n##           21.83290           23.32972           23.88512           23.98220\n## modelY61_Y80_Run02 modelY61_Y80_Run01 modelY61_Y80_Run08 modelY61_Y80_Run06\n##           23.98610           24.03094           24.13232           24.41824\n## modelY61_Y80_Run12 modelY61_Y80_Run09 modelY61_Y80_Run07 modelY61_Y80_Run11\n##           24.48810           24.53152           24.77651           25.09102\nRuns suggested by this slice are Run 05, Run 09, Run 03 and Run 11\nRun 3 and 5 suggested above",
    "crumbs": [
      "R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "R/misc/Identifying_Runs.html#everything-combined",
    "href": "R/misc/Identifying_Runs.html#everything-combined",
    "title": "1 Identifying Runs for bias correction",
    "section": "",
    "text": "The result per time slice suggest different runs, aside from run 5\n\n\nUpdate 13.05.23 - Adding in the infill data, and taking the anova result across the whole time period\nY &lt;- rep(c(2001:2020), each=360)\n\ndfs_Y00_Y20 &lt;- lapply(names_Y00_Y20, function(i){\n  df &lt;- dfs_Y00_Y20[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\nY &lt;- rep(c(2041:2060), each=360)\n\ndfs_Y41_Y60 &lt;- lapply(names_Y41_Y60, function(i){\n  df &lt;- dfs_Y41_Y60[[i]]\n  names(df) &lt;- c(\"day\", \"mean\", \"sd\")\n  df$model &lt;- i\n  df$dn &lt;- 1:nrow(df)\n  df$Y &lt;- Y\n  df$Yf &lt;- as.factor(df$Y)\n  return(df)\n})\n\n\n#Create a single df in long form as above\nY00_Y20_means &lt;- dfs_Y00_Y20 %&gt;% reduce(rbind)\nY41_Y60_means &lt;- dfs_Y41_Y60 %&gt;% reduce(rbind)\nAssessing what the combined times slices suggest via anova\n\n\n#-1 removes the intercept to compare coefficients of all Runs\nall.means &lt;- rbind(historical_means, Y00_Y20_means, Y21_Y40_means, Y41_Y60_means, Y61_Y80_means)\n\nx &lt;- as.character(all.means$model)\nall.means$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\n\nav1 &lt;- aov(mean ~ model - 1, all.means)\nav1$coefficients[order(av1$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\n\n\n\n# As above, creating annual means\ninfill.L &lt;- list(Y00_Y20_means, Y41_Y60_means)\n\ninfill.L_y &lt;- lapply(infill.L, function(x){\n  means_y &lt;- x %&gt;%\n  group_by(Yf, model) %&gt;%\n  dplyr::summarise(mean.annual=mean(mean, na.rm=T), sd.annual=sd(mean, na.rm = T))})\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\n## `summarise()` has grouped output by 'Yf'. You can override using the `.groups`\n## argument.\nall.means_y &lt;- rbind(historical_means_y,\n                     infill.L_y[[1]],\n                     Y21_Y40_means_y,\n                     infill.L_y[[2]],\n                     Y61_Y80_means_y)\n\nx &lt;- as.character(all.means_y$model)\nall.means_y$model &lt;- substr(x, nchar(x)-4, nchar(x))\n\nav2 &lt;- aov(mean.annual ~ model - 1, all.means_y)\nav2$coefficients[order(av2$coefficients)]\n## modelRun10 modelRun05 modelRun09 modelRun04 modelRun03 modelRun07 modelRun08\n##   11.12464   12.48165   12.79216   12.89910   12.91685   12.91894   12.95115\n## modelRun02 modelRun01 modelRun12 modelRun06 modelRun11\n##   12.95347   12.97947   13.38267   13.40644   13.61157\nUpdated June 13th 2023 result\nConsidering all together, suggests: Runs 05, Run07, Run08 and Run06",
    "crumbs": [
      "R",
      "Identifying Runs"
    ]
  },
  {
    "objectID": "docs/contributing.html",
    "href": "docs/contributing.html",
    "title": "Contributing to our project",
    "section": "",
    "text": "We welcome contributions to our repository and follow the Turing Way Contributing Guidelines. As the project develops we will expand this section with details more specific to our code base and potential use/application.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-quarto-locally",
    "href": "docs/contributing.html#running-quarto-locally",
    "title": "Contributing to our project",
    "section": "1.1 Running Quarto Locally",
    "text": "1.1 Running Quarto Locally\nIf you would like to render documentation locally you can do so via a conda or docker\nWe appreciate your patience and encourage you to check back for updates on our ongoing documentation efforts.\n\n1.1.1 Locally via conda\n\nEnsure you have a local installation of conda or anaconda .\nCheckout a copy of our git repository\nCreate a local conda environment via our environment.yml file. This should install quarto.\nActivate that environment\nRun quarto preview.\n\nBelow are example bash shell commands to render locally after installing conda:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ quarto preview\n\n\n1.1.2 Locally via docker\nIf you have docker installed, you can run a version of the jupyter conifiguration and quarto. The simplest and quickest solution, assuming you have docker running, is:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\nThis should generate local sessions of:\n\njupyter for the python/ model: http://localhost:8888\nquarto documentation: http://localhost:8080",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-conda",
    "href": "docs/contributing.html#running-tests-in-conda",
    "title": "Contributing to our project",
    "section": "4.1 Running tests in conda",
    "text": "4.1 Running tests in conda\nOnce the conda environment is installed, it should be straightforward to run the basic tests. The following example starts from a fresh clone of the repository:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ conda create -n clim-recal -f environment.yml\n$ conda activate clim-recal\n$ cd python         # Currently the tests must be run within the `python` folder\n$ pytest\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: code/clim-recal/python          # Path printed is tweaked here for convenience\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓sss✓✓✓✓✓✓✓✓sss✓                                  82% ████████▎\n utils.py ✓✓✓✓                                                              100% ██████████\nSaved badge to clim-recal/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 -----------\nName                                 Stmts   Miss  Cover\n--------------------------------------------------------\nconftest.py                             32      4    88%\ndata_download/ceda_ftp_download.py      59     59     0%\ndebiasing/preprocess_data.py           134    134     0%\ndebiasing/run_cmethods.py              108    108     0%\nload_data/data_loader.py                83     83     0%\nresampling/check_calendar.py            46     46     0%\nresampling/resampling_hads.py           59     59     0%\ntests/test_debiasing.py                188     27    86%\n--------------------------------------------------------\nTOTAL                                  732    520    29%\n\n5 files skipped due to complete coverage.\n\n=========================== short test summary info ============================\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.mod_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.obs_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.preprocess_out_path[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_obs_folder[0]&gt;:2: requires linux server mount paths\nSKIPPED [1] &lt;doctest test_debiasing.RunConfig.yield_preprocess_out_folder[0]&gt;:2: requires linux server mount paths\n\nResults (0.14s):\n      16 passed\n       6 skipped\n       4 deselected\nThe SKIPPED messages of 6 doctests show they are automatically skipped if the linux server mount is not found, specifically data to test in /mnt/vmfileshare/ClimateData.",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-tests-in-docker",
    "href": "docs/contributing.html#running-tests-in-docker",
    "title": "Contributing to our project",
    "section": "4.2 Running tests in Docker",
    "text": "4.2 Running tests in Docker\nWith a docker install, tests can be run in two ways. The simplest is via docker compose:\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker compose build\n$ docker compose up\n$ docker compose exec jupyter bash -c \"conda run -n clim-recal --cwd python pytest\"\nThis mirrors the way tests are run via GitHub Actions for continuous integration on https://github.com/alan-turing-institute/clim-recal.\nTo run tests that require mounting ClimateData (which are not enabled by default), you will need to have a local mount of the relevant drive. This is easiest to achieve by building the compose/Dockerfile separately (not using compose) with that drive mounted.\n$ git clone https://github.com/alan-turing-institute/clim-recal\n$ cd clim-recal\n$ docker build -f compose/Dockerfile --tag 'clim-recal-test' .\n$ docker run -it -p 8888:8888 -v /Volumes/vmfileshare:/mnt/vmfileshare clim-recal-test .\nThis will print information to the terminal including the link to the new jupyter session in this form:\n[I 2023-11-16 13:46:31.350 ServerApp]     http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url\nBy copying your equivalent of http://127.0.0.1:8888/lab?token=a-long-list-of-characters-to-include-in-a-url you should be able to get a jupyer instance with all necessary packages installed running in your browser.\nFrom there, you can select a Terminal options under Other to get access to the terminal within your local docker build. You can then change to the python folder and run the tests with the server option to include ClimateData tests as well (note the a-hash-sequence will depend on your build):\n(clim-recal) jovyan@a-hash-sequence:~$ cd python\n(clim-recal) jovyan@a-hash-sequence:~/python$ pytest -m server\nTest session starts (platform: linux, Python 3.9.18, pytest 7.4.3, pytest-sugar 0.9.7)\nrootdir: /home/jovyan/python\nconfigfile: .pytest.ini\ntestpaths: tests, utils.py\nplugins: cov-4.1.0, sugar-0.9.7\n\n tests/test_debiasing.py ✓✓✓✓                         100% ██████████\nSaved badge to /home/jovyan/python/docs/assets/coverage.svg\n\n---------- coverage: platform linux, python 3.9.18-final-0 ---------\nName                                             Stmts   Miss  Cover\n--------------------------------------------------------------------\nconftest.py                                         32      4    88%\ndata_download/ceda_ftp_download.py                  59     59     0%\ndebiasing/preprocess_data.py                       134     21    84%\ndebiasing/python-cmethods/cmethods/CMethods.py     213    144    32%\ndebiasing/run_cmethods.py                          108      8    93%\nload_data/data_loader.py                            83     83     0%\nresampling/check_calendar.py                        46     46     0%\nresampling/resampling_hads.py                       59     59     0%\ntests/test_debiasing.py                            188      5    97%\nutils.py                                            23      5    78%\n--------------------------------------------------------------------\nTOTAL                                              945    434    54%\n\n5 files skipped due to complete coverage.\n\n\nResults (955.60s (0:15:55)):\n       4 passed\n      22 deselected",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html",
    "href": "docs/reference/clim_recal.resample.html",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "clim_recal.resample\nResample UKHADS data and UKCP18 data.\n\nUKHADS is resampled spatially from 1km to 2.2km.\nUKCP18 is resampled temporally from a 360 day calendar to a standard (365/366 day) calendar and projected to British National Grid (BNG) (from rotated polar grid).\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nCPMResampler\nCPM specific changes to HADsResampler.\n\n\nCPMResamplerManager\nClass to manage processing CPM resampling.\n\n\nHADsResampler\nManage resampling HADs datafiles for modelling.\n\n\nHADsResamplerManager\nClass to manage processing HADs resampling.\n\n\nResamblerBase\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nclim_recal.resample.CPMResampler(self, *, input_path=RAW_CPM_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=CPM_SPATIAL_COORDS_PATH, input_file_x_column_name=CPRUK_XDIM, input_file_y_column_name=CPRUK_YDIM, start_index=0, stop_index=None, standard_calendar_relative_path=CPM_STANDARD_CALENDAR_PATH)\nCPM specific changes to HADsResampler.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nos.PathLike | None\nPath to CPM files to process.\n\n\noutput\n\nPath to save processed CPM files.\n\n\ngrid_data_path\n\nPath to load to self.grid.\n\n\ngrid\n\nDataset of grid (either passed via grid_data_path or as a parameter).\n\n\ninput_files\n\nNCF or TIF files to process with self.grid etc.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\nresampling_func\n\nFunction to call on self.input_files with self.grid\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\n\nCoordinate Reference System (CRS) to return final format in.\n\n\ngrid_x_column_name\n\nColumn name in input_files or input for x coordinates.\n\n\ngrid_y_column_name\n\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\n\nFile extensions to glob input_files with.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     input_path=REPROJECTED_CPM_TASMAX_01_LATEST_INPUT_PATH,\n...     output_path=resample_test_cpm_output_path,\n...     input_file_extension=TIF_EXTENSION_STR,\n... )\n&gt;&gt;&gt; cpm_resampler\n&lt;CPMResampler(...count=100,...\n    ...input_path='.../tasmax/01/latest',...\n    ...output_path='.../test-run-results_..._.../cpm')&gt;\n&gt;&gt;&gt; pprint(cpm_resampler.input_files)\n(...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_19801201-19811130.tif'),\n ...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_19811201-19821130.tif'),\n ...,\n ...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_20791201-20801130.tif'))\n\n\n\n\nclim_recal.resample.CPMResamplerManager(self, *, input_paths=RAW_CPM_PATH, output_paths=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), sub_path=Path('latest'), start_index=0, stop_index=None, start_date=CPM_START_DATE, end_date=CPM_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=CPMResampler, cpus=None, _var_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, runs=RunOptions.preferred())\nClass to manage processing CPM resampling.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; cpm_resampler_manager: CPMResamplerManager = CPMResamplerManager(\n...     stop_index=10,\n...     output_paths=resample_test_cpm_output_path,\n...     )\n&gt;&gt;&gt; cpm_resampler_manager\n&lt;CPMResamplerManager(variables_count=1, runs_count=4,\n                     input_paths_count=4)&gt;\n&gt;&gt;&gt; configs: tuple[CPMResampler, ...] = tuple(\n...     cpm_resampler_manager.yield_configs())\n&gt;&gt;&gt; pprint(configs)\n(&lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/05/latest',\n               output_path='.../tasmax/05/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/06/latest',\n               output_path='.../tasmax/06/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/07/latest',\n               output_path='.../tasmax/07/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/08/latest',\n               output_path='.../tasmax/08/latest')&gt;)\n\n\n\n\nclim_recal.resample.HADsResampler(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=HADS_2_2K_RESOLUTION_PATH, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None)\nManage resampling HADs datafiles for modelling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nos.PathLike | None\nPath to HADs files to process.\n\n\noutput\n\nPath to save processed HADS files.\n\n\ngrid_data_path\n\nPath to load to self.grid.\n\n\ngrid\nos.PathLike | xarray.Dataset\nDataset of grid (either passed via grid_data_path or as a parameter).\n\n\ninput_files\ntyping.Iterable[os.PathLike] | None\nNCF or TIF files to process with self.grid etc.\n\n\nresampling_func\n\nFunction to call on self.input_files with self.grid\n\n\ncrop\nos.PathLike | geopandas.GeoDataFrame | None\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\nstr\nCoordinate Reference System (CRS) to return final format in.\n\n\ngrid_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ngrid_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\npython.clim_recal.resample.NETCDF_OR_TIF\nFile extensions to glob input_files with.\n\n\n\n\n\n\n\nTry time projection first\nCombine with space (this worked)\nAdd crop step\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; hads_resampler: HADsResampler = HADsResampler(\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; hads_resampler\n&lt;HADsResampler(...count=504,...\n    ...input_path='.../tasmax/day',...\n    ...output_path='...run-results_..._.../hads')&gt;\n&gt;&gt;&gt; pprint(hads_resampler.input_files)\n(...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800101-19800131.nc'),\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800201-19800229.nc'),\n ...,\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_20211201-20211231.nc'))\n\n\n\n\nclim_recal.resample.HADsResamplerManager(self, *, input_paths=RAW_HADS_PATH, output_paths=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), sub_path=Path('day'), start_index=0, stop_index=None, start_date=HADS_START_DATE, end_date=HADS_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=HADsResampler, cpus=None, _var_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False)\nClass to manage processing HADs resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nos.PathLike | typing.Sequence[os.PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\noutput_paths\nos.PathLike | typing.Sequence[os.PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\ntyping.Sequence[python.clim_recal.utils.data.VariableOptions | str]\nWhich VariableOptions to include.\n\n\nsub_path\npathlib.Path\nPath to include at the stem of generating input_paths.\n\n\ncpus\nint | None\nNumber of cpu cores to use during multiprocessing.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; hads_resampler_manager: HADsResamplerManager = HADsResamplerManager(\n...     variables=VariableOptions.all(),\n...     output_paths=resample_test_hads_output_path,\n...     )\n&gt;&gt;&gt; hads_resampler_manager\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nVarirableInBaseImportPathError\nChecking import path validity for self.variables.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.VarirableInBaseImportPathError()\nChecking import path validity for self.variables.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_paths\nCheck if all self.input_paths exist.\n\n\nexecute_resample_configs\nRun all resampler configurations\n\n\nset_input_paths\nPropagate self.input_paths if needed.\n\n\nset_output_paths\nPropagate self.output_paths if needed.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.check_paths(run_set_data_paths=True)\nCheck if all self.input_paths exist.\n\n\n\nclim_recal.resample.HADsResamplerManager.execute_resample_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.set_input_paths()\nPropagate self.input_paths if needed.\n\n\n\nclim_recal.resample.HADsResamplerManager.set_output_paths()\nPropagate self.output_paths if needed.\n\n\n\nclim_recal.resample.HADsResamplerManager.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerBase(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=HADS_2_2K_RESOLUTION_PATH, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None)\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nName\nDescription\n\n\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nexecute\nRun all steps for processing\n\n\nset_grid\nSet check and set (if necessary) grid attribute of self.\n\n\nset_grid_x_y\nSet the x y values via grid_x_column_name and grid_y_column_name.\n\n\nset_input_files\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerBase.execute(skip_spatial=False, **kwargs)\nRun all steps for processing\n\n\n\nclim_recal.resample.ResamblerBase.set_grid(new_grid_data_path=None)\nSet check and set (if necessary) grid attribute of self.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnew_grid_data_path\nos.PathLike | None\nNew Path to load to self.grid.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerBase.set_grid_x_y(grid_x_column_name=None, grid_y_column_name=None)\nSet the x y values via grid_x_column_name and grid_y_column_name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngrid_x_column_name\nstr | None\nName of column in self.grid Dataset to extract to self.x. If None use self.grid_x_column_name, else overwrite.\nNone\n\n\ngrid_y_column_name\nstr | None\nName of column in self.grid Dataset to extract to self.y. If None use self.grid_y_column_name, else overwrite.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerBase.set_input_files(new_input_path=None)\nReplace self.input and process self.input_files.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nreproject_2_2km_filename\nReturn tweaked path to indicate standard day projection.\n\n\nreproject_standard_calendar_filename\nReturn tweaked path to indicate standard day projection.\n\n\n\n\n\nclim_recal.resample.reproject_2_2km_filename(path)\nReturn tweaked path to indicate standard day projection.\n\n\n\nclim_recal.resample.reproject_standard_calendar_filename(path)\nReturn tweaked path to indicate standard day projection.",
    "crumbs": [
      "python",
      "Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html#attributes",
    "href": "docs/reference/clim_recal.resample.html#attributes",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDEFAULT_INTERPOLATION_METHOD\nDefault method to infer missing estimates in a time series.",
    "crumbs": [
      "python",
      "Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html#classes",
    "href": "docs/reference/clim_recal.resample.html#classes",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCPMResampler\nCPM specific changes to HADsResampler.\n\n\nCPMResamplerManager\nClass to manage processing CPM resampling.\n\n\nHADsResampler\nManage resampling HADs datafiles for modelling.\n\n\nHADsResamplerManager\nClass to manage processing HADs resampling.\n\n\nResamblerBase\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nclim_recal.resample.CPMResampler(self, *, input_path=RAW_CPM_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=CPM_SPATIAL_COORDS_PATH, input_file_x_column_name=CPRUK_XDIM, input_file_y_column_name=CPRUK_YDIM, start_index=0, stop_index=None, standard_calendar_relative_path=CPM_STANDARD_CALENDAR_PATH)\nCPM specific changes to HADsResampler.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nos.PathLike | None\nPath to CPM files to process.\n\n\noutput\n\nPath to save processed CPM files.\n\n\ngrid_data_path\n\nPath to load to self.grid.\n\n\ngrid\n\nDataset of grid (either passed via grid_data_path or as a parameter).\n\n\ninput_files\n\nNCF or TIF files to process with self.grid etc.\n\n\ncpus\n\nNumber of cpu cores to use during multiprocessing.\n\n\nresampling_func\n\nFunction to call on self.input_files with self.grid\n\n\ncrop\n\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\n\nCoordinate Reference System (CRS) to return final format in.\n\n\ngrid_x_column_name\n\nColumn name in input_files or input for x coordinates.\n\n\ngrid_y_column_name\n\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\n\nFile extensions to glob input_files with.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; cpm_resampler: CPMResampler = CPMResampler(\n...     input_path=REPROJECTED_CPM_TASMAX_01_LATEST_INPUT_PATH,\n...     output_path=resample_test_cpm_output_path,\n...     input_file_extension=TIF_EXTENSION_STR,\n... )\n&gt;&gt;&gt; cpm_resampler\n&lt;CPMResampler(...count=100,...\n    ...input_path='.../tasmax/01/latest',...\n    ...output_path='.../test-run-results_..._.../cpm')&gt;\n&gt;&gt;&gt; pprint(cpm_resampler.input_files)\n(...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_19801201-19811130.tif'),\n ...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_19811201-19821130.tif'),\n ...,\n ...Path('.../tasmax/01/latest/tasmax_...-cpm_uk_2.2km_01_day_20791201-20801130.tif'))\n\n\n\n\nclim_recal.resample.CPMResamplerManager(self, *, input_paths=RAW_CPM_PATH, output_paths=RESAMPLING_OUTPUT_PATH / CPM_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), sub_path=Path('latest'), start_index=0, stop_index=None, start_date=CPM_START_DATE, end_date=CPM_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=CPMResampler, cpus=None, _var_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False, runs=RunOptions.preferred())\nClass to manage processing CPM resampling.\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; cpm_resampler_manager: CPMResamplerManager = CPMResamplerManager(\n...     stop_index=10,\n...     output_paths=resample_test_cpm_output_path,\n...     )\n&gt;&gt;&gt; cpm_resampler_manager\n&lt;CPMResamplerManager(variables_count=1, runs_count=4,\n                     input_paths_count=4)&gt;\n&gt;&gt;&gt; configs: tuple[CPMResampler, ...] = tuple(\n...     cpm_resampler_manager.yield_configs())\n&gt;&gt;&gt; pprint(configs)\n(&lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/05/latest',\n               output_path='.../tasmax/05/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/06/latest',\n               output_path='.../tasmax/06/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/07/latest',\n               output_path='.../tasmax/07/latest')&gt;,\n &lt;CPMResampler(count=10, max_count=100,\n               input_path='.../tasmax/08/latest',\n               output_path='.../tasmax/08/latest')&gt;)\n\n\n\n\nclim_recal.resample.HADsResampler(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=HADS_2_2K_RESOLUTION_PATH, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None)\nManage resampling HADs datafiles for modelling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_path\nos.PathLike | None\nPath to HADs files to process.\n\n\noutput\n\nPath to save processed HADS files.\n\n\ngrid_data_path\n\nPath to load to self.grid.\n\n\ngrid\nos.PathLike | xarray.Dataset\nDataset of grid (either passed via grid_data_path or as a parameter).\n\n\ninput_files\ntyping.Iterable[os.PathLike] | None\nNCF or TIF files to process with self.grid etc.\n\n\nresampling_func\n\nFunction to call on self.input_files with self.grid\n\n\ncrop\nos.PathLike | geopandas.GeoDataFrame | None\nPath or file to spatially crop input_files with.\n\n\nfinal_crs\nstr\nCoordinate Reference System (CRS) to return final format in.\n\n\ngrid_x_column_name\nstr\nColumn name in input_files or input for x coordinates.\n\n\ngrid_y_column_name\nstr\nColumn name in input_files or input for y coordinates.\n\n\ninput_file_extension\npython.clim_recal.resample.NETCDF_OR_TIF\nFile extensions to glob input_files with.\n\n\n\n\n\n\n\nTry time projection first\nCombine with space (this worked)\nAdd crop step\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; hads_resampler: HADsResampler = HADsResampler(\n...     output_path=resample_test_hads_output_path,\n... )\n&gt;&gt;&gt; hads_resampler\n&lt;HADsResampler(...count=504,...\n    ...input_path='.../tasmax/day',...\n    ...output_path='...run-results_..._.../hads')&gt;\n&gt;&gt;&gt; pprint(hads_resampler.input_files)\n(...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800101-19800131.nc'),\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_19800201-19800229.nc'),\n ...,\n ...Path('.../tasmax/day/tasmax_hadukgrid_uk_1km_day_20211201-20211231.nc'))\n\n\n\n\nclim_recal.resample.HADsResamplerManager(self, *, input_paths=RAW_HADS_PATH, output_paths=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variables=(VariableOptions.default()), sub_path=Path('day'), start_index=0, stop_index=None, start_date=HADS_START_DATE, end_date=HADS_END_DATE, configs=list(), config_default_kwargs=dict(), resampler_class=HADsResampler, cpus=None, _var_path_dict=dict(), _strict_fail_if_var_in_input_path=True, _allow_check_fail=False)\nClass to manage processing HADs resampling.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_paths\nos.PathLike | typing.Sequence[os.PathLike]\nPath or Paths to CPM files to process. If Path, will be propegated with files matching\n\n\noutput_paths\nos.PathLike | typing.Sequence[os.PathLike]\nPath or Paths to to save processed CPM files to. If Path will be propagated to match input_paths.\n\n\nvariables\ntyping.Sequence[python.clim_recal.utils.data.VariableOptions | str]\nWhich VariableOptions to include.\n\n\nsub_path\npathlib.Path\nPath to include at the stem of generating input_paths.\n\n\ncpus\nint | None\nNumber of cpu cores to use during multiprocessing.\n\n\n\n\n\n\n&gt;&gt;&gt; if not is_data_mounted:\n...     pytest.skip(mount_doctest_skip_message)\n&gt;&gt;&gt; hads_resampler_manager: HADsResamplerManager = HADsResamplerManager(\n...     variables=VariableOptions.all(),\n...     output_paths=resample_test_hads_output_path,\n...     )\n&gt;&gt;&gt; hads_resampler_manager\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nVarirableInBaseImportPathError\nChecking import path validity for self.variables.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.VarirableInBaseImportPathError()\nChecking import path validity for self.variables.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_paths\nCheck if all self.input_paths exist.\n\n\nexecute_resample_configs\nRun all resampler configurations\n\n\nset_input_paths\nPropagate self.input_paths if needed.\n\n\nset_output_paths\nPropagate self.output_paths if needed.\n\n\nyield_configs\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.check_paths(run_set_data_paths=True)\nCheck if all self.input_paths exist.\n\n\n\nclim_recal.resample.HADsResamplerManager.execute_resample_configs(multiprocess=False, cpus=None)\nRun all resampler configurations\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmultiprocess\nbool\nIf True run parameters in resample_configs with multiprocess_execute.\nFalse\n\n\ncpus\nint | None\nNumber of cpus to pass to multiprocess_execute.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.HADsResamplerManager.set_input_paths()\nPropagate self.input_paths if needed.\n\n\n\nclim_recal.resample.HADsResamplerManager.set_output_paths()\nPropagate self.output_paths if needed.\n\n\n\nclim_recal.resample.HADsResamplerManager.yield_configs()\nGenerate a CPMResampler or HADsResampler for self.input_paths.\n\n\n\n\n\nclim_recal.resample.ResamblerBase(self, *, input_path=RAW_HADS_TASMAX_PATH, output_path=RESAMPLING_OUTPUT_PATH / HADS_OUTPUT_LOCAL_PATH, variable_name=VariableOptions.default(), grid=DEFAULT_RELATIVE_GRID_DATA_PATH, input_files=None, cpus=None, crop=None, final_crs=BRITISH_NATIONAL_GRID_EPSG, grid_x_column_name=HADS_XDIM, grid_y_column_name=HADS_YDIM, input_file_extension=NETCDF_EXTENSION_STR, export_file_extension=NETCDF_EXTENSION_STR, resolution_relative_path=HADS_2_2K_RESOLUTION_PATH, input_file_x_column_name=HADS_XDIM, input_file_y_column_name=HADS_YDIM, start_index=0, stop_index=None)\nBase class to inherit for HADs and CPM.\n\n\n\n\n\nName\nDescription\n\n\n\n\nmax_count\nMaximum length of self.input_files ignoring start_index and start_index.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nexecute\nRun all steps for processing\n\n\nset_grid\nSet check and set (if necessary) grid attribute of self.\n\n\nset_grid_x_y\nSet the x y values via grid_x_column_name and grid_y_column_name.\n\n\nset_input_files\nReplace self.input and process self.input_files.\n\n\n\n\n\nclim_recal.resample.ResamblerBase.execute(skip_spatial=False, **kwargs)\nRun all steps for processing\n\n\n\nclim_recal.resample.ResamblerBase.set_grid(new_grid_data_path=None)\nSet check and set (if necessary) grid attribute of self.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnew_grid_data_path\nos.PathLike | None\nNew Path to load to self.grid.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerBase.set_grid_x_y(grid_x_column_name=None, grid_y_column_name=None)\nSet the x y values via grid_x_column_name and grid_y_column_name.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngrid_x_column_name\nstr | None\nName of column in self.grid Dataset to extract to self.x. If None use self.grid_x_column_name, else overwrite.\nNone\n\n\ngrid_y_column_name\nstr | None\nName of column in self.grid Dataset to extract to self.y. If None use self.grid_y_column_name, else overwrite.\nNone\n\n\n\n\n\n\n\nclim_recal.resample.ResamblerBase.set_input_files(new_input_path=None)\nReplace self.input and process self.input_files.",
    "crumbs": [
      "python",
      "Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.resample.html#functions",
    "href": "docs/reference/clim_recal.resample.html#functions",
    "title": "1 clim_recal.resample",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nreproject_2_2km_filename\nReturn tweaked path to indicate standard day projection.\n\n\nreproject_standard_calendar_filename\nReturn tweaked path to indicate standard day projection.\n\n\n\n\n\nclim_recal.resample.reproject_2_2km_filename(path)\nReturn tweaked path to indicate standard day projection.\n\n\n\nclim_recal.resample.reproject_standard_calendar_filename(path)\nReturn tweaked path to indicate standard day projection.",
    "crumbs": [
      "python",
      "Reference",
      "Data Resampling"
    ]
  },
  {
    "objectID": "docs/docker-configurations.html",
    "href": "docs/docker-configurations.html",
    "title": "Docker configurations",
    "section": "",
    "text": "The compose.yml file and compose/ folder provider Docker configurations for deploying and testing this code and documentation reproducibly. These require an internet connection and Docker installation.",
    "crumbs": [
      "Docker"
    ]
  },
  {
    "objectID": "docs/docker-configurations.html#adding-multiple-users",
    "href": "docs/docker-configurations.html#adding-multiple-users",
    "title": "Docker configurations",
    "section": "3.1 Adding multiple users",
    "text": "3.1 Adding multiple users\nSome utility functions are provided to ease adding many users for a workshop. We do not recommend using this for any long term deployment, and certainly not for any data or environments with security concerns.\n\n\n\n\n\n\nWarning\n\n\n\nThese examples are not tested for security. Rather: this is intended for a short workshop that can scale a series of environements generated via docker for many users to have remote access to play with the code and data provided.\n\n\nOur usecase is primarily for within a docker deploy with root permission.\nA list of user names and passwords are needed, and by default we assume those are in a table format in the following structure:\n\n\n\nuser_name\npassword\n\n\n\n\nsally\nfig*new£kid\n\n\ngeorge\ntree&iguana*sky\n\n\nsusan\nhistory!bill-walk\n\n\n\nwhich in csv would look like:\nuser_name,password\nsally,fig*new£kid\ngeorge,tee&iguana*sky\nsusan,history!bill-walk\nTo generate basic user accounts one could run the following with root permission in an rstudio or jupyter environment (assuming rstudio has been built):\ndocker compose up -d jupyter\n[+] Running 1/1\n ✔ Container clim-recal-jupyter-1  Started\ndocker compose -u 0 exec jupyter bash\n(base) root@aha22hnum:~#\nwhich should instantiate a bash terminal in the Docker environment for the default jupyter Docker root user. The following demonstrates creating a csv file config in test_auth.csv:\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt;\n&gt;&gt;&gt; csv_path: Path = 'test_auth.csv'\n&gt;&gt;&gt; auth_dict: dict[str, str] = {\n...    'sally': 'fig*new£kid',\n...    'george': 'tee&iguana*sky',\n...    'susan': 'history!bill-walk',}\n&gt;&gt;&gt; field_names: tuple[str, str] = ('user_name', 'password')\n&gt;&gt;&gt; with open(csv_path, 'w') as csv_file:\n...     writer = csv.writer(csv_file)\n...     line_num: int = writer.writerow(('user_name', 'password'))\n...     for user_name, password in auth_dict.items():\n...         line_num = writer.writerow((user_name, password))\nand then with that file generate users and their home folders from test_auth.csv:\n&gt;&gt;&gt; from utils.server import JUPYTER_DOCKER_USER_PATH, csv_reader, make_users,\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=csv_path,\n...     user_col=\"user_name\",\n...     password_col=\"password\",\n...     file_reader=csv_reader,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; tuple(user_paths)\n('/home/sally', '/home/george', '/home/susan')\n&gt;&gt;&gt; csv_path.unlink()",
    "crumbs": [
      "Docker"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html",
    "href": "docs/pipeline_guidance.html",
    "title": "Analysis pipeline guidance",
    "section": "",
    "text": "This is a detailed guide to our analysis pipeline. see also this flowchart viz of the pipeline",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html#prerequisites",
    "href": "docs/pipeline_guidance.html#prerequisites",
    "title": "Analysis pipeline guidance",
    "section": "1 Prerequisites",
    "text": "1 Prerequisites\nFor our bias correction methods, we tap into dedicated packages in both python and R ecosystems. The integration of these languages allows us to utilize the cutting-edge functionalities implemented in each.\nGiven this dual-language nature of our analysis pipeline, we also provide preprocessing scripts written in both python and R. To facilitate a seamless experience, users are required to set up both python and R environments as detailed below.\n\n1.1 Setting up your R environment\n\n1.1.1 Download and Install R\nVisit CRAN (The Comprehensive R Archive Network) to download the latest version of R compatible with your operating system. Then verify successful installation via command line:\nR --version\n\n\n1.1.2 Install Necessary R Packages\nOur analysis utilizes several R packages. You can install these packages by starting R (just type R in your command line and press enter) and entering the following commands in the R console:\ninstall.packages(\"package1\")\ninstall.packages(\"package2\")\n#... (continue for all necessary packages)\nReplace \"package1\", \"package2\", etc., with the actual names of the required packages. A list of required R packages is usually at the top of each .R, .Rmd, some R specific .md files. Below is an example from WIP Comparing HADs grids:\nlibrary(terra)\nlibrary(sp)\nlibrary(exactextractr)\n\n\n\n1.2 Setting up your python environment\nFor your python environment, we provide a conda-lock.yml environment file for ease-of-use.\nconda-lock install -name clim-recal conda-lock.yml\n\nIn order to paralellize the reprojection step, we make use of the GNU parallel shell tool.\nparallel --version\n\n\n1.3 The cmethods library\nThis repository contains a python script used to run debiasing in climate data using a fork of the original python-cmethods module written by Benjamin Thomas Schwertfeger’s , which has been modified to function with the dataset used in the clim-recal project. This library has been included as a submodule to this project, so you must run the following command to pull the submodules required.\ngit submodule update --init --recursive\n\n\n1.4 Downloading the data\n\n1.4.1 Climate data\nThis streamlined pipeline is designed for raw data provided by the Met Office, accessible through the CEDA archive. It utilizes UKCP control, scenario data at 2.2km resolution, and HADs observational data. For those unfamiliar with this data, refer to our the dataset section.\nTo access the data, register here at the CEDA archive and configure your FTP credentials in “My Account”. Utilize our ceda_ftp_download.py script to download the data.\n# cpm data\npython3 ceda_ftp_download.py --input /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85/ --output 'output_dir' --username 'uuu' --psw 'ppp' --change_hierarchy\n\n# hads data\npython3 ceda_ftp_download.py --input /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km --output 'output_dir' --username 'uuu' --psw 'ppp'\nYou need to replace uuu and ppp with your CEDA username and FTP password respectively and replace output_dir with the directory you want to write the data to.\nThe --change_hierarchy flag modifies the folder hierarchy to fit with the hierarchy in the Turing Azure file store. This flag only applies to the UKCP data and should not be used with HADs data. You can use the same script without the --change_hierarchy flag in order to download files without any changes to the hierarchy.\n\n\n\n\n\n\nTip\n\n\n\n📢 If you are an internal collaborator you can access the raw data as well as intermediate steps through our Azure server. See here for a How-to.\n\n\n\n\n1.4.2 Geospatial data\nIn addition to the climate data we use geospatial data to divide the data into smaller chunks. Specifically we use the following datasets for city boundaries:\n\nScottish localities boundaries for cropping out Glasgow. Downloaded from nrscotland.gov.uk on 1st Aug 2023\nMajor Towns and Cities boundaries for cropping out Manchester. Downloaded from https://geoportal.statistics.gov.uk/\n\n\n\n\n\n\n\nWarning\n\n\n\nSections below may need updating.",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html#reprojecting-the-data",
    "href": "docs/pipeline_guidance.html#reprojecting-the-data",
    "title": "Analysis pipeline guidance",
    "section": "2 Reprojecting the data",
    "text": "2 Reprojecting the data\nThe HADs data and the UKCP projections have different resolution and coordinate system. For example: the HADs dataset uses the British National Grid coordinate system.\n\n\nClick for raw HADs and CPM coordinate structures\n\nCRS.from_wkt('PROJCS[\"undefined\",GEOGCS[\"undefined\",DATUM[\"undefined\",SPHEROID[\"undefined\",6377563.396,299.324961266495]],PRIMEM[\"undefined\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",49],PARAMETER[\"central_meridian\",-2],PARAMETER[\"scale_factor\",0.9996012717],PARAMETER[\"false_easting\",400000],PARAMETER[\"false_northing\",-100000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]')\nCRS.from_wkt('GEOGCRS[\"undefined\",BASEGEOGCRS[\"undefined\",DATUM[\"undefined\",ELLIPSOID[\"undefined\",6371229,0,LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]],PRIMEM[\"undefined\",0,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]],DERIVINGCONVERSION[\"Pole rotation (netCDF CF convention)\",METHOD[\"Pole rotation (netCDF CF convention)\"],PARAMETER[\"Grid north pole latitude (netCDF CF convention)\",37.5,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],PARAMETER[\"Grid north pole longitude (netCDF CF convention)\",177.5,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],PARAMETER[\"North pole grid longitude (netCDF CF convention)\",0,ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]],CS[ellipsoidal,2],AXIS[\"longitude\",east,ORDER[1],ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]],AXIS[\"latitude\",north,ORDER[2],ANGLEUNIT[\"degree\",0.0174532925199433,ID[\"EPSG\",9122]]]]')\n\nThe first step in our analysis pipeline is to reproject the UKCP datasets to the British National Grid coordinate system. For this purpose, we utilize the Geospatial Data Abstraction Library (GDAL), designed for reading and writing raster and vector geospatial data formats.\n\nNote that, to reproduce our exact pipeline, we switch environments here as explained in the requirements.\nconda activate gdal_env\n\nTo execute the reprojection in parallel fashion, run the reproject_all.sh script from your shell. As an input to the script replace path_to_netcdf_files with the path to the raw netCDF files.\ncd bash\nsh reproject_all.sh path_to_netcdf_files",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html#resampling-the-data",
    "href": "docs/pipeline_guidance.html#resampling-the-data",
    "title": "Analysis pipeline guidance",
    "section": "3 Resampling the data",
    "text": "3 Resampling the data\nResample the HADs UK dataset from 1km to 2.2km grid to match the UKCP reprojected grid. We run the resampling python script specifying the --input location of the reprojected files from the previous step, the UKCP --grid file an the --output location for saving the resampled files.\n# switch to main environment\nconda activate clim-recal\n\n# run resampling\ncd ../python/resampling\npython resampling_hads.py --input path_to_reprojected --grid path_to_grid_file --output path_to_resampled",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html#preparing-the-bias-correction-and-assessment",
    "href": "docs/pipeline_guidance.html#preparing-the-bias-correction-and-assessment",
    "title": "Analysis pipeline guidance",
    "section": "4 Preparing the bias correction and assessment",
    "text": "4 Preparing the bias correction and assessment\n\n4.1 Spatial cropping\nBecause the bias correction process is computationally intensive, handling large datasets can be challenging and time-consuming. Therefore, to make the pipeline more manageable and efficient, it is recommended to split the data into smaller subsets.\nFor the purposes of our example pipeline, we’ve opted for reducing the data to individual city boundaries. To crop you need to adjust the paths in Cropping_Rasters_to_three_cities.R script to fit 1your own directory sturcture. The cropping is implemented in the cpm_read_crop and hads_read_crop functions.\nRscript Cropping_Rasters_to_three_cities.R\n\n\n4.2 calibration-validation data split\nFor the purpose of assessing our bias correction, we then split our data, both the projection as well as the ground-truth observations by dates. In this example here we calibrate the bias correction based on the years 1981 to 1983.\nWe then use data from year 2010 to validate the quality of the bias correction. You need to replace path_to_cropped with the path where the data from the previous cropping step was saved and path_to_preprocessed with the output directory you choose. You can leave the -v and -r flags as specified below or choose another metric and run if you prefer.\ncd ../debiasing\npython preprocess_data.py --mod path_to_cropped --obs path_to_cropped -v tasmax -r '05' --out path_to_preprocessed --calib_dates 19810101-19831230 --valid_dates 20100101-20101230\nThe preprocess_data.py script also aligns the calendars of the historical simulation data and observed data, ensuring that they have the same time dimension and checks that the observed and simulated historical data have the same dimensions.\n\n\n\n\n\n\nNote\n\n\n\npreprocess_data.py makes use of our custom data loader functions. In data_loader.py we have written a few functions for loading and concatenating data into a single xarray which can be used for running debiasing methods. Instructions in how to use these functions can be found in python/notebooks/load_data_python.ipynb.",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/pipeline_guidance.html#applying-the-bias-correction",
    "href": "docs/pipeline_guidance.html#applying-the-bias-correction",
    "title": "Analysis pipeline guidance",
    "section": "5 Applying the bias correction",
    "text": "5 Applying the bias correction\n\n\n\n\n\n\nNote\n\n\n\nBy March 2023 we have only implemented the python-cmethods library.\n\n\nThe run_cmethods.py allow us to adjusts climate biases in climate data using the python-cmethods library. It takes as input observation data (HADs data), control data (historical UKCP data), and scenario data (future UKCP data), and applies a correction method to the scenario data. The resulting output is saved as a .nc to a specified directory.\nThe script will also produce a time-series and a map plot of the debiased data. To run this you need to replace path_to_validation_data with the output directories of the previous step and specify path_to_corrected_data as your output directory for the bias corrected data. You can also specify your preferred bias_correction_method (e.g. quantile_delta_mapping).\npython3 run_cmethods.py --input_data_folder path_to_validation_data --out path_to_corrected_data --method bias_correction_method --v 'tas'\nThe run_cmethods.py script loops over the time periods and applies debiasing in periods of 10 years in the following steps:\n\nLoads the scenario data for the current time period.\nApplies the specified correction method to the scenario data.\nSaves the resulting output to the specified directory.\nCreates diagnotic figues of the output dataset (time series and time dependent maps) and saves it into the specified directory.\n\nFor each 10 year time period it will produce an .nc output file with the adjusted data and a time-series plot and a time dependent map plot of the adjusted data.",
    "crumbs": [
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.ceda_ftp_download.html",
    "href": "docs/reference/clim_recal.ceda_ftp_download.html",
    "title": "1 clim_recal.ceda_ftp_download",
    "section": "",
    "text": "clim_recal.ceda_ftp_download\n\n\n\n\n\nName\nDescription\n\n\n\n\ndownload_ftp\nFunction to connect to the CEDA archive and download data.\n\n\n\n\n\nclim_recal.ceda_ftp_download.download_ftp(input, output, username, password, order)\nFunction to connect to the CEDA archive and download data.\nYou need to have a user account and provide your username and FTP password.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired",
    "crumbs": [
      "python",
      "Reference",
      "CEDA Data Access"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.ceda_ftp_download.html#functions",
    "href": "docs/reference/clim_recal.ceda_ftp_download.html#functions",
    "title": "1 clim_recal.ceda_ftp_download",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndownload_ftp\nFunction to connect to the CEDA archive and download data.\n\n\n\n\n\nclim_recal.ceda_ftp_download.download_ftp(input, output, username, password, order)\nFunction to connect to the CEDA archive and download data.\nYou need to have a user account and provide your username and FTP password.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput\nstr\nPath where the CEDA data to download is located (e.g /badc/ukmo-hadobs/data/insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/tasmin/day/v20220310 or top level folder like /badc/ukcp18/data/land-cpm/uk/2.2km/rcp85 if you want to download all files in all sub-directories).\nrequired\n\n\noutput\nstr\nPath to save the downloaded data - sub-directories will be created automatically under the output directory.\nrequired\n\n\nusername\nstr\nCEDA registered username\nrequired\n\n\npassword\nstr\nCEDA FPT password (obtained as explained in https://help.ceda.ac.uk/article/280-ftp)\nrequired\n\n\norder\nint\nOrder in which to run download 0: default order of file from FTP server 1: reverse order 2: shuffle. This functionality allows to run several downloads in parallel without rewriting files that are being downloaded.\nrequired",
    "crumbs": [
      "python",
      "Reference",
      "CEDA Data Access"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html",
    "href": "docs/reference/clim_recal.utils.server.html",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "clim_recal.utils.server\nUtility functions.\n\n\n\n\n\nName\nDescription\n\n\n\n\nCondaLockFileManager\nRun conda_lock install to generate conda yml.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager(self, conda_file_path=DEFAULT_CONDA_LOCK_PATH, env_paths=DEFAULT_ENV_PATHS, replace_file_path=False, legacy_arch=GITHUB_ACTIONS_ARCHITECTURE, legacy_name_prefix=CONDA_LEGACY_PREFIX, default_kwargs=lambda: DEFAULT_CONDA_LOCK_KWARGS())\nRun conda_lock install to generate conda yml.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nconda_file_path\nos.PathLike\nPath to write conda-lock file to.\n\n\nenv_paths\ntyping.Sequence[os.PathLike]\nPaths of configs to combine. For supported formats see: https://conda.github.io/conda-lock/\n\n\nreplace_file_path\nbool\nWhether to replace file_path if it already exists.\n\n\nlegacy_arch\nstr | None\nWhat archeticture to use for legacy export.\n\n\nlegacy_name_prefix\nos.PathLike | str\nstr to precede legacy_arch export file if run_legacy_mv() is run.\n\n\ndefault_kwargs\ndict[str, typing.Any]\nkwargs to pass to self.run_conda_lock().\n\n\n\n\n\n\nThis is derived from automating, with the -p osx-64 etc. components now specified in pyproject.toml and environment.yml, the following command:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64\nA full exmaple with options matching saved defaults:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64 --check-input-hash\n\n\n\n&gt;&gt;&gt; conda_lock = CondaLockFileManager()\n&gt;&gt;&gt; conda_lock\n&lt;CondaLockFileManager(conda_file_path='../conda-lock.yml', env_paths=('../environment.yml', 'pyproject.toml'), legacy_arch='linux-64')&gt;\n&gt;&gt;&gt; conda_lock.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; conda_lock.run(as_str=True, use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconda_lock_cmd_str\nReturn configured conda-lock command.\n\n\nlegacy_export_cmd_str\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\nrun\nReturn self configurations, optionally execute as subprocess.\n\n\nrun_conda_lock\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\nrun_legacy_export\nRun self.legacy_export_cmd_str().\n\n\nrun_legacy_mv\nRun self.legacy_export_cmd_str.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.conda_lock_cmd_str(use_default_kwargs=False, **kwargs)\nReturn configured conda-lock command.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.legacy_export_cmd_str(**kwargs)\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run(as_str=False, include_all=False, execute_all=False, conda_lock=True, execute_conda_lock=False, use_default_kwargs=False, legacy_export=False, execute_legacy_export=False, legacy_move=False, execute_legacy_move=False, cmds_list=None, execute_priors=False, cmds_post_list=None, execute_cmds_post=False, parent_dir_after_lock=False, **kwargs)\nReturn self configurations, optionally execute as subprocess.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nas_str\nbool\nWhether to return as a str, if not as a list[str].\nFalse\n\n\ninclude_all\nbool\nInclude all commands, overriding individual parameters like conda_lock etc. Combine with execute_all to also run.\nFalse\n\n\nexecute_all\nbool\nRun all included commands, overriding individual parameters like execute_conda_lock etc. Combine with include_all to run all commands.\nFalse\n\n\nconda_lock\nbool\nWhether to include self.run_conda_lock().\nTrue\n\n\nexecute_conda_lock\nbool\nWhether to run the generated commands via subprocess.run().\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to use self.default_kwargs params to run self.run_conda_lock().\nFalse\n\n\nlegacy_export\nbool\nWhether to add the self.legacy_export_cmd_str command.\nFalse\n\n\nexecute_legacy_export\nbool\nWhether to run the self.legacy_export_cmd_str().\nFalse\n\n\nlegacy_move\nbool\nWhether to add the self.legacy_mv_cmd_str() command.\nFalse\n\n\nexecute_legacy_move\nbool\nWhether to run the self.legacy_mv_cmd_str().\nFalse\n\n\ncmds_list\nlist[str] | None\nA list of commands to execute. If passed, these are executed prior.\nNone\n\n\nexecute_priors\nbool\nExecute commands passed in cmds_list prior to any others.\nFalse\n\n\ncmds_post_list\nlist[str] | None\nA list of commands to run after all others.\nNone\n\n\nexecute_cmds_post\nbool\nExecute commands passed in cmds_post_list after all others.\nFalse\n\n\nparent_dir_after_lock\nbool\nWhether to return to parent dir after lock command.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str] | str\nA list of commands generated, or a str of each command separated by a newline character (\\n).\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; print(conda_lock_file_manager.run(\n...     as_str=True, legacy_export=True, legacy_move=True))\nconda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml\nconda-lock render --kind explicit --platform linux-64\nmv conda-linux-64.lock .conda-linux-64.lock\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_conda_lock(execute=False, use_default_kwargs=False, parent_dir_after_lock=False, **kwargs)\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to include the self.default_kwargs in run.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.conda_lock_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock()\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(pdb=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_export(execute=False, **kwargs)\nRun self.legacy_export_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.legacy_export_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export()\n'conda-lock render --kind explicit --platform linux-64'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export(pdb=True)\n'conda-lock render --kind explicit --platform linux-64 --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_mv(execute=False)\nRun self.legacy_export_cmd_str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock .conda-linux-64.lock'\n&gt;&gt;&gt; conda_lock_file_manager.legacy_name_prefix = '../.'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock ../.conda-linux-64.lock'\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nkwargs_to_cli_str\nConvert kwargs into a cli str.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\nrm_user\nRemove user and user home folder.\n\n\nset_and_pop_attr_kwargs\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\nclim_recal.utils.server.kwargs_to_cli_str(space_prefix=True, **kwargs)\nConvert kwargs into a cli str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nkey=val parameters to concatenate as str.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA final str of concatenated **kwargs in command line form.\n\n\n\n\n\n\n&gt;&gt;&gt; kwargs_to_cli_str(cat=4, in_a=\"hat\", fun=False)\n' --cat 4 --in-a hat --not-fun'\n&gt;&gt;&gt; kwargs_to_cli_str(space_prefix=False, cat=4, fun=True)\n'--cat 4 --fun'\n&gt;&gt;&gt; kwargs_to_cli_str()\n''\n\n\n\n\nclim_recal.utils.server.make_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nName for user and home folder name to append to user_home_path.\nrequired\n\n\npassword\nstr\nLogin password.\nrequired\n\n\ncode_path\nos.PathLike\nPath to copy code from to user home directory.\nRSTUDIO_DOCKER_USER_PATH\n\n\nuser_home_path\nos.PathLike\nPath that user folder will be in, often Path('/home') in linux.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npathlib.Path\nFull path to generated user home folder.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'an_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = JUPYTER_DOCKER_USER_PATH\n&gt;&gt;&gt; make_user(user_name, password, code_path=code_path)\nPosixPath('/home/an_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'an_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.make_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\nos.PathLike\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\ntyping.Callable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; tmp_data_path = getfixture('data_fixtures_path')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = tmp_data_path / 'test_user_accounts.xlsx'\n&gt;&gt;&gt; assert test_accounts_path.exists()\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file()\n...  for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\nclim_recal.utils.server.rm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\nos.PathLike\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nuser name of account and home folder deleted.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.set_and_pop_attr_kwargs(instance, **kwargs)\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\ntyping.Any\nAn object to modify.\nrequired\n\n\nkwargs\n\nkey: val parameters to potentially modify instance attributes.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, typing.Any]\nAny remaining kwargs not used to modify instance.\n\n\n\n\n\n\n&gt;&gt;&gt; kwrgs = set_and_pop_attr_kwargs(\n...    conda_lock_file_manager, env_paths=['pyproject.toml'], cat=4)\n&gt;&gt;&gt; conda_lock_file_manager.env_paths\n['pyproject.toml']\n&gt;&gt;&gt; kwrgs\n{'cat': 4}",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html#classes",
    "href": "docs/reference/clim_recal.utils.server.html#classes",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCondaLockFileManager\nRun conda_lock install to generate conda yml.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager(self, conda_file_path=DEFAULT_CONDA_LOCK_PATH, env_paths=DEFAULT_ENV_PATHS, replace_file_path=False, legacy_arch=GITHUB_ACTIONS_ARCHITECTURE, legacy_name_prefix=CONDA_LEGACY_PREFIX, default_kwargs=lambda: DEFAULT_CONDA_LOCK_KWARGS())\nRun conda_lock install to generate conda yml.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nconda_file_path\nos.PathLike\nPath to write conda-lock file to.\n\n\nenv_paths\ntyping.Sequence[os.PathLike]\nPaths of configs to combine. For supported formats see: https://conda.github.io/conda-lock/\n\n\nreplace_file_path\nbool\nWhether to replace file_path if it already exists.\n\n\nlegacy_arch\nstr | None\nWhat archeticture to use for legacy export.\n\n\nlegacy_name_prefix\nos.PathLike | str\nstr to precede legacy_arch export file if run_legacy_mv() is run.\n\n\ndefault_kwargs\ndict[str, typing.Any]\nkwargs to pass to self.run_conda_lock().\n\n\n\n\n\n\nThis is derived from automating, with the -p osx-64 etc. components now specified in pyproject.toml and environment.yml, the following command:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64\nA full exmaple with options matching saved defaults:\nconda-lock -f environment.yml -f python/pyproject.toml -p osx-64 -p linux-64 -p linux-aarch64 --check-input-hash\n\n\n\n&gt;&gt;&gt; conda_lock = CondaLockFileManager()\n&gt;&gt;&gt; conda_lock\n&lt;CondaLockFileManager(conda_file_path='../conda-lock.yml', env_paths=('../environment.yml', 'pyproject.toml'), legacy_arch='linux-64')&gt;\n&gt;&gt;&gt; conda_lock.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; conda_lock.run(as_str=True, use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nconda_lock_cmd_str\nReturn configured conda-lock command.\n\n\nlegacy_export_cmd_str\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\nrun\nReturn self configurations, optionally execute as subprocess.\n\n\nrun_conda_lock\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\nrun_legacy_export\nRun self.legacy_export_cmd_str().\n\n\nrun_legacy_mv\nRun self.legacy_export_cmd_str.\n\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.conda_lock_cmd_str(use_default_kwargs=False, **kwargs)\nReturn configured conda-lock command.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.legacy_export_cmd_str(**kwargs)\nCommand to export legacy conda_lock file from self.conda_file_path.\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run(as_str=False, include_all=False, execute_all=False, conda_lock=True, execute_conda_lock=False, use_default_kwargs=False, legacy_export=False, execute_legacy_export=False, legacy_move=False, execute_legacy_move=False, cmds_list=None, execute_priors=False, cmds_post_list=None, execute_cmds_post=False, parent_dir_after_lock=False, **kwargs)\nReturn self configurations, optionally execute as subprocess.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nas_str\nbool\nWhether to return as a str, if not as a list[str].\nFalse\n\n\ninclude_all\nbool\nInclude all commands, overriding individual parameters like conda_lock etc. Combine with execute_all to also run.\nFalse\n\n\nexecute_all\nbool\nRun all included commands, overriding individual parameters like execute_conda_lock etc. Combine with include_all to run all commands.\nFalse\n\n\nconda_lock\nbool\nWhether to include self.run_conda_lock().\nTrue\n\n\nexecute_conda_lock\nbool\nWhether to run the generated commands via subprocess.run().\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to use self.default_kwargs params to run self.run_conda_lock().\nFalse\n\n\nlegacy_export\nbool\nWhether to add the self.legacy_export_cmd_str command.\nFalse\n\n\nexecute_legacy_export\nbool\nWhether to run the self.legacy_export_cmd_str().\nFalse\n\n\nlegacy_move\nbool\nWhether to add the self.legacy_mv_cmd_str() command.\nFalse\n\n\nexecute_legacy_move\nbool\nWhether to run the self.legacy_mv_cmd_str().\nFalse\n\n\ncmds_list\nlist[str] | None\nA list of commands to execute. If passed, these are executed prior.\nNone\n\n\nexecute_priors\nbool\nExecute commands passed in cmds_list prior to any others.\nFalse\n\n\ncmds_post_list\nlist[str] | None\nA list of commands to run after all others.\nNone\n\n\nexecute_cmds_post\nbool\nExecute commands passed in cmds_post_list after all others.\nFalse\n\n\nparent_dir_after_lock\nbool\nWhether to return to parent dir after lock command.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str] | str\nA list of commands generated, or a str of each command separated by a newline character (\\n).\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run()\n['conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml']\n&gt;&gt;&gt; print(conda_lock_file_manager.run(\n...     as_str=True, legacy_export=True, legacy_move=True))\nconda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml\nconda-lock render --kind explicit --platform linux-64\nmv conda-linux-64.lock .conda-linux-64.lock\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_conda_lock(execute=False, use_default_kwargs=False, parent_dir_after_lock=False, **kwargs)\nCheck and optionally execute self.conda_lock_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nuse_default_kwargs\nbool\nWhether to include the self.default_kwargs in run.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.conda_lock_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock()\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(use_default_kwargs=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --check-input-hash'\n&gt;&gt;&gt; conda_lock_file_manager.run_conda_lock(pdb=True)\n'conda-lock lock --lockfile ../conda-lock.yml -f ../environment.yml -f pyproject.toml --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_export(execute=False, **kwargs)\nRun self.legacy_export_cmd_str().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\nkwargs\n\nAny additional parameters to pass to self.legacy_export_cmd_str().\n{}\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export()\n'conda-lock render --kind explicit --platform linux-64'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_export(pdb=True)\n'conda-lock render --kind explicit --platform linux-64 --pdb'\n\n\n\n\nclim_recal.utils.server.CondaLockFileManager.run_legacy_mv(execute=False)\nRun self.legacy_export_cmd_str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexecute\nbool\nWhether to run self.conda_lock_cmd_str() as a subprocess.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nFinal generated command str, whether excuted or not.\n\n\n\n\n\n\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock .conda-linux-64.lock'\n&gt;&gt;&gt; conda_lock_file_manager.legacy_name_prefix = '../.'\n&gt;&gt;&gt; conda_lock_file_manager.run_legacy_mv()\n'mv conda-linux-64.lock ../.conda-linux-64.lock'",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.server.html#functions",
    "href": "docs/reference/clim_recal.utils.server.html#functions",
    "title": "1 clim_recal.utils.server",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nkwargs_to_cli_str\nConvert kwargs into a cli str.\n\n\nmake_user\nMake user account and copy code to that environment.\n\n\nmake_users\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\nrm_user\nRemove user and user home folder.\n\n\nset_and_pop_attr_kwargs\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\nclim_recal.utils.server.kwargs_to_cli_str(space_prefix=True, **kwargs)\nConvert kwargs into a cli str.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nkey=val parameters to concatenate as str.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nA final str of concatenated **kwargs in command line form.\n\n\n\n\n\n\n&gt;&gt;&gt; kwargs_to_cli_str(cat=4, in_a=\"hat\", fun=False)\n' --cat 4 --in-a hat --not-fun'\n&gt;&gt;&gt; kwargs_to_cli_str(space_prefix=False, cat=4, fun=True)\n'--cat 4 --fun'\n&gt;&gt;&gt; kwargs_to_cli_str()\n''\n\n\n\n\nclim_recal.utils.server.make_user(user, password, code_path=RSTUDIO_DOCKER_USER_PATH, user_home_path=DEBIAN_HOME_PATH)\nMake user account and copy code to that environment.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nName for user and home folder name to append to user_home_path.\nrequired\n\n\npassword\nstr\nLogin password.\nrequired\n\n\ncode_path\nos.PathLike\nPath to copy code from to user home directory.\nRSTUDIO_DOCKER_USER_PATH\n\n\nuser_home_path\nos.PathLike\nPath that user folder will be in, often Path('/home') in linux.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npathlib.Path\nFull path to generated user home folder.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; user_name: str = 'an_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; code_path: Path = JUPYTER_DOCKER_USER_PATH\n&gt;&gt;&gt; make_user(user_name, password, code_path=code_path)\nPosixPath('/home/an_unlinkely_test_user')\n&gt;&gt;&gt; Path(f'/home/{user_name}/python/conftest.py').is_file()\nTrue\n&gt;&gt;&gt; rm_user(user_name)\n'an_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.make_users(file_path, user_col, password_col, file_reader, **kwargs)\nLoad a file of usernames and passwords and pass each line to make_user.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfile_path\nos.PathLike\nPath to collumned file including user names and passwords per row.\nrequired\n\n\nuser_col\nstr\nstr of column name for user names.\nrequired\n\n\npassword_col\nstr\nstr of column name for passwords.\nrequired\n\n\nfile_reader\ntyping.Callable\nCallable (function) to read file_path.\nrequired\n\n\n**kwargs\n\nAdditional parameters for to pass to file_reader function.\n{}\n\n\n\n\n\n\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; tmp_data_path = getfixture('data_fixtures_path')\n&gt;&gt;&gt; from pandas import read_excel\n&gt;&gt;&gt; def excel_row_iter(path: Path, **kwargs) -&gt; dict:\n...     df: DataFrame = read_excel(path, **kwargs)\n...     return df.to_dict(orient=\"records\")\n&gt;&gt;&gt; test_accounts_path: Path = tmp_data_path / 'test_user_accounts.xlsx'\n&gt;&gt;&gt; assert test_accounts_path.exists()\n&gt;&gt;&gt; user_paths: tuple[Path, ...] = tuple(make_users(\n...     file_path=test_accounts_path,\n...     user_col=\"User Name\",\n...     password_col=\"Password\",\n...     file_reader=excel_row_iter,\n...     code_path=JUPYTER_DOCKER_USER_PATH,\n... ))\n&gt;&gt;&gt; [(path / 'python' / 'conftest.py').is_file()\n...  for path in user_paths]\n[True, True, True, True, True]\n&gt;&gt;&gt; [rm_user(user_path.name) for user_path in user_paths]\n['sally', 'george', 'jean', 'felicity', 'frank']\n\n\n\n\nclim_recal.utils.server.rm_user(user, user_home_path=DEBIAN_HOME_PATH)\nRemove user and user home folder.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUser home folder name (usually the same as the user login name).\nrequired\n\n\nuser_home_path\nos.PathLike\nParent path of user folder name.\nDEBIAN_HOME_PATH\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nuser name of account and home folder deleted.\n\n\n\n\n\n\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; if os.geteuid() != 0:\n...     pytest.skip('requires root permission to run')\n&gt;&gt;&gt; if is_platform_darwin:\n...     pytest.skip('test designed for docker jupyter')\n&gt;&gt;&gt; user_name: str = 'very_unlinkely_test_user'\n&gt;&gt;&gt; password: str = 'test_pass'\n&gt;&gt;&gt; make_user(user_name, password, code_path=JUPYTER_DOCKER_USER_PATH)\nPosixPath('/home/very_unlinkely_test_user')\n&gt;&gt;&gt; rm_user(user_name)\n'very_unlinkely_test_user'\n\n\n\n\nclim_recal.utils.server.set_and_pop_attr_kwargs(instance, **kwargs)\nExtract any key: val pairs from kwargs to modify instance.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninstance\ntyping.Any\nAn object to modify.\nrequired\n\n\nkwargs\n\nkey: val parameters to potentially modify instance attributes.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, typing.Any]\nAny remaining kwargs not used to modify instance.\n\n\n\n\n\n\n&gt;&gt;&gt; kwrgs = set_and_pop_attr_kwargs(\n...    conda_lock_file_manager, env_paths=['pyproject.toml'], cat=4)\n&gt;&gt;&gt; conda_lock_file_manager.env_paths\n['pyproject.toml']\n&gt;&gt;&gt; kwrgs\n{'cat': 4}",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "server"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html",
    "href": "docs/reference/clim_recal.pipeline.html",
    "title": "1 clim_recal.pipeline",
    "section": "",
    "text": "clim_recal.pipeline\nWrappers to automate the entire pipeline.\nFollowing Andy’s very helpful excel file, this manages a reproduction of all steps of the debiasing pipeline.",
    "crumbs": [
      "python",
      "Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html#todo",
    "href": "docs/reference/clim_recal.pipeline.html#todo",
    "title": "1 clim_recal.pipeline",
    "section": "4.1 Todo",
    "text": "4.1 Todo\n\nUpdate the example here\nRemove out of date elements\nHardcode the process below\nRefactor to facilitate testing\nEnsure HADs still works\nAdd function for UKCP\nCheck convert_xr_calendar doctest examples\nFix order of UKCP changes\n\nTo run this step in the pipeline the following should work for the default combindations of variables: tasmax, tasmin, and rainfall and the default set of runs: 05, 06, 07 and 08, assuming the necessary data is mounted.\nIf installed via pipx/pip etc. on your local path (or within Docker) clim-recal should be a command line function\n$ clim-recal --all-variables --default-runs --output-path /where/results/should/be/written\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, cities_count=1, methods_count=1,\n                 cpm_folders_count=12, hads_folders_count=3, start_index=0,\n                 stop_index=None, cpus=2)&gt;\n&lt;CPMResamplerManager(variables_count=3, runs_count=4, input_paths_count=12)&gt;\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\nOtherwise, you can install locally and either run via pdm from the python folder\n$ cd clim-recal/python\n$ pdm run clim-recal --all-variables --default-runs --output-path /where/results/should/be/written\n# Skipping output for brevity\nOr within an ipython or jupyter instance (truncated below for brevity)\n&gt;&gt;&gt; from clim_recal.pipeline import main\n&gt;&gt;&gt; main(all_variables=True, default_runs=True)  # doctest: +SKIP\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, ...&gt;\nRegardless of your route, once you’re confident with the configuration, add the --execute parameter to run. For example, assuming a local install:\n$ clim-recal --all-variables --default-runs --output-path /where/results/should/be/written --execute\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=3, runs_count=4, cities_count=1, methods_count=1,\n                 cpm_folders_count=12, hads_folders_count=3, start_index=0,\n                 stop_index=None, cpus=2)&gt;\n&lt;CPMResamplerManager(variables_count=3, runs_count=4, input_paths_count=12)&gt;\n&lt;HADsResamplerManager(variables_count=3, input_paths_count=3)&gt;\nRunning CPM Standard Calendar projection...\n&lt;CPMResampler(count=100, max_count=100,\n              input_path='/mnt/vmfileshare/ClimateData/Raw/UKCP2.2/tasmax/05/latest',\n              output_path='/mnt/vmfileshare/ClimateData/CPM-365/test-run-3-may/resample/\n              cpm/tasmax/05/latest')&gt;\n 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100  [ 0:38:27 &lt; 0:00:00 , 0 it/s ]\n  87% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/100  [ 0:17:42 &lt; 0:03:07 , 0 it/s ]",
    "crumbs": [
      "python",
      "Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.pipeline.html#functions",
    "href": "docs/reference/clim_recal.pipeline.html#functions",
    "title": "1 clim_recal.pipeline",
    "section": "8.1 Functions",
    "text": "8.1 Functions\n\n\n\nName\nDescription\n\n\n\n\nmain\nRun all elements of the pipeline.\n\n\n\n\n8.1.1 main\nclim_recal.pipeline.main(execute=False, output_path=DEFAULT_OUTPUT_PATH, variables=(VariableOptions.default()), cities=(CityOptions.default()), runs=(RunOptions.default()), methods=(MethodOptions.default()), all_variables=False, all_cities=False, default_runs=False, all_runs=False, all_methods=False, skip_cpm_standard_calendar_projection=False, skip_hads_spatial_2k_projection=False, cpus=None, multiprocess=False, start_index=0, stop_index=None, total=None, print_range_length=5, **kwargs)\nRun all elements of the pipeline.\n\n8.1.1.1 Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\ntyping.Sequence[python.clim_recal.config.VariableOptions | str]\nVariables to include in the model, eg. tasmax, tasmin.\n(VariableOptions.default())\n\n\nruns\ntyping.Sequence[python.clim_recal.config.RunOptions | str]\nWhich model runs to include, eg. “01”, “08”, “11”.\n(RunOptions.default())\n\n\ncities\ntyping.Sequence[python.clim_recal.config.CityOptions | str] | None\nWhich cities to crop data to. Future plans facilitate skipping to run for entire UK.\n(CityOptions.default())\n\n\nmethods\ntyping.Sequence[python.clim_recal.config.MethodOptions | str]\nWhich debiasing methods to apply.\n(MethodOptions.default())\n\n\noutput_path\nos.PathLike\nPath to save intermediate and final results to.\nDEFAULT_OUTPUT_PATH\n\n\ncpus\nint | None\nNumber of cpus to use when multiprocessing.\nNone\n\n\nmultiprocess\nbool\nWhether to use multiprocessing.\nFalse\n\n\nstart_index\nint\nIndex to start all iterations from.\n0\n\n\ntotal\nint | None\nTotal number of records to iterate over. 0 and None indicate all values from start_index.\nNone\n\n\n**kwargs\n\nAdditional parameters to pass to a ClimRecalConfig.\n{}\n\n\n\n\n\n8.1.1.2 Notes\nThe default parameters here are meant to reflect the entire workflow process to ease reproducibility.\n\n\n8.1.1.3 Examples\nNote the _allow_check_fail parameters support running the examples without data mounted from a server.\n&gt;&gt;&gt; main(variables=(\"rainfall\", \"tasmin\"),\n...      output_path=test_runs_output_path,\n...      cpm_kwargs=dict(_allow_check_fail=True),\n...      hads_kwargs=dict(_allow_check_fail=True),\n... )\nclim-recal pipeline configurations:\n&lt;ClimRecalConfig(variables_count=2, runs_count=1,\n                 cities_count=1, methods_count=1,\n                 cpm_folders_count=2, hads_folders_count=2,\n                 start_index=0, stop_index=None,\n                 cpus=...)&gt;\n&lt;CPMResamplerManager(variables_count=2, runs_count=1,\n                     input_paths_count=2)&gt;\n&lt;HADsResamplerManager(variables_count=2, input_paths_count=2)&gt;\nNo steps run. Add '--execute' to run steps.",
    "crumbs": [
      "python",
      "Reference",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.data.html",
    "href": "docs/reference/clim_recal.utils.data.html",
    "title": "1 clim_recal.utils.data",
    "section": "",
    "text": "clim_recal.utils.data\n\n\n\n\n\nName\nDescription\n\n\n\n\nCityOptions\nSupported options for variables.\n\n\nDataLicense\nClass for standardising data license references.\n\n\nMetaData\nManage info on source material.\n\n\nMethodOptions\nSupported options for methods.\n\n\nRunOptions\nSupported options for variables.\n\n\nVariableOptions\nSupported options for variables\n\n\n\n\n\nclim_recal.utils.data.CityOptions()\nSupported options for variables.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.CityOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.CityOptions.default()\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.DataLicense(self, name, url, version)\nClass for standardising data license references.\n\n\n\nclim_recal.utils.data.MetaData(self, name, slug, region=None, description=None, date_created=None, authors=None, url=None, info_url=None, doi=None, path=None, license=lambda: OpenGovernmentLicense(), dates=None, date_published=None, unit=None, cite_as=None)\nManage info on source material.\n\n\n\nclim_recal.utils.data.MethodOptions()\nSupported options for methods.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.MethodOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.MethodOptions.default()\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.RunOptions()\nSupported options for variables.\n\n\nOptions TWO and THREE are not available for UKCP2.2.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\npreferred\nReturn the prferred runs determined by initial results.\n\n\n\n\n\nclim_recal.utils.data.RunOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RunOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.RunOptions.preferred()\nReturn the prferred runs determined by initial results.\n\n\nSee R/misc/Identifying_Runs.md for motivation and results.\n\n\n\n\n\n\nclim_recal.utils.data.VariableOptions()\nSupported options for variables\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ncpm_value\nReturn CPM value equivalent of variable.\n\n\ncpm_values\nReturn CPM values equivalent of variable.\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_value(variable)\nReturn CPM value equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nVariableOptions attribute to query value of.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_value('rainfall')\n'pr'\n&gt;&gt;&gt; VariableOptions.cpm_value('tasmin')\n'tasmin'\n\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_values(variables=None)\nReturn CPM values equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\ntyping.Iterable[str] | None\nVariableOptions attributes to query values of.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_values(['rainfall', 'tasmin'])\n('pr', 'tasmin')\n\n\n\n\nclim_recal.utils.data.VariableOptions.default()\nDefault option.",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "data"
    ]
  },
  {
    "objectID": "docs/reference/clim_recal.utils.data.html#classes",
    "href": "docs/reference/clim_recal.utils.data.html#classes",
    "title": "1 clim_recal.utils.data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nCityOptions\nSupported options for variables.\n\n\nDataLicense\nClass for standardising data license references.\n\n\nMetaData\nManage info on source material.\n\n\nMethodOptions\nSupported options for methods.\n\n\nRunOptions\nSupported options for variables.\n\n\nVariableOptions\nSupported options for variables\n\n\n\n\n\nclim_recal.utils.data.CityOptions()\nSupported options for variables.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.CityOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.CityOptions.default()\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.DataLicense(self, name, url, version)\nClass for standardising data license references.\n\n\n\nclim_recal.utils.data.MetaData(self, name, slug, region=None, description=None, date_created=None, authors=None, url=None, info_url=None, doi=None, path=None, license=lambda: OpenGovernmentLicense(), dates=None, date_published=None, unit=None, cite_as=None)\nManage info on source material.\n\n\n\nclim_recal.utils.data.MethodOptions()\nSupported options for methods.\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.MethodOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.MethodOptions.default()\nDefault method option.\n\n\n\n\n\nclim_recal.utils.data.RunOptions()\nSupported options for variables.\n\n\nOptions TWO and THREE are not available for UKCP2.2.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ndefault\nDefault option.\n\n\npreferred\nReturn the prferred runs determined by initial results.\n\n\n\n\n\nclim_recal.utils.data.RunOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.RunOptions.default()\nDefault option.\n\n\n\nclim_recal.utils.data.RunOptions.preferred()\nReturn the prferred runs determined by initial results.\n\n\nSee R/misc/Identifying_Runs.md for motivation and results.\n\n\n\n\n\n\nclim_recal.utils.data.VariableOptions()\nSupported options for variables\n\n\n\n\n\nName\nDescription\n\n\n\n\nall\nReturn a tuple of all options\n\n\ncpm_value\nReturn CPM value equivalent of variable.\n\n\ncpm_values\nReturn CPM values equivalent of variable.\n\n\ndefault\nDefault option.\n\n\n\n\n\nclim_recal.utils.data.VariableOptions.all()\nReturn a tuple of all options\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_value(variable)\nReturn CPM value equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariable\nstr\nVariableOptions attribute to query value of.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_value('rainfall')\n'pr'\n&gt;&gt;&gt; VariableOptions.cpm_value('tasmin')\n'tasmin'\n\n\n\n\nclim_recal.utils.data.VariableOptions.cpm_values(variables=None)\nReturn CPM values equivalent of variable.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\ntyping.Iterable[str] | None\nVariableOptions attributes to query values of.\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; VariableOptions.cpm_values(['rainfall', 'tasmin'])\n('pr', 'tasmin')\n\n\n\n\nclim_recal.utils.data.VariableOptions.default()\nDefault option.",
    "crumbs": [
      "python",
      "Reference",
      "Utilities",
      "data"
    ]
  }
]