---
title: "Assessing and Processing LCAT data"
author: "Ruth C E Bowyer"
date: "`r format(Sys.Date())`"
output:
  github_document
---



```{r libs and setup, message=FALSE, warning=F}
rm(list=ls())

knitr::opts_knit$set(root.dir="/mnt/vmfileshare/ClimateData/")

library(ggplot2)
library(terra)
library(tmap) #pretty maps
library(RColorBrewer)

dd <- "/mnt/vmfileshare/ClimateData/"

```


## **0. About**

LCAT require 'bias corrected' data for the whole of the UK. 
We have applied a widely used approach, quantile mapping, to the data. Specifically, we have used *non-parametric quantile mapping using empirical quantiles* as available in the `Qmap` package. 
Because the data is so large, we have applied this bias correction to the UK broked down into regions, with Scotland brokedn down into further regions (see `R/LCAT/Region.Refs.csv`)

We will now:

- Assess the bias correction using some of the segments
- Process the data back to geotiff 
- Either process as monthly data or as UK wide rasters (maybe just write them seperately) and av across runs

The data is within `ClimateData/Debiased/R/QuantileMapping` and is in RDS format, with each object containing a list.

The objects within this R list are as follows:
- 't.obs': transposed observation df
- 't.cal': transposed calibration df
- 't.proj': transposed projection df (included the validation period)
- 'qm1.hist.a' - bias corrected values for the historical period, values fitted with linear interpolation
- 'qm1.hist.b' - bias corrected values for the historical period, values fitted with tricubic interpolation
- 'qm1.proj.a' - bias corrected values for the validation/projection period, values fitted with linear interpolation
- 'qm1.proj.b' - bias corrected values for the validation/projection period, values fitted with tricubic interpolation

## **1. Bias Correction Assessment: trends**

### **London - tasmax = Run 08**

Using the London region (UKI) as this is the smallest -- not this is the same regional area as the 'three.cities' crops but cut to shapefile edges rather than the square grid 

```{r}

London <- readRDS(paste0(dd,"/Debiased/R/QuantileMapping/resultsLRun08_UKI_tasmax.RDS"))

```

Load in Hads validation data 
(So this can be run for all of the LCAT data, I'm going to read in the whole HADs files for the calibration years)

**The calibration period is 2009-12-01 to 2019-11-30 to relate to the CPM month grouping**

Hads data were also cropped to the regional files for the calibration years - some of the dates might need to be added from the observation (or just be ignored for ease)

```{r}
fp <- paste0(dd, "Interim/HadsUK/Data_as_df/")
f <- list.files(fp)
v <- "tasmax"
reg <- "UKI"
f <- f[grepl("2010_2020", f)&grepl(v,f)&grepl(reg, f)]

obs.val.df <- read.csv(paste0(fp,f)) # Starts here from 2010 - 01 -01 -- because for the BC I removed these vals to align with the cpm years we're missing the month of Dec - so need to update the cpm data to reflect this in the assessment -- wont be a problem for other BC data 

```


### **1b. Check trends**

The next set of chunks visualise the data by converting back to raster, and by looking at the trends of data across all time periods

```{r convert to df and raster}

## Load a source raster to extract the crs
r <- list.files(paste0(dd, "Reprojected/UKCP2.2/tasmax/05/latest/"))
r <- r[1]
rp <- paste0(dd, "Reprojected/UKCP2.2/tasmax/05/latest/", r)
rast <- rast(rp)

crs <- crs(rast)

## Convert from matix to df, transpose, create x and y cols - when run in chunk this works fine but for some reason can throw an error when run otherwise
London.df <- lapply(London, 
                    function(d){
  df <- as.data.frame(t(d))
  rn <- row.names(df) #The x_y coords were saves as rownames
  x <- gsub("_.*", "", rn)
  y <- gsub(".*_", "", rn)
  xy <- data.frame(x=x,y=y)
  df <- cbind(xy,df)
  })

## Convert to rasters
London.rasts <- lapply(London.df, function(x){
  r <- rast(x, type="xyz")
  crs(r) <- crs
  return(r)
})


```

#### *Raster vis comparison*

Random selection of 3 days of the observation, calibration and two adjusted cals, for three historic days

```{r}
tm_shape(London.rasts$t.obs[[1]]) + tm_raster(title="Observation, 1980-12-01")
tm_shape(London.rasts$t.cal[[1]]) + tm_raster(title="Calibration, 1980-12-01")
tm_shape(London.rasts$qm1.hist.a[[1]]) + tm_raster(title="Calibration, bias corrected, linear 1980-12-01")
tm_shape(London.rasts$qm1.hist.b[[1]]) + tm_raster(title="Calibration, bias corrected, tricubic 1980-12-01")
```
#### *Annual trends - Calibration period - daily mean*

```{r}

London.dfg <- lapply(names(London.df), function(i){
  dfi <- London.df[[i]]
  x <- 3:ncol(dfi)
  
  dfx <- lapply(x, function(x){
    y <- dfi[,x]
    mean <- mean(y, na.rm=T)
    sd <- sd(y, na.rm=T)
    dfr <- data.frame(mean=mean, 
             sd.high=mean+sd,
             sd.low=mean-sd)
    names(dfr) <- paste0(i,".",names(dfr))
    dfr$day <- names(dfi)[x]
    return(dfr)
  })

  dfx_g <- dfx %>% purrr::reduce(rbind)
})

names(London.dfg) <- c("obs.daymeans", "raw.cal.daymeans",
                       "raw.proj.daymeans", "bc.a.cal.daymeans",
                       "bc.b.cal.daymeans", "bc.a.proj.daymeans",
                       "bc.b.proj.daymeans")

```

```{r}
#Add a day index to align the cal and obs 

London.dfg.calp <- London.dfg[c("obs.daymeans", "raw.cal.daymeans",
                       "bc.b.cal.daymeans", "bc.a.cal.daymeans")]

London.dfg.calp <- lapply(London.dfg.calp, function(x){
  x$dayi <- 1:nrow(x)
  x$day<- NULL
  return(x)
})

London.dfg.calp <- London.dfg.calp %>% reduce(merge, "dayi")

head(London.dfg.calp)
```

```{r}

London.dfg.calp_m <- reshape2::melt(London.dfg.calp, id="dayi") #create long df for plotting multiple lines

London.dfg.calp_mm <- London.dfg.calp_m[grepl(".mean", London.dfg.calp_m$variable),] #For easy vis, only keep mean vals
```


```{r Historic trend 1}

ggplot(London.dfg.calp_mm, aes(dayi, value, group=variable, colour=variable)) + 
  geom_line() +
  theme_bw() + ylab("Av daily max temp oC") + 
  ggtitle("Tasmax Hisotric trends") +
 scale_x_discrete(labels = NULL, breaks = NULL) + xlab("Day, 1980.12.01 - 2009.12.01") +
  scale_color_brewer(palette="Set1", name="Model", labels=c("Obs (Hads)", "Raw CPM", "BC CPM 1", "BC CPM 2"))

```

#### *Annual trends - Calibration period - seasonal mean*

Annotate season based on month index - the dates have different formats depending on the input data (ie hads vs cpm) so am pulling out the necessary to adjust sep 

```{r}

#Hads/obs df
obs.daymeans.df <- London.dfg$obs.daymeans

x <- obs.daymeans.df$day
obs.daymeans.df$season <- ifelse(grepl("1231_|0131_|0228_|0229_", x), "Winter",
                      ifelse(grepl("0331_|0430_|0531_", x), "Spring",
                          ifelse(grepl("0630_|0731_|0831_", x), "Summer", "Autumn")))

#A note here - the seasons should each have 90 days but seemingly Winter and Autumn have 89 and Spring and Summer have 91 - this is due to how the manual aligning worked out and should be updated when the hads data is re-run 


# Mutate to a seasonal mean
obs.seasonal.mean.df <- obs.daymeans.df %>% 
  group_by(season_year) %>% 
          mutate(mean.seasonal = mean(t.obs.mean),
                                    sd.high.seasonal = mean.seasonal + sd(t.obs.mean),
                                    sd.low.seasonal = mean.seasonal - sd(t.obs.mean))

obs.seasonal.mean.df <- obs.seasonal.mean.df %>%
  dplyr::select(season_year:sd.low.seasonal) %>% distinct()

#Grouping variable for later vars 
obs.seasonal.mean.df$model <- "obs"
```

```{r}
#lapply needs to needed 

London.dfg[c("raw.cal.daymeans", "bc.b.cal.daymeans", "bc.a.cal.daymeans")]

#lapply for remaining dfs
  x <- df$day
  #The CPM days are consecutive 1 - 360 by year
  winter <- paste0(30, "_", 1:90, collapse="|")
  spring <- paste0(30, "_", 91:180, collapse="|")
  summer <- paste0(30, "_", 181:270, collapse="|")
```
#HERE - sorting out below season
```{r}
  df$season <- ifelse(grepl(winter, x), "Winter",
                      ifelse(grepl(spring, x), "Spring",
                          ifelse(grepl(summer, x), "Summer", "Autumn")))

# Mutate to a seasonal mean
obs.seasonal.mean.df <- obs.daymeans.df %>% 
  group_by(season_year) %>% 
          mutate(mean.seasonal = mean(t.obs.mean),
                                    sd.high.seasonal = mean.seasonal + sd(t.obs.mean),
                                    sd.low.seasonal = mean.seasonal - sd(t.obs.mean))

obs.seasonal.mean.df <- obs.seasonal.mean.df %>%
  dplyr::select(season_year:sd.low.seasonal) %>% distinct()


obs.seasonal.mean.df$model <- "obs"
```


```{r Raw trend seasonal}

#Add in missing years for clearer plotting of trend
dfg_sm <- seasonal.mean

seas.miss <- rep(c("Spring", "Summer", "Autumn", "Winter"), 19)
year.miss <- rep(2041:2059, each=4)

add.s.y <- paste0(seas.miss, "_", year.miss)
add.s.y <- c("Winter_2040", add.s.y)

dfg_sm <- plyr::rbind.fill(dfg_sm,
                           data.frame(year=c(2040, year.miss),
                                      season_year=add.s.y, 
                                      mean.seasonal=NA,
                                      sd.low.seasonal=NA,
                                      sd.high.seasonal=NA))

dfg_sm <- dfg_sm[order(dfg_sm$year),]
```


**'Raw' - seasonal**

```{r Raw seasonal}

ggplot(dfg_sm) + 
  geom_ribbon(aes(x = 1:length(season_year), ymin = sd.low.seasonal, ymax=sd.high.seasonal), color="lightgrey", alpha=0.5) +
  geom_line(aes(x=1:length(season_year), y=mean.seasonal), color="cornflowerblue", group=1) +
  theme_bw() + ylab("Av daily max temp oC") + 
  ggtitle("'Raw' - tasmax seasonal") + 
  xlab("Season - Year") +
  scale_x_discrete(labels = c(dfg_sm$season_year)) + 
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust=1))

```


**'Raw' - Winter only**

```{r Raw seasonal winter}

dfg_sm_w <- subset(dfg_sm, grepl("Winter", season_year))

ggplot(dfg_sm_w) + 
  geom_ribbon(aes(year, ymin = sd.low.seasonal, ymax=sd.high.seasonal), 
              fill="lightblue3", alpha=0.5) +
  geom_line(aes(year, y=mean.seasonal), color="lightblue4", group=1) +
  theme_bw() + ylab("Av daily max temp oC") + 
  ggtitle("'Raw' - tasmax seasonal - Winter only") + 
  xlab("Year") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```


**'Raw' - Summer only**

```{r Raw seasonal summer}

dfg_sm_s <- subset(dfg_sm, grepl("Summer", season_year))

ggplot(dfg_sm_s) + 
  geom_ribbon(aes(year, ymin = sd.low.seasonal, ymax=sd.high.seasonal), 
              fill="darkgoldenrod", alpha=0.5) +
  geom_line(aes(year, y=mean.seasonal), color="darkred", group=1) +
  theme_bw() + ylab("Av daily max temp oC") + 
  ggtitle("'Raw' - tasmax seasonal - Summer only") + 
  xlab("Year") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

#### *Annual trends - seasonal max*

I think visualising the daily data is not mega helpful, but now grouping to season and calculating the seasonal maxima vals (i.e. rather than means above)

#### Create validaton df list

Adding in the observational HADs data and aligning based on dates

*Note* So as not to re-run the UK wide LCAT data processing, a workaround was added to the bias correction function used to group the obs data - this means that to align the validation cpm data we have to remove a month in the beginning - ie the LCAT specific 

20191201-20201130_30 - runs to the end of the month of December of 2019 -- so should map the obvs.val

```{r}

#Extract validation period of raw and bias corrected CPM data 
cpm.val.dfs <- lapply(London.df[c("t.proj", "qm1.proj.a", "qm1.proj.b")], function(x){
  i <- grep("20191201-20201130_30", names(x))[1]
  df <- x[,1:i]
})

#Using the old cpm data for the hads obs - so need to remove the dates to ensure theres 30 days per year
remove <- c("0229_29", "0430_30", "0731_31", "0930_30", "1130_30")
remove <- paste0(remove, collapse = "|")
      
obs.val.df <- obs.val.df[,!grepl(remove, names(obs.val.df))]
row.names(obs.val.df) <- paste0(obs.val.df$x, "_", obs.val.df$y)

val.dfs <- c(list(obs.val.df), cpm.val.dfs)
names(val.dfs) <- c("obs.val.df", "raw.cpm.val", "bc1.cpm.val", "bc2.cpm.val")
```


#### *Validation period - annual trends - seasonal mean*



#### *Validation period - annual trends - seasonal max*


## **2. Bias Correction Assessment: Metrics**

Using the validation data set for this

Most metrics will just require vectors of values at this point, although it would be nice to have the georeferenced incorporated incase values spatially vary depending on eg topography 

```{r}
val.dfs.v <- lapply(val.dfs, function(x){
  #Remove x and y 
  x$x <- NULL
  x$y <- NULL
  #Convert to single vector
  unlist(as.vector(x))
})

val.dfs.v.df <- val.dfs.v %>% reduce(cbind)
val.dfs.v.df <- as.data.frame(val.dfs.v.df)
```

### **2a. Descriptive statistics**

```{r descriptives validation}

descriptives <-  lapply(val.dfs.v, function(x){
  per <- data.frame(as.list(quantile(x, probs=c(0.1, 0.9))))
  data.frame(mean=mean(x), sd=sd(x), per10th=per$X10.,per90th=per$X90.)
})

descriptives <- descriptives %>% reduce(rbind)
row.names(descriptives) <- c("obs", "raw.cpm", "bc1.cpm", "bc2.cpm")
descriptives
```



#### **Distribution**

```{r}

cal.val.dfg <- reshape2::melt(as.matrix(val.dfs.v))


```


### **2b. RMSE**

sqrt(mean((data$actual - data$predicted)^2))

```{r}

actual <- val.dfs.v$obs.val.df

rmse <- sapply(val.dfs.v[c(2:4)], function(x){
  sqrt(mean((actual - x)^2))
})

data.frame(as.list(rmse), row.names = "RMSE")
```

In this example, the first bias correction has a lower RMSE (just!) and therefore is better fitting than the raw 

# Taylor diagram


## **3. Bias Correction Assessment: Metric specific - tasmax**

### mean by cell 
